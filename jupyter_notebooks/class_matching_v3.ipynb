{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of class names to check\n",
    "# initial_class_names_to_check = [ 'Compression','AmperePerJoule','nfdi','stress', 'Advertiser+content_Article', 'Tensiletest']\n",
    "initial_class_names_to_check = [ 'biochemicalreaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ontology files to process\n",
    "ontology_files = [\n",
    "    # \"../Ontologies/materialsmine.ttl\", ### is not complete!\n",
    "    \"../Ontologies/materialsmine_converted.ttl\",\n",
    "    \"../Ontologies/pmdco_core.ttl\",\n",
    "    # \"../Ontologies/nfdicore_2.ttl\",\n",
    "    # \"../Ontologies/bfo.owl\", #### using this ---->  long time to proccess!\n",
    "    # \"../Ontologies/emmo.ttl\",\n",
    "    # \"../Ontologies/owlapi.xrdf\",\n",
    "    # \"../Ontologies/schemaorg.owl\",\n",
    "    # \"../Ontologies/MaterialsMine.xrdf\",\n",
    "    # '../Ontologies/emmo.owl', ### has problem of reading file\n",
    "    # \"../Ontologies/Physical_Activity_Ontology_V2.owl\",\n",
    "    # \"../Ontologies/Physical_Activity_Ontology_V2.xrdf\",\n",
    "    # \"../Ontologies/oboe.owl\",\n",
    "    # Add more file paths as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "# isPartOf = dcterms.isPartOf\n",
    "\n",
    "\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "# isPartOf = Namespace(\"http://semanticscience.org/resource/isPartOf\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "# print(isPartOf)\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_description(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return descriptions[0] if descriptions else None\n",
    "\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g = Graph()\n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "        description = get_class_description(g, cls)\n",
    "        for label in labels:\n",
    "            normalized_label = normalize_string(label)\n",
    "            found_class_labels.add(normalized_label)\n",
    "            if normalized_label in normalized_class_names_to_check:\n",
    "                data.append([file_path, str(cls), str(label), str(description)])\n",
    "                for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
    "                for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
    "                for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_description(g, URIRef(relation[1]))\n",
    "                object_description = get_class_description(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 2\n",
    "iteration_count = 0\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Class URI\", \"Label\", \"Description\"])\n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
    "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
    "    writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, Graph(), writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_description(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return descriptions[0] if descriptions else None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g = Graph()\n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "        description = get_class_description(g, cls)\n",
    "        for label in labels:\n",
    "            normalized_label = normalize_string(label)\n",
    "            found_class_labels.add(normalized_label)\n",
    "            if normalized_label in normalized_class_names_to_check:\n",
    "                data.append([file_path, str(cls), str(label), str(description)])\n",
    "                for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    obj_description = get_class_description(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    obj_description = get_class_description(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label), str(obj_description)])\n",
    "                for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    obj_description = get_class_description(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_description(g, URIRef(relation[1]))\n",
    "                object_description = get_class_description(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 2\n",
    "iteration_count = 0\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "# Filter and save class data\n",
    "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Class URI\", \"Label\", \"Description\"])\n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "# Filter and save class relations\n",
    "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
    "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\", \"Object Description\"])\n",
    "    writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, Graph(), writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_description(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return descriptions[0] if descriptions else None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g = Graph()\n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "        description = get_class_description(g, cls)\n",
    "        for label in labels:\n",
    "            normalized_label = normalize_string(label)\n",
    "            found_class_labels.add(normalized_label)\n",
    "            if normalized_label in normalized_class_names_to_check:\n",
    "                data.append([file_path, str(cls), str(label), str(description)])\n",
    "                for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    obj_description = get_class_description(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    obj_description = get_class_description(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label), str(obj_description)])\n",
    "                for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    obj_description = get_class_description(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_description(g, URIRef(relation[1]))\n",
    "                object_description = get_class_description(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 2\n",
    "iteration_count = 0\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "# Filter and save class data\n",
    "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Class URI\", \"Label\", \"Description\"])\n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "# Filter and save class relations\n",
    "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
    "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\", \"Object Description\"])\n",
    "    writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, Graph(), writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_description(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return descriptions[0] if descriptions else None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g = Graph()\n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "        description = get_class_description(g, cls)\n",
    "        for label in labels:\n",
    "            normalized_label = normalize_string(label)\n",
    "            found_class_labels.add(normalized_label)\n",
    "            if normalized_label in normalized_class_names_to_check:\n",
    "                data.append([file_path, str(cls), str(label), str(description)])\n",
    "                for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    obj_description = get_class_description(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    obj_description = get_class_description(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label), str(obj_description)])\n",
    "                for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                    obj_label = get_class_label(g, obj)\n",
    "                    obj_description = get_class_description(g, obj)\n",
    "                    relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_description(g, URIRef(relation[1]))\n",
    "                object_description = get_class_description(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 2\n",
    "iteration_count = 0\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "# Filter and save class data\n",
    "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Class URI\", \"Label\", \"Description\"])\n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "# Filter and save class relations\n",
    "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
    "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\", \"Object Description\"])\n",
    "    writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, Graph(), writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### removing blank nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_description(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return descriptions[0] if descriptions else None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g = Graph()\n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_description(g, cls)\n",
    "            for label in labels:\n",
    "                normalized_label = normalize_string(label)\n",
    "                found_class_labels.add(normalized_label)\n",
    "                if normalized_label in normalized_class_names_to_check:\n",
    "                    data.append([file_path, str(cls), str(label), str(description)])\n",
    "                    for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                        if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_description(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_description(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_description(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_description(g, URIRef(relation[1]))\n",
    "                object_description = get_class_description(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 2\n",
    "iteration_count = 0\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "# Filter and save class data\n",
    "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Class URI\", \"Label\", \"Description\"])\n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "# Filter and save class relations\n",
    "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
    "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\", \"Object Description\"])\n",
    "    writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, Graph(), writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_description(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return \" \".join([str(desc) for desc in descriptions]) if descriptions else None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g = Graph()\n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_description(g, cls)\n",
    "            for label in labels:\n",
    "                normalized_label = normalize_string(label)\n",
    "                found_class_labels.add(normalized_label)\n",
    "                if normalized_label in normalized_class_names_to_check:\n",
    "                    data.append([file_path, str(cls), str(label), str(description)])\n",
    "                    for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                        if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_description(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_description(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_description(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "# def print_hierarchy(class_name, relations, g, writer):\n",
    "#     def recursive_print(class_name, depth=0):\n",
    "#         for relation in relations:\n",
    "#             if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "#                 subject_description = get_class_description(g, URIRef(relation[1]))\n",
    "#                 object_description = get_class_description(g, URIRef(relation[4]))\n",
    "#                 writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "#                 indent = '  ' * depth\n",
    "#                 print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "#                 recursive_print(relation[5], depth + 1)\n",
    "\n",
    "#     recursive_print(class_name)\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_description(g, URIRef(relation[1]))\n",
    "                object_description = get_class_description(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "    recursive_print(class_name)\n",
    "\n",
    "    \n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "# max_iterations = 2\n",
    "# iteration_count = 0\n",
    "\n",
    "max_iterations = 2\n",
    "iteration_count = 0\n",
    "g = Graph()  # Initialize the RDF graph here\n",
    "\n",
    "# while class_names_to_check and iteration_count < max_iterations:\n",
    "#     iteration_count += 1\n",
    "#     new_data = []\n",
    "#     new_relations = []\n",
    "#     new_found_class_labels = set()\n",
    "\n",
    "#     for ontology_file in ontology_files:\n",
    "#         file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
    "#         new_data.extend(file_data)\n",
    "#         new_relations.extend(file_relations)\n",
    "#         new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "#     all_data.extend(new_data)\n",
    "#     all_relations.extend(new_relations)\n",
    "#     all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "#     class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "\n",
    "#     class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "\n",
    "# Filter and save class data\n",
    "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Class URI\", \"Label\", \"Description\"])\n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "# Filter and save class relations\n",
    "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
    "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\", \"Object Description\"])\n",
    "    writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "# print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "# with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "#     for class_name in initial_class_names_to_check:\n",
    "#         normalized_class_name = normalize_string(class_name)\n",
    "#         print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "\n",
    "# print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n",
    "\n",
    "\n",
    "# Save class hierarchy to CSV file\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Finally fixed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_descriptions(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return \" \".join([str(desc) for desc in descriptions]) if descriptions else None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g = Graph()\n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_descriptions(g, cls)\n",
    "            for label in labels:\n",
    "                normalized_label = normalize_string(label)\n",
    "                found_class_labels.add(normalized_label)\n",
    "                if normalized_label in normalized_class_names_to_check:\n",
    "                    data.append([file_path, str(cls), str(label), str(description)])\n",
    "                    for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                        if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels, g\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_descriptions(g, URIRef(relation[1]))\n",
    "                object_description = get_class_descriptions(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 2\n",
    "iteration_count = 0\n",
    "graph = None\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels, g = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "    graph = g  # Ensure we keep the last loaded graph for hierarchy printing\n",
    "\n",
    "# Filter and save class data\n",
    "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Class URI\", \"Label\", \"Description\"])\n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "# Filter and save class relations\n",
    "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
    "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\", \"Object Description\"])\n",
    "    writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels, _ = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, graph, writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### above code works only for single input file to capture descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### here I try to fix the above isuue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces (unchanged)\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_descriptions(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return \" \".join([str(desc) for desc in descriptions]) if descriptions else None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check, g):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_descriptions(g, cls)\n",
    "            for label in labels:\n",
    "                normalized_label = normalize_string(label)\n",
    "                found_class_labels.add(normalized_label)\n",
    "                if normalized_label in normalized_class_names_to_check:\n",
    "                    data.append([file_path, str(cls), str(label), str(description)])\n",
    "                    for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                        if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_descriptions(g, URIRef(relation[1]))\n",
    "                object_description = get_class_descriptions(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 2\n",
    "iteration_count = 0\n",
    "g = Graph()\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check, g)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "# Filter and save class data\n",
    "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Class URI\", \"Label\", \"Description\"])\n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "# Filter and save class relations\n",
    "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
    "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\", \"Object Description\"])\n",
    "    writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check, g)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### above code get classes several times and iterate several times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here the issue is solved :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_descriptions(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return \" \".join([str(desc) for desc in descriptions]) if descriptions else None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check, g, processed_classes, processed_relations):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_descriptions(g, cls)\n",
    "            for label in labels:\n",
    "                normalized_label = normalize_string(label)\n",
    "                found_class_labels.add(normalized_label)\n",
    "                if normalized_label in normalized_class_names_to_check:\n",
    "                    # Check if already processed in this iteration\n",
    "                    if str(cls) not in processed_classes:\n",
    "                        processed_classes.add(str(cls))\n",
    "                        data.append([file_path, str(cls), str(label), str(description)])\n",
    "                    for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                        if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'subClassOf', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'subClassOf', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'isPartOf', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'isPartOf', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_descriptions(g, URIRef(relation[1]))\n",
    "                object_description = get_class_descriptions(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 2\n",
    "iteration_count = 0\n",
    "g = Graph()\n",
    "processed_classes = set()\n",
    "processed_relations = set()\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check, g, processed_classes, processed_relations)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "# Filter and save class data\n",
    "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Class URI\", \"Label\", \"Description\"])\n",
    "    writer.writerows(filtered_data)\n",
    "\n",
    "# Filter and save class relations\n",
    "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
    "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\", \"Object Description\"])\n",
    "    writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check, g, processed_classes, processed_relations)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_descriptions(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return \" \".join([str(desc) for desc in descriptions]) if descriptions else None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check, g, processed_classes, processed_relations):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_descriptions(g, cls)\n",
    "            for label in labels:\n",
    "                normalized_label = normalize_string(label)\n",
    "                found_class_labels.add(normalized_label)\n",
    "                if normalized_label in normalized_class_names_to_check:\n",
    "                    # Check if already processed in this iteration\n",
    "                    if str(cls) not in processed_classes:\n",
    "                        processed_classes.add(str(cls))\n",
    "                        data.append([file_path, str(cls), str(label), str(description)])\n",
    "                    for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                        if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'subClassOf', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'subClassOf', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'isPartOf', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'isPartOf', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_descriptions(g, URIRef(relation[1]))\n",
    "                object_description = get_class_descriptions(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 2\n",
    "iteration_count = 0\n",
    "g = Graph()\n",
    "processed_classes = set()\n",
    "processed_relations = set()\n",
    "last_class_name_written = None  # Track the last class name written\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check, g, processed_classes, processed_relations)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "    # Filter and save class data\n",
    "    filtered_data = [row for row in new_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "    with open(class_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_data:\n",
    "            current_class_name = filtered_data[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_data)\n",
    "\n",
    "    # Filter and save class relations\n",
    "    filtered_relations = filter_relations(new_relations, initial_class_names_to_check)\n",
    "    with open(relations_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_relations:\n",
    "            current_class_name = filtered_relations[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check, g, processed_classes, processed_relations)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### extra feature:   adding -----------  after finding a new matched class to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_class_data(file, data, initial_class_names_to_check):\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"File\", \"Class URI\", \"Label\", \"Description\"])\n",
    "#     last_class_name = None\n",
    "#     for row in data:\n",
    "#         class_name = row[2]\n",
    "#         normalized_class_name = normalize_string(class_name)\n",
    "#         if last_class_name is None or normalized_class_name in [normalize_string(name) for name in initial_class_names_to_check]:\n",
    "#             if last_class_name is not None:\n",
    "#                 writer.writerow([\"------\", \"------\", \"------\", \"------\"])  # Insert separator\n",
    "#             last_class_name = normalized_class_name\n",
    "#         writer.writerow(row)\n",
    "\n",
    "# def write_relations_data(file, relations, initial_class_names_to_check):\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\", \"Object Description\"])\n",
    "#     last_class_name = None\n",
    "#     for relation in relations:\n",
    "#         subject_label = relation[2]\n",
    "#         object_label = relation[5]\n",
    "#         if last_class_name is None or normalize_string(subject_label) in [normalize_string(name) for name in initial_class_names_to_check]:\n",
    "#             if last_class_name is not None:\n",
    "#                 writer.writerow([\"------\", \"------\", \"------\", \"------\", \"------\", \"------\", \"------\"])  # Insert separator\n",
    "#             last_class_name = normalize_string(subject_label)\n",
    "#         writer.writerow(relation)\n",
    "\n",
    "# def write_hierarchy_data(file, initial_class_names_to_check, all_relations, g):\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "#     last_class_name = None\n",
    "#     for class_name in initial_class_names_to_check:\n",
    "#         normalized_class_name = normalize_string(class_name)\n",
    "#         if last_class_name is None or normalized_class_name in [normalize_string(name) for name in initial_class_names_to_check]:\n",
    "#             if last_class_name is not None:\n",
    "#                 writer.writerow([\"------\", \"------\", \"------\", \"------\", \"------\", \"------\", \"------\", \"------\"])  # Insert separator\n",
    "#             last_class_name = normalized_class_name\n",
    "#         print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "\n",
    "# # Save class data\n",
    "# filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "# with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     write_class_data(file, filtered_data, initial_class_names_to_check)\n",
    "\n",
    "# # Save class relations\n",
    "# filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
    "# with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     write_relations_data(file, filtered_relations, initial_class_names_to_check)\n",
    "\n",
    "# # Save class hierarchy\n",
    "# with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     write_hierarchy_data(file, initial_class_names_to_check, all_relations, g)\n",
    "\n",
    "# print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "# print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "# print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Find the childeren!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_descriptions(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return \" \".join([str(desc) for desc in descriptions]) if descriptions else None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check, g, processed_classes, processed_relations):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_descriptions(g, cls)\n",
    "            for label in labels:\n",
    "                normalized_label = normalize_string(label)\n",
    "                found_class_labels.add(normalized_label)\n",
    "                if normalized_label in normalized_class_names_to_check:\n",
    "                    # Check if already processed in this iteration\n",
    "                    if str(cls) not in processed_classes:\n",
    "                        processed_classes.add(str(cls))\n",
    "                        data.append([file_path, str(cls), str(label), str(description)])\n",
    "                    for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                        if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'subClassOf', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'subClassOf', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label), str(obj_description)])\n",
    "                        elif (obj, RDF.type, OWL.Restriction) in g:\n",
    "                            # Handle the case where equivalentClass is a Restriction\n",
    "                            on_property = list(g.objects(obj, OWL.onProperty))\n",
    "                            some_values_from = list(g.objects(obj, OWL.someValuesFrom))\n",
    "                            all_values_from = list(g.objects(obj, OWL.allValuesFrom))\n",
    "                            if on_property and (some_values_from or all_values_from):\n",
    "                                on_property_label = get_class_label(g, on_property[0]) if on_property else None\n",
    "                                some_values_from_label = get_class_label(g, some_values_from[0]) if some_values_from else None\n",
    "                                all_values_from_label = get_class_label(g, all_values_from[0]) if all_values_from else None\n",
    "                                # Check if already processed in this iteration\n",
    "                                if (str(cls), 'equivalentClass', str(on_property[0])) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'equivalentClass', str(on_property[0])))\n",
    "                                    relations.append([\n",
    "                                        file_path, str(cls), str(label), 'equivalentClass', \n",
    "                                        f\"Restriction on {on_property[0]} (onProperty: {on_property_label}, someValuesFrom: {some_values_from_label}, allValuesFrom: {all_values_from_label})\", \n",
    "                                        \"\", \"\"\n",
    "                                    ])\n",
    "                    for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'isPartOf', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'isPartOf', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_descriptions(g, URIRef(relation[1]))\n",
    "                object_description = get_class_descriptions(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 3\n",
    "iteration_count = 0\n",
    "g = Graph()\n",
    "processed_classes = set()\n",
    "processed_relations = set()\n",
    "last_class_name_written = None  # Track the last class name written\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check, g, processed_classes, processed_relations)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "    # Filter and save class data\n",
    "    filtered_data = [row for row in new_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "    with open(class_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_data:\n",
    "            current_class_name = filtered_data[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_data)\n",
    "\n",
    "    # Filter and save class relations\n",
    "    filtered_relations = filter_relations(new_relations, initial_class_names_to_check)\n",
    "    with open(relations_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_relations:\n",
    "            current_class_name = filtered_relations[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check, g, processed_classes, processed_relations)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered class data has been saved to ontology_classes.csv\n",
      "Filtered class relations have been saved to ontology_relations.csv\n",
      "\n",
      "Initial class names found in the output:\n",
      "Class 'Tensiletest' found in:\n",
      "- ../Ontologies/materialsmine_converted.ttl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restriction on https://w3id.org/pmd/co/participant (onProperty: has participant, someValuesFrom: Tensile Testing Machine, allValuesFrom: None) does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ../Ontologies/pmdco_core.ttl\n",
      "\n",
      "Saving class hierarchy to CSV file:\n",
      "https://w3id.org/pmd/co/TensileTest Tensile Test is subClassOf https://w3id.org/pmd/co/MechanicalTestingProcess Mechanical Testing Process (from ../Ontologies/pmdco_core.ttl)\n",
      "  https://w3id.org/pmd/co/MechanicalTestingProcess Mechanical Testing Process is subClassOf https://w3id.org/pmd/co/AnalysingProcess Analyseprozess (from ../Ontologies/materialsmine_converted.ttl)\n",
      "    https://w3id.org/pmd/co/AnalysingProcess Analyseprozess is subClassOf https://w3id.org/pmd/co/Process Process (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      https://w3id.org/pmd/co/Process Process is subClassOf http://www.w3.org/ns/prov#Activity None (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      http://semanticscience.org/resource/Process process is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing None (from ../Ontologies/materialsmine_converted.ttl)\n",
      "https://w3id.org/pmd/co/TensileTest Tensile Test is equivalentClass Restriction on https://w3id.org/pmd/co/participant (onProperty: has participant, someValuesFrom: Tensile Testing Machine, allValuesFrom: None)  (from ../Ontologies/pmdco_core.ttl)\n",
      "Class hierarchy has been saved to class_hierarchy.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_descriptions(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return \" \".join([str(desc) for desc in descriptions]) if descriptions else None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check, g, processed_classes, processed_relations):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_descriptions(g, cls)\n",
    "            for label in labels:\n",
    "                normalized_label = normalize_string(label)\n",
    "                found_class_labels.add(normalized_label)\n",
    "                if normalized_label in normalized_class_names_to_check:\n",
    "                    # Check if already processed in this iteration\n",
    "                    if str(cls) not in processed_classes:\n",
    "                        processed_classes.add(str(cls))\n",
    "                        data.append([file_path, str(cls), str(label), str(description)])\n",
    "                    for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                        if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'subClassOf', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'subClassOf', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label), str(obj_description)])\n",
    "                        elif (obj, RDF.type, OWL.Restriction) in g:\n",
    "                            # Handle the case where equivalentClass is a Restriction\n",
    "                            on_property = list(g.objects(obj, OWL.onProperty))\n",
    "                            some_values_from = list(g.objects(obj, OWL.someValuesFrom))\n",
    "                            all_values_from = list(g.objects(obj, OWL.allValuesFrom))\n",
    "                            if on_property and (some_values_from or all_values_from):\n",
    "                                on_property_label = get_class_label(g, on_property[0]) if on_property else None\n",
    "                                some_values_from_label = get_class_label(g, some_values_from[0]) if some_values_from else None\n",
    "                                all_values_from_label = get_class_label(g, all_values_from[0]) if all_values_from else None\n",
    "                                # Check if already processed in this iteration\n",
    "                                if (str(cls), 'equivalentClass', str(on_property[0])) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'equivalentClass', str(on_property[0])))\n",
    "                                    relations.append([\n",
    "                                        file_path, str(cls), str(label), 'equivalentClass', \n",
    "                                        f\"Restriction on {on_property[0]} (onProperty: {on_property_label}, someValuesFrom: {some_values_from_label}, allValuesFrom: {all_values_from_label})\", \n",
    "                                        \"\", \"\"\n",
    "                                    ])\n",
    "                    for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'isPartOf', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'isPartOf', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_descriptions(g, URIRef(relation[1]))\n",
    "                object_description = get_class_descriptions(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 3\n",
    "iteration_count = 0\n",
    "g = Graph()\n",
    "processed_classes = set()\n",
    "processed_relations = set()\n",
    "last_class_name_written = None  # Track the last class name written\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check, g, processed_classes, processed_relations)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "    # Filter and save class data\n",
    "    filtered_data = [row for row in new_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "    with open(class_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_data:\n",
    "            current_class_name = filtered_data[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_data)\n",
    "\n",
    "    # Filter and save class relations\n",
    "    filtered_relations = filter_relations(new_relations, initial_class_names_to_check)\n",
    "    with open(relations_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_relations:\n",
    "            current_class_name = filtered_relations[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check, g, processed_classes, processed_relations)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered class data has been saved to ontology_classes.csv\n",
      "Filtered class relations have been saved to ontology_relations.csv\n",
      "\n",
      "Initial class names found in the output:\n",
      "Class 'Tensiletest' found in:\n",
      "- ../Ontologies/materialsmine_converted.ttl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restriction (onProperty: https://w3id.org/pmd/co/participant, someValuesFrom: https://w3id.org/pmd/co/TensileTestingMachine, allValuesFrom: None) does not look like a valid URI, trying to serialize this will break.\n",
      "Restriction (onProperty: https://w3id.org/pmd/co/participant, someValuesFrom: https://w3id.org/pmd/co/TestPiece, allValuesFrom: None) does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ../Ontologies/pmdco_core.ttl\n",
      "\n",
      "Saving class hierarchy to CSV file:\n",
      "https://w3id.org/pmd/co/TensileTest Tensile Test is subClassOf https://w3id.org/pmd/co/MechanicalTestingProcess Mechanical Testing Process (from ../Ontologies/pmdco_core.ttl)\n",
      "  https://w3id.org/pmd/co/MechanicalTestingProcess Mechanical Testing Process is subClassOf https://w3id.org/pmd/co/AnalysingProcess Analyseprozess (from ../Ontologies/materialsmine_converted.ttl)\n",
      "    https://w3id.org/pmd/co/AnalysingProcess Analyseprozess is subClassOf https://w3id.org/pmd/co/Process Process (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      https://w3id.org/pmd/co/Process Process is subClassOf http://www.w3.org/ns/prov#Activity None (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      http://semanticscience.org/resource/Process process is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing None (from ../Ontologies/materialsmine_converted.ttl)\n",
      "https://w3id.org/pmd/co/TensileTest Tensile Test is equivalentClass Restriction (onProperty: https://w3id.org/pmd/co/participant, someValuesFrom: https://w3id.org/pmd/co/TensileTestingMachine, allValuesFrom: None) None (from ../Ontologies/pmdco_core.ttl)\n",
      "https://w3id.org/pmd/co/TensileTest Tensile Test is equivalentClass Restriction (onProperty: https://w3id.org/pmd/co/participant, someValuesFrom: https://w3id.org/pmd/co/TestPiece, allValuesFrom: None) None (from ../Ontologies/pmdco_core.ttl)\n",
      "Class hierarchy has been saved to class_hierarchy.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal, BNode\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    if s is None:\n",
    "        return ''\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_descriptions(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return \" \".join([str(desc) for desc in descriptions]) if descriptions else None\n",
    "\n",
    "def parse_equivalent_class(g, equivalent_class):\n",
    "    if isinstance(equivalent_class, BNode):\n",
    "        if (equivalent_class, RDF.type, OWL.Restriction) in g:\n",
    "            on_property = g.value(equivalent_class, OWL.onProperty)\n",
    "            some_values_from = g.value(equivalent_class, OWL.someValuesFrom)\n",
    "            all_values_from = g.value(equivalent_class, OWL.allValuesFrom)\n",
    "            return f\"Restriction (onProperty: {on_property}, someValuesFrom: {some_values_from}, allValuesFrom: {all_values_from})\"\n",
    "        elif (equivalent_class, RDF.type, OWL.Class) in g:\n",
    "            union_of = g.value(equivalent_class, OWL.unionOf)\n",
    "            if union_of:\n",
    "                union_classes = list(union_of)\n",
    "                return f\"Union of: ({', '.join(str(cls) for cls in union_classes)})\"\n",
    "    else:\n",
    "        return str(equivalent_class)\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check, g, processed_classes, processed_relations):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_descriptions(g, cls)\n",
    "            for label in labels:\n",
    "                normalized_label = normalize_string(label)\n",
    "                found_class_labels.add(normalized_label)\n",
    "                if normalized_label in normalized_class_names_to_check:\n",
    "                    # Check if already processed in this iteration\n",
    "                    if str(cls) not in processed_classes:\n",
    "                        processed_classes.add(str(cls))\n",
    "                        data.append([file_path, str(cls), str(label), str(description)])\n",
    "                    for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                        if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'subClassOf', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'subClassOf', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label), str(obj_description)])\n",
    "                    for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                        eq_class_desc = parse_equivalent_class(g, obj)\n",
    "                        obj_label = get_class_label(g, obj) if isinstance(obj, URIRef) else None\n",
    "                        obj_description = get_class_descriptions(g, obj) if isinstance(obj, URIRef) else None\n",
    "                        # Check if already processed in this iteration\n",
    "                        if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                            processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                            relations.append([file_path, str(cls), str(label), 'equivalentClass', eq_class_desc, obj_label, obj_description])\n",
    "                    for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                        if isinstance(obj, URIRef):\n",
    "                            obj_label = get_class_label(g, obj)\n",
    "                            obj_description = get_class_descriptions(g, obj)\n",
    "                            # Check if already processed in this iteration\n",
    "                            if (str(cls), 'isPartOf', str(obj)) not in processed_relations:\n",
    "                                processed_relations.add((str(cls), 'isPartOf', str(obj)))\n",
    "                                relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label), str(obj_description)])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_descriptions(g, URIRef(relation[1]))\n",
    "                object_description = get_class_descriptions(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description, relation[3], relation[4], relation[5], object_description, relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 3\n",
    "iteration_count = 0\n",
    "g = Graph()\n",
    "processed_classes = set()\n",
    "processed_relations = set()\n",
    "last_class_name_written = None  # Track the last class name written\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check, g, processed_classes, processed_relations)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "    # Filter and save class data\n",
    "    filtered_data = [row for row in new_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "    with open(class_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_data:\n",
    "            current_class_name = filtered_data[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_data)\n",
    "\n",
    "    # Filter and save class relations\n",
    "    filtered_relations = filter_relations(new_relations, initial_class_names_to_check)\n",
    "    with open(relations_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_relations:\n",
    "            current_class_name = filtered_relations[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check, g, processed_classes, processed_relations)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered class data has been saved to ontology_classes.csv\n",
      "Filtered class relations have been saved to ontology_relations.csv\n",
      "\n",
      "Initial class names found in the output:\n",
      "Class 'Tensiletest' found in:\n",
      "- ../Ontologies/materialsmine_converted.ttl\n",
      "- ../Ontologies/pmdco_core.ttl\n",
      "Class 'biochemicalreaction' found in:\n",
      "- ../Ontologies/materialsmine_converted.ttl\n",
      "- ../Ontologies/pmdco_core.ttl\n",
      "\n",
      "Saving class hierarchy to CSV file:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Complex class expression does not look like a valid URI, trying to serialize this will break.\n",
      "Complex class expression does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://w3id.org/pmd/co/TensileTest Tensile Test is subClassOf https://w3id.org/pmd/co/MechanicalTestingProcess Mechanical Testing Process (from ../Ontologies/pmdco_core.ttl)\n",
      "  https://w3id.org/pmd/co/MechanicalTestingProcess Mechanical Testing Process is subClassOf https://w3id.org/pmd/co/AnalysingProcess Analyseprozess (from ../Ontologies/materialsmine_converted.ttl)\n",
      "    https://w3id.org/pmd/co/AnalysingProcess Analyseprozess is subClassOf https://w3id.org/pmd/co/Process Process (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      https://w3id.org/pmd/co/Process Process is subClassOf http://www.w3.org/ns/prov#Activity  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      http://semanticscience.org/resource/Process process is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "https://w3id.org/pmd/co/TensileTest Tensile Test is equivalentClass Complex class expression Restriction on has participant some Tensile Testing Machine (from ../Ontologies/pmdco_core.ttl)\n",
      "https://w3id.org/pmd/co/TensileTest Tensile Test is equivalentClass Complex class expression Restriction on has participant some Prfkrper (from ../Ontologies/pmdco_core.ttl)\n",
      "http://semanticscience.org/resource/BiochemicalReaction biochemical reaction is subClassOf http://semanticscience.org/resource/CatalyzedReaction catalyzed reaction (from ../Ontologies/materialsmine_converted.ttl)\n",
      "  http://semanticscience.org/resource/CatalyzedReaction catalyzed reaction is subClassOf http://semanticscience.org/resource/ChemicalReaction chemical reaction (from ../Ontologies/materialsmine_converted.ttl)\n",
      "    http://semanticscience.org/resource/ChemicalReaction chemical reaction is subClassOf http://semanticscience.org/resource/ChemicalInteraction chemical interaction (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      http://semanticscience.org/resource/ChemicalInteraction chemical interaction is subClassOf http://semanticscience.org/resource/Interacting interacting (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Interacting interacting is subClassOf http://semanticscience.org/resource/Process process (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          https://w3id.org/pmd/co/Process Process is subClassOf http://www.w3.org/ns/prov#Activity  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          http://semanticscience.org/resource/Process process is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
      "            http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "Class hierarchy has been saved to class_hierarchy.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_descriptions(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return \" \".join([str(desc) for desc in descriptions if desc is not None]) if descriptions else None\n",
    "\n",
    "def get_complex_expression_label(g, node):\n",
    "    if (node, RDF.type, OWL.Restriction) in g:\n",
    "        prop = list(g.objects(node, OWL.onProperty))\n",
    "        val = list(g.objects(node, OWL.someValuesFrom))\n",
    "        if prop and val:\n",
    "            prop_label = get_class_label(g, prop[0])\n",
    "            val_label = get_class_label(g, val[0])\n",
    "            return f\"Restriction on {prop_label} some {val_label}\"\n",
    "    elif (node, RDF.type, OWL.Class) in g:\n",
    "        union = list(g.objects(node, OWL.unionOf))\n",
    "        if union:\n",
    "            members = list(g.items(union[0]))\n",
    "            labels = [get_class_label(g, m) for m in members if get_class_label(g, m) is not None]\n",
    "            return f\"Union of {' and '.join(labels)}\"\n",
    "    return None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check, g, processed_classes, processed_relations):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_descriptions(g, cls)\n",
    "            for label in labels:\n",
    "                if label is not None:\n",
    "                    normalized_label = normalize_string(label)\n",
    "                    found_class_labels.add(normalized_label)\n",
    "                    if normalized_label in normalized_class_names_to_check:\n",
    "                        # Check if already processed in this iteration\n",
    "                        if str(cls) not in processed_classes:\n",
    "                            processed_classes.add(str(cls))\n",
    "                            data.append([file_path, str(cls), str(label), str(description) if description is not None else \"\"])\n",
    "                        for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                            if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                                obj_label = get_class_label(g, obj)\n",
    "                                obj_description = get_class_descriptions(g, obj)\n",
    "                                # Check if already processed in this iteration\n",
    "                                if (str(cls), 'subClassOf', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'subClassOf', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "                        for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                            if isinstance(obj, URIRef):\n",
    "                                obj_label = get_class_label(g, obj)\n",
    "                                obj_description = get_class_descriptions(g, obj)\n",
    "                                # Check if already processed in this iteration\n",
    "                                if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "                            else:\n",
    "                                # Handle blank nodes for equivalentClass\n",
    "                                obj_label = get_complex_expression_label(g, obj)\n",
    "                                obj_description = \"Complex class expression\"\n",
    "                                if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'equivalentClass', \"Complex class expression\", str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "                        for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                            if isinstance(obj, URIRef):\n",
    "                                obj_label = get_class_label(g, obj)\n",
    "                                obj_description = get_class_descriptions(g, obj)\n",
    "                                # Check if already processed in this iteration\n",
    "                                if (str(cls), 'isPartOf', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'isPartOf', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_descriptions(g, URIRef(relation[1]))\n",
    "                object_description = get_class_descriptions(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description if subject_description is not None else \"\", relation[3], relation[4], relation[5], object_description if object_description is not None else \"\", relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 3\n",
    "iteration_count = 0\n",
    "g = Graph()\n",
    "processed_classes = set()\n",
    "processed_relations = set()\n",
    "last_class_name_written = None  # Track the last class name written\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check, g, processed_classes, processed_relations)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "    # Filter and save class data\n",
    "    filtered_data = [row for row in new_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "    with open(class_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_data:\n",
    "            current_class_name = filtered_data[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_data)\n",
    "\n",
    "    # Filter and save class relations\n",
    "    filtered_relations = filter_relations(new_relations, initial_class_names_to_check)\n",
    "    with open(relations_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_relations:\n",
    "            current_class_name = filtered_relations[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check, g, processed_classes, processed_relations)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### funtion to capture intersectionof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered class data has been saved to ontology_classes.csv\n",
      "Filtered class relations have been saved to ontology_relations.csv\n",
      "\n",
      "Initial class names found in the output:\n",
      "Class 'biochemicalreaction' found in:\n",
      "- ../Ontologies/materialsmine_converted.ttl\n",
      "- ../Ontologies/pmdco_core.ttl\n",
      "\n",
      "Saving class hierarchy to CSV file:\n",
      "http://semanticscience.org/resource/BiochemicalReaction biochemical reaction is subClassOf http://semanticscience.org/resource/CatalyzedReaction catalyzed reaction (from ../Ontologies/materialsmine_converted.ttl)\n",
      "  http://semanticscience.org/resource/CatalyzedReaction catalyzed reaction is subClassOf http://semanticscience.org/resource/ChemicalReaction chemical reaction (from ../Ontologies/materialsmine_converted.ttl)\n",
      "    http://semanticscience.org/resource/ChemicalReaction chemical reaction is subClassOf http://semanticscience.org/resource/ChemicalInteraction chemical interaction (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      http://semanticscience.org/resource/ChemicalInteraction chemical interaction is subClassOf http://semanticscience.org/resource/Interacting interacting (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Interacting interacting is subClassOf http://semanticscience.org/resource/Process process (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          https://w3id.org/pmd/co/Process Process is subClassOf http://www.w3.org/ns/prov#Activity  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          http://semanticscience.org/resource/Process process is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
      "            http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "Class hierarchy has been saved to class_hierarchy.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal, BNode\n",
    "\n",
    "\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "\n",
    "# Define namespaces (assuming these are already defined in your script)\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_descriptions(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return \" \".join([str(desc) for desc in descriptions if desc is not None]) if descriptions else None\n",
    "\n",
    "def get_complex_expression_label(g, node):\n",
    "    if (node, RDF.type, OWL.Restriction) in g:\n",
    "        prop = list(g.objects(node, OWL.onProperty))\n",
    "        val = list(g.objects(node, OWL.someValuesFrom))\n",
    "        if prop and val:\n",
    "            prop_label = get_class_label(g, prop[0])\n",
    "            val_label = get_class_label(g, val[0])\n",
    "            return f\"Restriction on {prop_label} some {val_label}\"\n",
    "    elif (node, RDF.type, OWL.Class) in g:\n",
    "        intersection = list(g.objects(node, OWL.intersectionOf))\n",
    "        if intersection:\n",
    "            components = []\n",
    "            for item in g.items(intersection[0]):\n",
    "                if isinstance(item, URIRef):\n",
    "                    component_label = get_class_label(g, item)\n",
    "                    if component_label:\n",
    "                        components.append(component_label)\n",
    "                elif isinstance(item, BNode):\n",
    "                    restriction_label = get_complex_expression_label(g, item)\n",
    "                    if restriction_label:\n",
    "                        components.append(restriction_label)\n",
    "            if components:\n",
    "                return f\"Intersection of {' and '.join(components)}\"\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check, g, processed_classes, processed_relations):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_descriptions(g, cls)\n",
    "            for label in labels:\n",
    "                if label is not None:\n",
    "                    normalized_label = normalize_string(label)\n",
    "                    found_class_labels.add(normalized_label)\n",
    "                    if normalized_label in normalized_class_names_to_check:\n",
    "                        # Check if already processed in this iteration\n",
    "                        if str(cls) not in processed_classes:\n",
    "                            processed_classes.add(str(cls))\n",
    "                            data.append([file_path, str(cls), str(label), str(description) if description is not None else \"\"])\n",
    "                        for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                            if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                                obj_label = get_class_label(g, obj)\n",
    "                                obj_description = get_class_descriptions(g, obj)\n",
    "                                # Check if already processed in this iteration\n",
    "                                if (str(cls), 'subClassOf', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'subClassOf', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "                        for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                            if isinstance(obj, URIRef):\n",
    "                                obj_label = get_class_label(g, obj)\n",
    "                                obj_description = get_class_descriptions(g, obj)\n",
    "                                # Check if already processed in this iteration\n",
    "                                if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "                            else:\n",
    "                                # Handle blank nodes for equivalentClass\n",
    "                                obj_label = get_complex_expression_label(g, obj)\n",
    "                                obj_description = \"Complex class expression\"\n",
    "                                if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'equivalentClass', \"Complex class expression\", str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "                        for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                            if isinstance(obj, URIRef):\n",
    "                                obj_label = get_class_label(g, obj)\n",
    "                                obj_description = get_class_descriptions(g, obj)\n",
    "                                # Check if already processed in this iteration\n",
    "                                if (str(cls), 'isPartOf', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'isPartOf', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "\n",
    "            # Check for owl:intersectionOf\n",
    "            intersections = list(g.objects(cls, OWL.intersectionOf))\n",
    "            if intersections:\n",
    "                for intersection in intersections:\n",
    "                    if isinstance(intersection, BNode):\n",
    "                        components = []\n",
    "                        for item in g.items(intersection):\n",
    "                            if isinstance(item, URIRef):\n",
    "                                component_label = get_class_label(g, item)\n",
    "                                if component_label:\n",
    "                                    components.append(component_label)\n",
    "                            elif isinstance(item, BNode):\n",
    "                                restriction_label = get_complex_expression_label(g, item)\n",
    "                                if restriction_label:\n",
    "                                    components.append(restriction_label)\n",
    "                        if components:\n",
    "                            data.append([file_path, str(cls), \"\", f\"Intersection of {' and '.join(components)}\"])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_descriptions(g, URIRef(relation[1]))\n",
    "                object_description = get_class_descriptions(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description if subject_description is not None else \"\", relation[3], relation[4], relation[5], object_description if object_description is not None else \"\", relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 3\n",
    "iteration_count = 0\n",
    "g = Graph()\n",
    "processed_classes = set()\n",
    "processed_relations = set()\n",
    "last_class_name_written = None  # Track the last class name written\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check, g, processed_classes, processed_relations)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "    # Filter and save class data\n",
    "    filtered_data = [row for row in new_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "    with open(class_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_data:\n",
    "            current_class_name = filtered_data[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_data)\n",
    "\n",
    "    # Filter and save class relations\n",
    "    filtered_relations = filter_relations(new_relations, initial_class_names_to_check)\n",
    "    with open(relations_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_relations:\n",
    "            current_class_name = filtered_relations[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check, g, processed_classes, processed_relations)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n",
    "\n",
    "def save_intersection_info_to_csv(intersection_data, output_file):\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"File\", \"Class URI\", \"Class Name\", \"Intersection Description\"])\n",
    "        writer.writerows(intersection_data)\n",
    "\n",
    "\n",
    "# Inside your main loop where you process ontology files:\n",
    "# Add after collecting data and relations\n",
    "intersection_data = []\n",
    "for file_path in ontology_files:\n",
    "    file_data, _, _ = load_and_collect_classes_and_relations(file_path, class_names_to_check, g, processed_classes, processed_relations)\n",
    "    intersection_data.extend(file_data)\n",
    "\n",
    "save_intersection_info_to_csv(intersection_data, \"intersection_info.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bnode(); not working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered class data has been saved to ontology_classes.csv\n",
      "Filtered class relations have been saved to ontology_relations.csv\n",
      "\n",
      "Initial class names found in the output:\n",
      "Class 'biochemicalreaction' found in:\n",
      "- ../Ontologies/materialsmine_converted.ttl\n",
      "- ../Ontologies/pmdco_core.ttl\n",
      "\n",
      "Saving class hierarchy to CSV file:\n",
      "http://semanticscience.org/resource/BiochemicalReaction biochemical reaction is subClassOf http://semanticscience.org/resource/CatalyzedReaction catalyzed reaction (from ../Ontologies/materialsmine_converted.ttl)\n",
      "  http://semanticscience.org/resource/CatalyzedReaction catalyzed reaction is subClassOf http://semanticscience.org/resource/ChemicalReaction chemical reaction (from ../Ontologies/materialsmine_converted.ttl)\n",
      "    http://semanticscience.org/resource/ChemicalReaction chemical reaction is subClassOf http://semanticscience.org/resource/ChemicalInteraction chemical interaction (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      http://semanticscience.org/resource/ChemicalInteraction chemical interaction is subClassOf http://semanticscience.org/resource/Interacting interacting (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Interacting interacting is subClassOf http://semanticscience.org/resource/Process process (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          https://w3id.org/pmd/co/Process Process is subClassOf http://www.w3.org/ns/prov#Activity  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          http://semanticscience.org/resource/Process process is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
      "            http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          http://semanticscience.org/resource/Process process is subClassOf n6ce9e33195794cbfa845367061420300b895  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          http://semanticscience.org/resource/Process process is subClassOf n6ce9e33195794cbfa845367061420300b896  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          http://semanticscience.org/resource/Process process is subClassOf n617319f4514b4f829d6f922d774d5733b895  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          http://semanticscience.org/resource/Process process is subClassOf n617319f4514b4f829d6f922d774d5733b896  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          http://semanticscience.org/resource/Process process is subClassOf ne310a4e8ff524b759bb4f4ec4eb74ffdb895  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "          http://semanticscience.org/resource/Process process is subClassOf ne310a4e8ff524b759bb4f4ec4eb74ffdb896  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Interacting interacting is subClassOf n6ce9e33195794cbfa845367061420300b618  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Interacting interacting is subClassOf n6ce9e33195794cbfa845367061420300b623  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Interacting interacting is subClassOf n617319f4514b4f829d6f922d774d5733b618  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Interacting interacting is subClassOf n617319f4514b4f829d6f922d774d5733b623  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Interacting interacting is subClassOf ne310a4e8ff524b759bb4f4ec4eb74ffdb618  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "        http://semanticscience.org/resource/Interacting interacting is subClassOf ne310a4e8ff524b759bb4f4ec4eb74ffdb623  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      http://semanticscience.org/resource/ChemicalInteraction chemical interaction is subClassOf n6ce9e33195794cbfa845367061420300b402  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      http://semanticscience.org/resource/ChemicalInteraction chemical interaction is subClassOf n617319f4514b4f829d6f922d774d5733b402  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "      http://semanticscience.org/resource/ChemicalInteraction chemical interaction is subClassOf ne310a4e8ff524b759bb4f4ec4eb74ffdb402  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "    http://semanticscience.org/resource/ChemicalReaction chemical reaction is subClassOf n6ce9e33195794cbfa845367061420300b403  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "    http://semanticscience.org/resource/ChemicalReaction chemical reaction is subClassOf n617319f4514b4f829d6f922d774d5733b403  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "    http://semanticscience.org/resource/ChemicalReaction chemical reaction is subClassOf ne310a4e8ff524b759bb4f4ec4eb74ffdb403  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "  http://semanticscience.org/resource/CatalyzedReaction catalyzed reaction is subClassOf n6ce9e33195794cbfa845367061420300b369  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "  http://semanticscience.org/resource/CatalyzedReaction catalyzed reaction is subClassOf n617319f4514b4f829d6f922d774d5733b369  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "  http://semanticscience.org/resource/CatalyzedReaction catalyzed reaction is subClassOf ne310a4e8ff524b759bb4f4ec4eb74ffdb369  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "http://semanticscience.org/resource/BiochemicalReaction biochemical reaction is subClassOf n6ce9e33195794cbfa845367061420300b317  (from ../Ontologies/materialsmine_converted.ttl)\n",
      "Class hierarchy has been saved to class_hierarchy.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal, BNode\n",
    "\n",
    "# Define namespaces\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "def get_class_label(g, cls):\n",
    "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "    return labels[0] if labels else None\n",
    "\n",
    "def get_class_descriptions(g, cls):\n",
    "    descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "    return \" \".join([str(desc) for desc in descriptions if desc is not None]) if descriptions else None\n",
    "\n",
    "def get_complex_expression_label(g, node):\n",
    "    if (node, RDF.type, OWL.Restriction) in g:\n",
    "        prop = list(g.objects(node, OWL.onProperty))\n",
    "        val = list(g.objects(node, OWL.someValuesFrom))\n",
    "        if prop and val:\n",
    "            prop_label = get_class_label(g, prop[0])\n",
    "            val_label = get_class_label(g, val[0])\n",
    "            return f\"Restriction on {prop_label} some {val_label}\"\n",
    "    elif (node, RDF.type, OWL.Class) in g:\n",
    "        union = list(g.objects(node, OWL.unionOf))\n",
    "        if union:\n",
    "            members = list(g.items(union[0]))\n",
    "            labels = [get_class_label(g, m) for m in members if get_class_label(g, m) is not None]\n",
    "            return f\"Union of {' and '.join(labels)}\"\n",
    "    return None\n",
    "\n",
    "def load_and_collect_classes_and_relations(file_path, class_names_to_check, g, processed_classes, processed_relations):\n",
    "    if file_path.endswith('.ttl'):\n",
    "        file_format = 'ttl'\n",
    "    elif file_path.endswith('.owl'):\n",
    "        file_format = 'xml'\n",
    "    elif file_path.endswith('.xrdf'):\n",
    "        file_format = 'xml'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "    \n",
    "    g.parse(file_path, format=file_format)\n",
    "\n",
    "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "    data = []\n",
    "    relations = []\n",
    "    found_class_labels = set()\n",
    "    for cls in classes:\n",
    "        if isinstance(cls, (URIRef, BNode)):  # Check if the subject is a URI or BNode\n",
    "            labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "            description = get_class_descriptions(g, cls)\n",
    "            for label in labels:\n",
    "                if label is not None:\n",
    "                    normalized_label = normalize_string(label)\n",
    "                    found_class_labels.add(normalized_label)\n",
    "                    if normalized_label in normalized_class_names_to_check:\n",
    "                        # Check if already processed in this iteration\n",
    "                        if str(cls) not in processed_classes:\n",
    "                            processed_classes.add(str(cls))\n",
    "                            data.append([file_path, str(cls), str(label), str(description) if description is not None else \"\"])\n",
    "                        for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                            if isinstance(obj, (URIRef, BNode)):  # Check if the object is a URI or BNode\n",
    "                                obj_label = get_class_label(g, obj)\n",
    "                                obj_description = get_class_descriptions(g, obj)\n",
    "                                # Check if already processed in this iteration\n",
    "                                if (str(cls), 'subClassOf', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'subClassOf', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "                        for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                            if isinstance(obj, (URIRef, BNode)):\n",
    "                                obj_label = get_class_label(g, obj)\n",
    "                                obj_description = get_class_descriptions(g, obj)\n",
    "                                # Check if already processed in this iteration\n",
    "                                if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "                            else:\n",
    "                                # Handle blank nodes for equivalentClass\n",
    "                                obj_label = get_complex_expression_label(g, obj)\n",
    "                                obj_description = \"Complex class expression\"\n",
    "                                if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'equivalentClass', \"Complex class expression\", str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "                        for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                            if isinstance(obj, (URIRef, BNode)):\n",
    "                                obj_label = get_class_label(g, obj)\n",
    "                                obj_description = get_class_descriptions(g, obj)\n",
    "                                # Check if already processed in this iteration\n",
    "                                if (str(cls), 'isPartOf', str(obj)) not in processed_relations:\n",
    "                                    processed_relations.add((str(cls), 'isPartOf', str(obj)))\n",
    "                                    relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "\n",
    "    return data, relations, found_class_labels\n",
    "\n",
    "def filter_relations(all_relations, initial_class_names_to_check):\n",
    "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
    "\n",
    "def print_hierarchy(class_name, relations, g, writer):\n",
    "    def recursive_print(class_name, depth=0):\n",
    "        for relation in relations:\n",
    "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
    "                subject_description = get_class_descriptions(g, URIRef(relation[1]))\n",
    "                object_description = get_class_descriptions(g, URIRef(relation[4]))\n",
    "                writer.writerow([relation[1], relation[2], subject_description if subject_description is not None else \"\", relation[3], relation[4], relation[5], object_description if object_description is not None else \"\", relation[0]])\n",
    "                indent = '  ' * depth\n",
    "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
    "                recursive_print(relation[5], depth + 1)\n",
    "\n",
    "    recursive_print(class_name)\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n",
    "all_data = []\n",
    "all_relations = []\n",
    "all_found_class_labels = set()\n",
    "\n",
    "class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "max_iterations = 3\n",
    "iteration_count = 0\n",
    "g = Graph()\n",
    "processed_classes = set()\n",
    "processed_relations = set()\n",
    "last_class_name_written = None  # Track the last class name written\n",
    "\n",
    "while class_names_to_check and iteration_count < max_iterations:\n",
    "    iteration_count += 1\n",
    "    new_data = []\n",
    "    new_relations = []\n",
    "    new_found_class_labels = set()\n",
    "\n",
    "    for ontology_file in ontology_files:\n",
    "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check, g, processed_classes, processed_relations)\n",
    "        new_data.extend(file_data)\n",
    "        new_relations.extend(file_relations)\n",
    "        new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "    all_data.extend(new_data)\n",
    "    all_relations.extend(new_relations)\n",
    "    all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "    # Filter and save class data\n",
    "    filtered_data = [row for row in new_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "    with open(class_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_data:\n",
    "            current_class_name = filtered_data[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_data)\n",
    "\n",
    "    # Filter and save class relations\n",
    "    filtered_relations = filter_relations(new_relations, initial_class_names_to_check)\n",
    "    with open(relations_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if filtered_relations:\n",
    "            current_class_name = filtered_relations[0][2]  # Get the class name from the first row\n",
    "            if current_class_name != last_class_name_written:\n",
    "                # writer.writerow(['------'])  # Write separator\n",
    "                last_class_name_written = current_class_name\n",
    "        writer.writerows(filtered_relations)\n",
    "\n",
    "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "print(\"\\nInitial class names found in the output:\")\n",
    "for class_name in initial_class_names_to_check:\n",
    "    normalized_class_name = normalize_string(class_name)\n",
    "    found = False\n",
    "    for label in all_found_class_labels:\n",
    "        if normalized_class_name in label:\n",
    "            found = True\n",
    "            print(f\"Class '{class_name}' found in:\")\n",
    "            for ontology_file in ontology_files:\n",
    "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check, g, processed_classes, processed_relations)\n",
    "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                if normalized_class_name in normalized_labels:\n",
    "                    print(f\"- {ontology_file}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "print(\"\\nSaving class hierarchy to CSV file:\")\n",
    "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Class URI\", \"Class Name\", \"Class Description\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\", \"Related Class Description\", \"File\"])\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        print_hierarchy(normalized_class_name, all_relations, g, writer)\n",
    "\n",
    "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Testing\n",
    "### getting more info from the ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class information saved to class_info.csv\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL, URIRef, BNode\n",
    "import csv\n",
    "\n",
    "# Define the namespaces\n",
    "SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "CO = Namespace(\"https://w3id.org/pmd/co/\")\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "\n",
    "# Initialize the graph\n",
    "g = Graph()\n",
    "\n",
    "# Parse the Turtle files\n",
    "ttl_data_1 = \"\"\"@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix dc11: <http://purl.org/dc/elements/1.1/> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix bibo: <http://purl.org/ontology/bibo/> .\n",
    "@prefix vann: <http://purl.org/vocab/vann/> .\n",
    "@prefix schema: <http://schema.org/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix vs: <http://www.w3.org/2003/06/sw-vocab-status/ns#> .\n",
    "@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\n",
    "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix unit: <http://qudt.org/vocab/unit/> .\n",
    "\n",
    "<http://semanticscience.org/resource/Biopolymer> a owl:Class ;\n",
    "\towl:equivalentClass [\n",
    "\t\trdf:type owl:Class ;\n",
    "\t\towl:intersectionOf (\n",
    "\t\t\t<http://semanticscience.org/resource/OrganicPolymer>\n",
    "\t\t\t[\n",
    "\t\t\t\trdf:type owl:Restriction ;\n",
    "\t\t\t\towl:onProperty <http://semanticscience.org/resource/hasDirectPart> ;\n",
    "\t\t\t\towl:someValuesFrom [\n",
    "\t\t\t\t\trdf:type owl:Class ;\n",
    "\t\t\t\t\towl:unionOf (\n",
    "\t\t\t\t\t\t<http://semanticscience.org/resource/AminoAcidResidue>\n",
    "\t\t\t\t\t\t<http://semanticscience.org/resource/CarbohydrateResidue>\n",
    "\t\t\t\t\t\t<http://semanticscience.org/resource/LipidResidue>\n",
    "\t\t\t\t\t\t<http://semanticscience.org/resource/NucleotideResidue>\n",
    "\t\t\t\t\t) ;\n",
    "\t\t\t\t] ;\n",
    "\t\t\t]\n",
    "\t\t) ;\n",
    "\t] ;\n",
    "\trdfs:subClassOf <http://semanticscience.org/resource/OrganicPolymer>, [\n",
    "\t\trdf:type owl:Restriction ;\n",
    "\t\towl:onProperty <http://semanticscience.org/resource/hasDirectPart> ;\n",
    "\t\towl:someValuesFrom [\n",
    "\t\t\trdf:type owl:Class ;\n",
    "\t\t\towl:unionOf (\n",
    "\t\t\t\t<http://semanticscience.org/resource/AminoAcidResidue>\n",
    "\t\t\t\t<http://semanticscience.org/resource/CarbohydrateResidue>\n",
    "\t\t\t\t<http://semanticscience.org/resource/LipidResidue>\n",
    "\t\t\t\t<http://semanticscience.org/resource/NucleotideResidue>\n",
    "\t\t\t) ;\n",
    "\t\t] ;\n",
    "\t] ;\n",
    "\t<http://data.bioontology.org/metadata/prefixIRI> \"sio:Biopolymer\" ;\n",
    "\tdcterms:description \"A biopolymer is an organic polymer that are typically produced by the cells of living organisms.\"@en ;\n",
    "\trdfs:isDefinedBy <http://semanticscience.org/ontology/sio/v1.53/sio-subset-labels.owl> ;\n",
    "\trdfs:label \"biopolymer\"@en .\n",
    "\"\"\"\n",
    "\n",
    "ttl_data_2 = \"\"\"@prefix : <https://w3id.org/pmd/co/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix xml: <http://www.w3.org/XML/1998/namespace> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@base <https://w3id.org/pmd/co/> .\n",
    "\n",
    ":ISO6892-1TensileTest rdf:type owl:Class ;\n",
    "                      owl:equivalentClass [ rdf:type owl:Restriction ;\n",
    "                                            owl:onProperty :participant ;\n",
    "                                            owl:someValuesFrom :Extensometer\n",
    "                                          ] ,\n",
    "                                          [ rdf:type owl:Restriction ;\n",
    "                                            owl:onProperty :participant ;\n",
    "                                            owl:someValuesFrom :LoadCell\n",
    "                                          ] ,\n",
    "                                          [ rdf:type owl:Restriction ;\n",
    "                                            owl:onProperty :participant ;\n",
    "                                            owl:someValuesFrom :TensileTestingMachine\n",
    "                                          ] ,\n",
    "                                          [ rdf:type owl:Restriction ;\n",
    "                                            owl:onProperty :participant ;\n",
    "                                            owl:someValuesFrom :TestPiece\n",
    "                                          ] ;\n",
    "                      rdfs:subClassOf :TensileTest ;\n",
    "                      rdfs:isDefinedBy <https://w3id.org/pmd/co> ;\n",
    "                      rdfs:label \"Tensile Test in accordance with ISO 6892-1\"@en ,\n",
    "                                 \"Zugversuch nach ISO 6892-1\"@de .\n",
    "\"\"\"\n",
    "\n",
    "g.parse(data=ttl_data_1, format='ttl')\n",
    "g.parse(data=ttl_data_2, format='ttl')\n",
    "\n",
    "# Helper functions to extract details\n",
    "def get_restriction_details(node):\n",
    "    details = {}\n",
    "    for p, o in g.predicate_objects(node):\n",
    "        if p == OWL.onProperty:\n",
    "            details['onProperty'] = get_readable_label(o)\n",
    "        elif p == OWL.someValuesFrom:\n",
    "            if isinstance(o, URIRef):\n",
    "                details['someValuesFrom'] = get_readable_label(o)\n",
    "            elif isinstance(o, BNode):\n",
    "                details['someValuesFrom'] = get_union_or_intersection(o)\n",
    "    return details\n",
    "\n",
    "def get_union_or_intersection(node):\n",
    "    items = []\n",
    "    for p, o in g.predicate_objects(node):\n",
    "        if p in (OWL.unionOf, OWL.intersectionOf):\n",
    "            for item in g.items(o):\n",
    "                items.append(get_readable_label(item))\n",
    "    return items\n",
    "\n",
    "# Function to get a readable label for a URI or BNode\n",
    "def get_readable_label(node):\n",
    "    label = g.value(subject=node, predicate=RDFS.label)\n",
    "    if label:\n",
    "        return str(label)\n",
    "    elif isinstance(node, URIRef):\n",
    "        return str(node)\n",
    "    else:\n",
    "        return str(node)\n",
    "\n",
    "# Extract information and save to a list of dictionaries\n",
    "class_info_list = []\n",
    "\n",
    "def extract_class_info(class_uri):\n",
    "    class_info = {\"Class\": get_readable_label(class_uri)}\n",
    "    \n",
    "    for _, p, o in g.triples((class_uri, None, None)):\n",
    "        if p == OWL.equivalentClass or p == RDFS.subClassOf:\n",
    "            if isinstance(o, BNode):\n",
    "                for _, p2, o2 in g.triples((o, None, None)):\n",
    "                    if p2 == OWL.intersectionOf or p2 == OWL.unionOf:\n",
    "                        members = [get_readable_label(member) for member in g.items(o2)]\n",
    "                        class_info[p2.split('#')[-1]] = ', '.join(members)\n",
    "                    elif p2 == RDF.type and o2 == OWL.Restriction:\n",
    "                        restriction_details = get_restriction_details(o)\n",
    "                        class_info['Restriction'] = ', '.join([f\"{k}: {v}\" for k, v in restriction_details.items()])\n",
    "                        \n",
    "                    if isinstance(o2, BNode):\n",
    "                        restriction_details = get_restriction_details(o2)\n",
    "                        class_info['Nested Restriction'] = ', '.join([f\"{k}: {v}\" for k, v in restriction_details.items()])\n",
    "        else:\n",
    "            class_info[p.split('#')[-1]] = get_readable_label(o)\n",
    "    \n",
    "    class_info_list.append(class_info)\n",
    "\n",
    "# Query for all classes in the graph\n",
    "for class_uri in g.subjects(RDF.type, OWL.Class):\n",
    "    extract_class_info(class_uri)\n",
    "\n",
    "# Determine fieldnames dynamically\n",
    "all_keys = set()\n",
    "for class_info in class_info_list:\n",
    "    all_keys.update(class_info.keys())\n",
    "\n",
    "fieldnames = list(all_keys)\n",
    "\n",
    "# Write the extracted information to a CSV file\n",
    "csv_file = \"class_info.csv\"\n",
    "\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for class_info in class_info_list:\n",
    "        writer.writerow(class_info)\n",
    "\n",
    "print(f\"Class information saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
