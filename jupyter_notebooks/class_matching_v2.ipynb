{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rdflib import Graph, RDF, RDFS, OWL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "g= Graph()\n",
        "ontology_file = '../Ontologies/materialsmine.ttl'\n",
        "g.parse(ontology_file, format='ttl')\n",
        "\n",
        "classes = set(g.subjects(RDF.type, RDFS.Class)).union(g.subjects(RDF.type, OWL.Class))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for cls in classes:\n",
        "    print (cls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for cls in classes:\n",
        "    # Query for labels\n",
        "    labels = list(g.objects(cls, RDFS.label))\n",
        "    if labels:\n",
        "        for label in labels:\n",
        "            print(f\"Class: {cls}, Label: {label}\")\n",
        "    else:\n",
        "        print(f\"Class: {cls}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rdflib import Graph, RDF, RDFS, OWL\n",
        "\n",
        "def load_and_print_classes(file_path, file_format=\"ttl\"):\n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, RDFS.Class)).union(g.subjects(RDF.type, OWL.Class))\n",
        "\n",
        "    # Print class URIs and labels if available\n",
        "    print(f\"\\nClasses in {file_path}:\")\n",
        "    for cls in classes:\n",
        "        # Query for labels\n",
        "        labels = list(g.objects(cls, RDFS.label))\n",
        "        if labels:\n",
        "            for label in labels:\n",
        "                print(f\"Class: {cls}, Label: {label}\")\n",
        "        else:\n",
        "            print(f\"Class: {cls}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    \"../Ontologies/materialsmine.ttl\",\n",
        "    \"../Ontologies/pmdco_core.ttl\",\n",
        "    \"../Ontologies/nfdicore_2.ttl\",\n",
        "    # \"../Ontologies/bfo.owl\",\n",
        "    # \"../Ontologies/emmo.ttl\",\n",
        "    # '../Ontologies/emmo.owl',\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from rdflib import Graph, RDF, RDFS, OWL\n",
        "\n",
        "def load_and_collect_classes(file_path, file_format=\"ttl\"):\n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, RDFS.Class)).union(g.subjects(RDF.type, OWL.Class))\n",
        "\n",
        "    # Collect class URIs and labels if available\n",
        "    data = []\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, RDFS.label))\n",
        "        if labels:\n",
        "            for label in labels:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "        else:\n",
        "            data.append([file_path, str(cls), \"\"])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data from all files\n",
        "all_data = []\n",
        "for ontology_file in ontology_files:\n",
        "    all_data.extend(load_and_collect_classes(ontology_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write data to CSV\n",
        "output_file = \"ontology_classes.csv\"\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(all_data)\n",
        "\n",
        "print(f\"Data has been saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from rdflib import Graph, RDF, RDFS, OWL\n",
        "\n",
        "def load_and_collect_classes(file_path):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, RDFS.Class)).union(g.subjects(RDF.type, OWL.Class))\n",
        "\n",
        "    # Collect class URIs and labels if available\n",
        "    data = []\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, RDFS.label))\n",
        "        if labels:\n",
        "            for label in labels:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "        else:\n",
        "            data.append([file_path, str(cls), \"\"])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    \"../Ontologies/materialsmine.ttl\",\n",
        "    \"../Ontologies/pmdco_core.ttl\",\n",
        "    \"../Ontologies/nfdicore_2.ttl\",\n",
        "    # \"../Ontologies/bfo.owl\",\n",
        "    # \"../Ontologies/emmo.ttl\",\n",
        "    '../Ontologies/emmo.owl',\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data from all files\n",
        "all_data = []\n",
        "for ontology_file in ontology_files:\n",
        "    all_data.extend(load_and_collect_classes(ontology_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write data to CSV\n",
        "output_file = \"ontology_classes.csv\"\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(all_data)\n",
        "\n",
        "print(f\"Data has been saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def load_and_collect_classes(file_path):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class URIs and labels if available\n",
        "    data = []\n",
        "    for cls in classes:\n",
        "        # Query for rdfs:label, skos:altLabel, and skos:prefLabel\n",
        "        # labels = list(g.objects(cls, RDFS.label)) + list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel))\n",
        "        labels = list(g.objects(cls, RDFS.label)) + list(g.objects(cls, SKOS.prefLabel))\n",
        "        if labels:\n",
        "            for label in labels:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "        else:\n",
        "            data.append([file_path, str(cls), \"\"])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    \"../Ontologies/materialsmine.ttl\",\n",
        "    \"../Ontologies/pmdco_core.ttl\",\n",
        "    \"../Ontologies/nfdicore_2.ttl\",\n",
        "    \"../Ontologies/bfo.owl\",\n",
        "    # \"../Ontologies/emmo.ttl\",\n",
        "    '../Ontologies/emmo.owl',\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data from all files\n",
        "all_data = []\n",
        "for ontology_file in ontology_files:\n",
        "    all_data.extend(load_and_collect_classes(ontology_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def load_and_collect_classes(file_path):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class URIs and labels if available\n",
        "    data = []\n",
        "    for cls in classes:\n",
        "        # Query for rdfs:label, skos:altLabel, and skos:prefLabel\n",
        "        labels = list(g.objects(cls, RDFS.label)) + list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel))\n",
        "        if labels:\n",
        "            for label in labels:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                print(f\"Class URI: {cls}, Label: {label}\")\n",
        "        else:\n",
        "            data.append([file_path, str(cls), \"\"])\n",
        "            print(f\"Class URI: {cls}\")\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data from all files\n",
        "all_data = []\n",
        "for ontology_file in ontology_files:\n",
        "    all_data.extend(load_and_collect_classes(ontology_file))\n",
        "\n",
        "# Write data to CSV\n",
        "output_file = \"ontology_classes.csv\"\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(all_data)\n",
        "\n",
        "print(f\"Data has been saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    # Normalize string by converting to lowercase and removing special characters\n",
        "    s = s.lower()\n",
        "    # Remove characters: '_', '-', '+', space, and '...'\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def load_and_collect_classes(file_path):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class labels and subclass relationships if available\n",
        "    data = []\n",
        "    found_class_labels = set()\n",
        "    subclass_relationships = {}\n",
        "    \n",
        "    for cls in classes:\n",
        "        # Query for skos:altLabel, skos:prefLabel, and rdfs:label\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            data.append([file_path, str(cls), str(label)])\n",
        "            found_class_labels.add(normalized_label)\n",
        "\n",
        "        # Query for subclasses using rdfs:subClassOf\n",
        "        subclasses = list(g.objects(cls, RDFS.subClassOf))\n",
        "        if subclasses:\n",
        "            subclass_relationships[str(cls)] = [str(sub) for sub in subclasses]\n",
        "\n",
        "    return data, found_class_labels, subclass_relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data, found class labels, and subclass relationships from all files\n",
        "all_data = []\n",
        "all_found_class_labels = set()\n",
        "all_subclass_relationships = {}\n",
        "for ontology_file in ontology_files:\n",
        "    file_data, found_class_labels, subclass_relationships = load_and_collect_classes(ontology_file)\n",
        "    all_data.extend(file_data)\n",
        "    all_found_class_labels.update(found_class_labels)\n",
        "    all_subclass_relationships.update(subclass_relationships)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    # Normalize string by converting to lowercase and removing special characters\n",
        "    s = s.lower()\n",
        "    # Remove characters: '_', '-', '+', space, and '...'\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def load_and_collect_classes(file_path):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class labels if available\n",
        "    data = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        # Query for skos:altLabel, skos:prefLabel, and rdfs:label\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            data.append([file_path, str(cls), str(label)])\n",
        "            found_class_labels.add(normalized_label)\n",
        "\n",
        "    return data, found_class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data and found class labels from all files\n",
        "all_data = []\n",
        "all_found_class_labels = set()\n",
        "for ontology_file in ontology_files:\n",
        "    file_data, found_class_labels = load_and_collect_classes(ontology_file)\n",
        "    all_data.extend(file_data)\n",
        "    all_found_class_labels.update(found_class_labels)\n",
        "\n",
        "# Write data to CSV\n",
        "output_file = \"ontology_classes.csv\"\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(all_data)\n",
        "\n",
        "print(f\"Data has been saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    \"../Ontologies/materialsmine.ttl\",\n",
        "    \"../Ontologies/pmdco_core.ttl\",\n",
        "    \"../Ontologies/nfdicore_2.ttl\",\n",
        "    \"../Ontologies/bfo.owl\",\n",
        "    \"../Ontologies/emmo.ttl\",\n",
        "    # '../Ontologies/emmo.owl',\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data and found class labels from all files\n",
        "all_data = []\n",
        "all_found_class_labels = set()\n",
        "for ontology_file in ontology_files:\n",
        "    file_data, found_class_labels = load_and_collect_classes(ontology_file)\n",
        "    all_data.extend(file_data)\n",
        "    all_found_class_labels.update(found_class_labels)\n",
        "\n",
        "# Write data to CSV\n",
        "output_file = \"ontology_classes.csv\"\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(all_data)\n",
        "\n",
        "print(f\"Data has been saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('ontology_classes.csv')\n",
        "print(df.head())\n",
        "\n",
        "# Get the number of rows and columns\n",
        "num_rows, num_cols = df.shape\n",
        "\n",
        "print(f\"Number of rows: {num_rows}\")\n",
        "print(f\"Number of columns: {num_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def load_and_collect_classes(file_path):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class URIs and labels if available\n",
        "    data = []\n",
        "    found_class_names = set()\n",
        "    for cls in classes:\n",
        "        # Query for rdfs:label, skos:altLabel, and skos:prefLabel\n",
        "        labels = list(g.objects(cls, RDFS.label)) + list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel))\n",
        "        if labels:\n",
        "            for label in labels:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                found_class_names.add(label)\n",
        "        else:\n",
        "            data.append([file_path, str(cls), \"\"])\n",
        "\n",
        "    return data, found_class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    \"../Ontologies/materialsmine.ttl\",\n",
        "    \"../Ontologies/pmdco_core.ttl\",\n",
        "    \"../Ontologies/nfdicore_2.ttl\",\n",
        "    \"../Ontologies/bfo.owl\",\n",
        "    \"../Ontologies/emmo.ttl\",\n",
        "    \"../Ontologies/owlapi.xrdf\",\n",
        "    # '../Ontologies/emmo.owl',\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data and found class names from all files\n",
        "all_data = []\n",
        "all_found_class_names = set()\n",
        "for ontology_file in ontology_files:\n",
        "    file_data, found_class_names = load_and_collect_classes(ontology_file)\n",
        "    all_data.extend(file_data)\n",
        "    all_found_class_names.update(found_class_names)\n",
        "\n",
        "\n",
        "# Write data to CSV\n",
        "output_file = \"ontology_classes.csv\"\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(all_data)\n",
        "\n",
        "print(f\"Data has been saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "class_names_to_check = ['Software Agent', 'SoftwareAgent', 'AmperePerJoule', 'characteristic','Bundle', '1DGeometry', 'force' , 'Agent Influence' \n",
        "                        , 'thing', 'meter', 'Accreditation-Document', \n",
        "                        'CoulombPerSquareMilliMetre', 'AcademicDiscipline', 'nfdi', 'CompressioN', 'Poissons Ratio']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if class names are in found_class_names and print corresponding input files\n",
        "print(\"\\nClass names found in the output:\")\n",
        "for class_name in class_names_to_check:\n",
        "    if class_name in all_found_class_names:\n",
        "        print(f\"Class '{class_name}' found in:\")\n",
        "        for ontology_file in ontology_files:\n",
        "            file_data, found_class_names = load_and_collect_classes(ontology_file)\n",
        "            if class_name in found_class_names:\n",
        "                print(f\"- {ontology_file}\")\n",
        "    else:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def load_and_collect_classes(file_path):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class labels if available\n",
        "    data = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        # Query for skos:altLabel, skos:prefLabel, and rdfs:label\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            data.append([file_path, str(cls), str(label)])\n",
        "            found_class_labels.add(label)\n",
        "\n",
        "    return data, found_class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data and found class labels from all files\n",
        "all_data = []\n",
        "all_found_class_labels = set()\n",
        "for ontology_file in ontology_files:\n",
        "    file_data, found_class_labels = load_and_collect_classes(ontology_file)\n",
        "    all_data.extend(file_data)\n",
        "    all_found_class_labels.update(found_class_labels)\n",
        "\n",
        "# Write data to CSV\n",
        "output_file = \"ontology_classes.csv\"\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(all_data)\n",
        "\n",
        "print(f\"Data has been saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "class_names_to_check = ['Software Agent', 'SoftwareAgent', 'AmperePerJoule', 'characteristic','Bundle', '1DGeometry', 'force' , 'Agent Influence' \n",
        "                        , 'thing', 'meter', 'Accreditation-Document', \n",
        "                        'CoulombPerSquareMilliMetre', 'AcademicDiscipline', 'nfdi', 'CompressioN', 'Poissons Ratio' , 'PoissonsRatio' , 'Poissons_Ratio']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if class names are in found_class_labels and print corresponding input files\n",
        "print(\"\\nClass names found in the output:\")\n",
        "for class_name in class_names_to_check:\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if class_name.lower() in label.lower():\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, found_class_labels = load_and_collect_classes(ontology_file)\n",
        "                if label in found_class_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    # Normalize string by converting to lowercase and removing special characters\n",
        "    s = s.lower()\n",
        "    # Remove characters: '_', '-', '+', space, and '...'\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def load_and_collect_classes(file_path):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class labels if available\n",
        "    data = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        # Query for skos:altLabel, skos:prefLabel, and rdfs:label\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            data.append([file_path, str(cls), str(label)])\n",
        "            found_class_labels.add(normalized_label)\n",
        "\n",
        "    return data, found_class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data and found class labels from all files\n",
        "all_data = []\n",
        "all_found_class_labels = set()\n",
        "for ontology_file in ontology_files:\n",
        "    file_data, found_class_labels = load_and_collect_classes(ontology_file)\n",
        "    all_data.extend(file_data)\n",
        "    all_found_class_labels.update(found_class_labels)\n",
        "\n",
        "# Write data to CSV\n",
        "output_file = \"ontology_classes.csv\"\n",
        "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(all_data)\n",
        "\n",
        "print(f\"Data has been saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if class names are in found_class_labels and print corresponding input files\n",
        "print(\"\\nClass names found in the output:\")\n",
        "for class_name in class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, found_class_labels = load_and_collect_classes(ontology_file)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    # Normalize string by converting to lowercase and removing special characters\n",
        "    s = s.lower()\n",
        "    # Remove characters: '_', '-', '+', space, and '...'\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class labels and relations\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        # Query for skos:altLabel, skos:prefLabel, and rdfs:label\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            data.append([file_path, str(cls), str(label)])\n",
        "            found_class_labels.add(normalized_label)\n",
        "\n",
        "        # Query for relations such as subClassOf and equivalentClass\n",
        "        for obj in g.objects(cls, RDFS.subClassOf):\n",
        "            relations.append([file_path, str(cls), 'subClassOf', str(obj)])\n",
        "        for obj in g.objects(cls, OWL.equivalentClass):\n",
        "            relations.append([file_path, str(cls), 'equivalentClass', str(obj)])\n",
        "\n",
        "    return data, relations, found_class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data, relations, and found class labels from all files\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "for ontology_file in ontology_files:\n",
        "    file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file)\n",
        "    all_data.extend(file_data)\n",
        "    all_relations.extend(file_relations)\n",
        "    all_found_class_labels.update(found_class_labels)\n",
        "\n",
        "# Write class data to CSV\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "class_names_to_check = ['Software Agent', 'SoftwareAgent', 'AmperePerJoule', 'characteristic','Bundle', '1DGeometry', 'force' , 'Agent Influence' \n",
        "                        , 'thing', 'meter', 'Accreditation-Document', \n",
        "                        'CoulombPerSquareMilliMetre', 'AcademicDiscipline', 'nfdi', 'CompressioN', 'Poissons Ratio']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write class relations to CSV\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Relation\", \"Object Class URI\"])\n",
        "    writer.writerows(all_relations)\n",
        "\n",
        "print(f\"Class data has been saved to {class_output_file}\")\n",
        "print(f\"Class relations have been saved to {relations_output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if class names are in found_class_labels and print corresponding input files\n",
        "print(\"\\nClass names found in the output:\")\n",
        "for class_name in class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    # Normalize string by converting to lowercase and removing special characters\n",
        "    s = s.lower()\n",
        "    # Remove characters: '_', '-', '+', space, and '...'\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Normalize the class names to check for comparison\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class labels and relations\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        # Query for skos:altLabel, skos:prefLabel, and rdfs:label\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            data.append([file_path, str(cls), str(label)])\n",
        "            found_class_labels.add(normalized_label)\n",
        "            # Check if the class is in the list of classes to check\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                # Query for relations such as subClassOf and equivalentClass\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    relations.append([file_path, str(cls), 'subClassOf', str(obj)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    relations.append([file_path, str(cls), 'equivalentClass', str(obj)])\n",
        "\n",
        "    return data, relations, found_class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data, relations, and found class labels from all files\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "for ontology_file in ontology_files:\n",
        "    file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "    all_data.extend(file_data)\n",
        "    all_relations.extend(file_relations)\n",
        "    all_found_class_labels.update(found_class_labels)\n",
        "\n",
        "# Write class data to CSV\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write class relations to CSV\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Relation\", \"Object Class URI\"])\n",
        "    writer.writerows(all_relations)\n",
        "\n",
        "print(f\"Class data has been saved to {class_output_file}\")\n",
        "print(f\"Class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "# Check if class names are in found_class_labels and print corresponding input files\n",
        "print(\"\\nClass names found in the output:\")\n",
        "for class_name in class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    # Normalize string by converting to lowercase and removing special characters\n",
        "    s = s.lower()\n",
        "    # Remove characters: '_', '-', '+', space, and '...'\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Normalize the class names to check for comparison\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class labels and relations\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        # Query for skos:altLabel, skos:prefLabel, and rdfs:label\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            data.append([file_path, str(cls), str(label)])\n",
        "            found_class_labels.add(normalized_label)\n",
        "            # Check if the class is in the list of classes to check\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                # Query for relations such as subClassOf and equivalentClass\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    \"../Ontologies/materialsmine.ttl\",\n",
        "    \"../Ontologies/pmdco_core.ttl\",\n",
        "    \"../Ontologies/nfdicore_2.ttl\",\n",
        "    \"../Ontologies/bfo.owl\",\n",
        "    \"../Ontologies/emmo.ttl\",\n",
        "    \"../Ontologies/owlapi.xrdf\",\n",
        "    # '../Ontologies/emmo.owl',\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "class_names_to_check = ['Software Agent', 'SoftwareAgent', 'AmperePerJoule', 'characteristic','Bundle', '1DGeometry', 'force' , 'Agent Influence' \n",
        "                        , 'thing', 'meter', 'Accreditation-Document', \n",
        "                        'CoulombPerSquareMilliMetre', 'AcademicDiscipline', 'nfdi', 'CompressioN', 'Poissons Ratio']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect data, relations, and found class labels from all files\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "for ontology_file in ontology_files:\n",
        "    file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "    all_data.extend(file_data)\n",
        "    all_relations.extend(file_relations)\n",
        "    all_found_class_labels.update(found_class_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write class data to CSV\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(all_data)\n",
        "\n",
        "# Write class relations to CSV\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(all_relations)\n",
        "\n",
        "print(f\"Class data has been saved to {class_output_file}\")\n",
        "print(f\"Class relations have been saved to {relations_output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if class names are in found_class_labels and print corresponding input files\n",
        "print(\"\\nClass names found in the output:\")\n",
        "for class_name in class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print class relationships for the specified classes\n",
        "print(\"\\nClass relationships for the specified classes:\")\n",
        "for class_name in class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    for relation in all_relations:\n",
        "        subject_label_normalized = normalize_string(relation[2])\n",
        "        if normalized_class_name == subject_label_normalized:\n",
        "            print(f\"File: {relation[0]}, Subject: {relation[1]} ({relation[2]}), Relation: {relation[3]}, Object: {relation[4]} ({relation[5]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    # Normalize string by converting to lowercase and removing special characters\n",
        "    s = s.lower()\n",
        "    # Remove characters: '_', '-', '+', space, and '...'\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Normalize the class names to check for comparison\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class labels and relations\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        # Query for skos:altLabel, skos:prefLabel, and rdfs:label\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            data.append([file_path, str(cls), str(label)])\n",
        "            found_class_labels.add(normalized_label)\n",
        "            # Check if the class is in the list of classes to check\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                # Query for relations such as subClassOf and equivalentClass\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    \"../Ontologies/materialsmine.ttl\",\n",
        "    \"../Ontologies/pmdco_core.ttl\",\n",
        "    \"../Ontologies/nfdicore_2.ttl\",\n",
        "    \"../Ontologies/bfo.owl\",\n",
        "    \"../Ontologies/emmo.ttl\",\n",
        "    \"../Ontologies/owlapi.xrdf\",\n",
        "    # '../Ontologies/emmo.owl',\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "initial_class_names_to_check = ['Poissons Ratio']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output file names\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "# Initialize variables to store all data and relations\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "# Initialize the list of class names to check\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "# Perform iterative process with a maximum of 3 iterations\n",
        "max_iterations = 3\n",
        "iteration_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    # Collect data, relations, and found class labels from all files\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    # Append new data and relations to the overall collection\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    # Write class data to CSV\n",
        "    with open(class_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if file.tell() == 0:\n",
        "            writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "        writer.writerows(new_data)\n",
        "\n",
        "    # Write class relations to CSV\n",
        "    with open(relations_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if file.tell() == 0:\n",
        "            writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "        writer.writerows(new_relations)\n",
        "\n",
        "    # Prepare for next iteration: set new class names to check\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    # Normalize new class names to check for consistency\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "print(f\"Class data has been saved to {class_output_file}\")\n",
        "print(f\"Class relations have been saved to {relations_output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if initial class names are in found_class_labels and print corresponding input files\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print class relationships for the initial specified classes\n",
        "print(\"\\nClass relationships for the initial specified classes:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    for relation in all_relations:\n",
        "        subject_label_normalized = normalize_string(relation[2])\n",
        "        if normalized_class_name == subject_label_normalized:\n",
        "            print(f\"File: {relation[0]}, Subject: {relation[1]} ({relation[2]}), Relation: {relation[3]}, Object: {relation[4]} ({relation[5]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    # Normalize string by converting to lowercase and removing special characters\n",
        "    s = s.lower()\n",
        "    # Remove characters: '_', '-', '+', space, and '...'\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Normalize the class names to check for comparison\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class labels and relations\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        # Query for skos:altLabel, skos:prefLabel, and rdfs:label\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            # Check if the class is in the list of classes to check\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                # Query for relations such as subClassOf and equivalentClass\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output file names\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "# Initialize variables to store all data and relations\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "# Initialize the list of class names to check\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "# Perform iterative process with a maximum of 3 iterations\n",
        "max_iterations = 5\n",
        "iteration_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    # Collect data, relations, and found class labels from all files\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    # Append new data and relations to the overall collection\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    # Prepare for next iteration: set new class names to check\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    # Normalize new class names to check for consistency\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write class data to CSV only if the class is in the initial list\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "# Write class relations to CSV only if the class is in the initial list\n",
        "filtered_relations = [row for row in all_relations if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if initial class names are in found_class_labels and print corresponding input files\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print class relationships for the initial specified classes\n",
        "print(\"\\nClass relationships for the initial specified classes:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    for relation in all_relations:\n",
        "        subject_label_normalized = normalize_string(relation[2])\n",
        "        if normalized_class_name == subject_label_normalized:\n",
        "            print(f\"File: {relation[0]}, Subject: {relation[1]} ({relation[2]}), Relation: {relation[3]}, Object: {relation[4]} ({relation[5]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "initial_class_names_to_check = ['Software Agent', 'SoftwareAgent', 'AmperePerJoule', 'characteristic','Bundle', '1DGeometry', 'force' , 'Agent Influence' \n",
        "                        , 'thing', 'meter', 'Accreditation-Document', \n",
        "                        'CoulombPerSquareMilliMetre', 'AcademicDiscipline', 'nfdi', 'CompressioN', 'Poissons Ratio']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    \"../Ontologies/materialsmine.ttl\",\n",
        "    \"../Ontologies/pmdco_core.ttl\",\n",
        "    \"../Ontologies/nfdicore_2.ttl\",\n",
        "    \"../Ontologies/bfo.owl\",\n",
        "    \"../Ontologies/emmo.ttl\",\n",
        "    \"../Ontologies/owlapi.xrdf\",\n",
        "    # '../Ontologies/emmo.owl',\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    # Normalize string by converting to lowercase and removing special characters\n",
        "    s = s.lower()\n",
        "    # Remove characters: '_', '-', '+', space, and '...'\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    # Determine the file format based on the file extension\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    # Load the ontology file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Normalize the class names to check for comparison\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    # Define the custom namespace (replace 'ex' with your custom namespace prefix)\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    # Query for all classes in the ontology\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    # Collect class labels and relations\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        # Query for skos:altLabel, skos:prefLabel, and rdfs:label\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            # Check if the class is in the list of classes to check\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                # Query for relations such as subClassOf and equivalentClass\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output file names\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "# Initialize variables to store all data and relations\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "# Initialize the list of class names to check\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "# Perform iterative process with a maximum of 3 iterations\n",
        "max_iterations = 3\n",
        "iteration_count = 0\n",
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    # Collect data, relations, and found class labels from all files\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    # Append new data and relations to the overall collection\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    # Prepare for next iteration: set new class names to check\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    # Normalize new class names to check for consistency\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "# Write class data to CSV only if the class is in the initial list\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "# Write class relations to CSV only if the class is in the initial list\n",
        "filtered_relations = [row for row in all_relations if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "# Check if initial class names are in found_class_labels and print corresponding input files\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "# Print class relationships for the initial specified classes\n",
        "print(\"\\nClass relationships for the initial specified classes:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    for relation in all_relations:\n",
        "        subject_label_normalized = normalize_string(relation[2])\n",
        "        if normalized_class_name == subject_label_normalized:\n",
        "            print(f\"File: {relation[0]}, Subject: {relation[1]} ({relation[2]}), Relation: {relation[3]}, Object: {relation[4]} ({relation[5]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "initial_class_names_to_check = ['Tensile Property', 'Viscoelastic Property', 'Geometry', 'Document','Value Object', 'LengthUnit']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 3\n",
        "iteration_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "initial_class_names_to_check = ['Poissons Ratio']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = [row for row in all_relations if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nClass relationships for the initial specified classes:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    for relation in all_relations:\n",
        "        subject_label_normalized = normalize_string(relation[2])\n",
        "        if normalized_class_name == subject_label_normalized:\n",
        "            print(f\"File: {relation[0]}, Subject: {relation[1]} ({relation[2]}), Relation: {relation[3]}, Object: {relation[4]} ({relation[5]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 6\n",
        "iteration_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nClass relationships for the initial specified classes and their hierarchy:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    for relation in all_relations:\n",
        "        subject_label_normalized = normalize_string(relation[2])\n",
        "        if normalized_class_name == subject_label_normalized:\n",
        "            print(f\"File: {relation[0]}, Subject: {relation[1]} ({relation[2]}), Relation: {relation[3]}, Object: {relation[4]} ({relation[5]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels\n",
        "\n",
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "def print_hierarchy(class_name, relations, depth=0):\n",
        "    for relation in relations:\n",
        "        if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "            indent = '  ' * depth\n",
        "            print(f\"{indent}{class_name} is {relation[3]} {relation[5]}\")\n",
        "            print_hierarchy(relation[5], relations, depth + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 2\n",
        "iteration_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nClass relationships for the initial specified classes and their hierarchy:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    print_hierarchy(normalized_class_name, all_relations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels\n",
        "\n",
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "def print_hierarchy(class_name, relations, ontology_files, depth=0):\n",
        "    for relation in relations:\n",
        "        if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "            indent = '  ' * depth\n",
        "            print(f\"{indent}{class_name} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "            print_hierarchy(relation[5], relations, ontology_files, depth + 1)\n",
        "\n",
        "\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 2\n",
        "iteration_count = 0\n",
        "\n",
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nClass relationships for the initial specified classes and their hierarchy:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    print_hierarchy(normalized_class_name, all_relations, ontology_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "initial_class_names_to_check = [ 'SoftwareAgent', 'AmperePerJoule',\n",
        "                        'CoulombPerSquareMilliMetre', 'AcademicDiscipline', 'nfdi', 'CompressioN', 'Poissons Ratio']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels\n",
        "\n",
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "def print_hierarchy(class_name, relations, ontology_files, depth=0):\n",
        "    for relation in relations:\n",
        "        if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "            indent = '  ' * depth\n",
        "            print(f\"{indent}{class_name} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "            print_hierarchy(relation[5], relations, ontology_files, depth + 1)\n",
        "\n",
        "\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 2\n",
        "iteration_count = 0\n",
        "\n",
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nClass relationships for the initial specified classes and their hierarchy:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    print_hierarchy(normalized_class_name, all_relations, ontology_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nClass relationships for the initial specified classes and their hierarchy:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    print_hierarchy(normalized_class_name, all_relations, ontology_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels\n",
        "\n",
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "def print_hierarchy(class_name, relations, ontology_files, output_file):\n",
        "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Class Name\", \"Relation Type\", \"Related Class\", \"From File\"])\n",
        "\n",
        "        for relation in relations:\n",
        "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "                writer.writerow([relation[2], relation[3], relation[5], relation[0]])\n",
        "                print_hierarchy(relation[5], relations, ontology_files, output_file)\n",
        "\n",
        "output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 2\n",
        "iteration_count = 0\n",
        "\n",
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    print_hierarchy(normalized_class_name, all_relations, ontology_files, output_hierarchy_file)\n",
        "\n",
        "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    \"../Ontologies/materialsmine.ttl\",\n",
        "    \"../Ontologies/pmdco_core.ttl\",\n",
        "    \"../Ontologies/nfdicore_2.ttl\",\n",
        "    \"../Ontologies/bfo.owl\",\n",
        "    \"../Ontologies/emmo.ttl\",\n",
        "    \"../Ontologies/owlapi.xrdf\",\n",
        "    # '../Ontologies/emmo.owl',\n",
        "    \"../Ontologies/Physical Activity Ontology_V2.owl\"\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "initial_class_names_to_check = [ 'SoftwareAgent', 'AmperePerJoule',\n",
        "                        'CoulombPerSquareMilliMetre', 'AcademicDiscipline', 'nfdi', 'CompressioN', 'Poissons Ratio']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels\n",
        "\n",
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "def print_hierarchy(class_name, relations, ontology_files, output_file):\n",
        "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Class Name\", \"Relation Type\", \"Related Class\", \"From File\"])\n",
        "\n",
        "        def recursive_print(class_name, depth=0):\n",
        "            for relation in relations:\n",
        "                if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "                    writer.writerow([relation[2], relation[3], relation[5], relation[0]])\n",
        "                    indent = '  ' * depth\n",
        "                    print(f\"{indent}{relation[2]} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "                    recursive_print(relation[5], depth + 1)\n",
        "\n",
        "        recursive_print(class_name)\n",
        "\n",
        "output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 2\n",
        "iteration_count = 0\n",
        "\n",
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    print_hierarchy(normalized_class_name, all_relations, ontology_files, output_hierarchy_file)\n",
        "\n",
        "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    # \"../Ontologies/materialsmine.ttl\",\n",
        "    \"../Ontologies/pmdco_core.ttl\",\n",
        "    \"../Ontologies/nfdicore_2.ttl\",\n",
        "    \"../Ontologies/bfo.owl\",\n",
        "    \"../Ontologies/emmo.ttl\",\n",
        "    \"../Ontologies/owlapi.xrdf\",\n",
        "    \"../Ontologies/schemaorg.owl\",\n",
        "    \"../Ontologies/MaterialsMine.xrdf\",\n",
        "    # '../Ontologies/emmo.owl',\n",
        "    \"../Ontologies/Physical_Activity_Ontology_V2.owl\",\n",
        "    # \"../Ontologies/Physical_Activity_Ontology_V2.xrdf\",\n",
        "    # \"../Ontologies/oboe.owl\",\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # List of class names to check\n",
        "# initial_class_names_to_check = [ 'SoftwareAgent', 'AmperePerJoule', 'BioChemEntity', 'stress',\n",
        "#                         'CoulombPerSquareMilliMetre', 'AcademicDiscipline', 'nfdi', 'CompressioN', 'Poissons Ratio', 'Amount']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "initial_class_names_to_check = [ 'Compression','AmperePerJoule','nfdi','stress']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "    sio = Namespace(\"http://semanticscience.org/resource/\")\n",
        "    skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
        "    owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
        "    rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "    materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
        "    bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
        "    rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
        "    xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
        "    xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
        "    foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
        "\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels\n",
        "\n",
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "def print_hierarchy(class_name, relations, writer):\n",
        "    def recursive_print(class_name, depth=0):\n",
        "        for relation in relations:\n",
        "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "                writer.writerow([relation[2], relation[3], relation[5], relation[0]])\n",
        "                indent = '  ' * depth\n",
        "                print(f\"{indent}{relation[2]} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "                recursive_print(relation[5], depth + 1)\n",
        "\n",
        "    recursive_print(class_name)\n",
        "\n",
        "output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 2\n",
        "iteration_count = 0\n",
        "\n",
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Class Name\", \"Relation Type\", \"Related Class\", \"From File\"])\n",
        "    for class_name in initial_class_names_to_check:\n",
        "        normalized_class_name = normalize_string(class_name)\n",
        "        print_hierarchy(normalized_class_name, all_relations, writer)\n",
        "\n",
        "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace\n",
        "\n",
        "# Mapping of specific class names to their replacements\n",
        "class_name_mapping = {\n",
        "    'amount': 'quantity'\n",
        "}\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    if s in class_name_mapping:\n",
        "        s = class_name_mapping[s]\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels\n",
        "\n",
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "def print_hierarchy(class_name, relations, writer):\n",
        "    def recursive_print(class_name, depth=0):\n",
        "        for relation in relations:\n",
        "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "                writer.writerow([relation[2], relation[3], relation[5], relation[0]])\n",
        "                indent = '  ' * depth\n",
        "                print(f\"{indent}{relation[2]} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "                recursive_print(relation[5], depth + 1)\n",
        "\n",
        "    recursive_print(class_name)\n",
        "\n",
        "output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 2\n",
        "iteration_count = 0\n",
        "\n",
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Class Name\", \"Relation Type\", \"Related Class\", \"From File\"])\n",
        "    for class_name in initial_class_names_to_check:\n",
        "        normalized_class_name = normalize_string(class_name)\n",
        "        print_hierarchy(normalized_class_name, all_relations, writer)\n",
        "\n",
        "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, Literal\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl, .owl and xrdf files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # # Hardcode label change for the specific file and class\n",
        "    # if file_path == '../Ontologies/materialsmine.ttl':\n",
        "    #     for cls in g.subjects(RDF.type, OWL.Class):\n",
        "    #         for label in g.objects(cls, RDFS.label):\n",
        "    #             if str(label) == 'amount':\n",
        "    #                 g.remove((cls, RDFS.label, label))\n",
        "    #                 g.add((cls, RDFS.label, Literal('quantity')))\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "    sio = Namespace(\"http://semanticscience.org/resource/\")\n",
        "    skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
        "    owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
        "    rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "    materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
        "    bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
        "    rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
        "    xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
        "    xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
        "    foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels\n",
        "\n",
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "def print_hierarchy(class_name, relations, writer):\n",
        "    def recursive_print(class_name, depth=0):\n",
        "        for relation in relations:\n",
        "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "                writer.writerow([relation[2], relation[3], relation[5], relation[0]])\n",
        "                indent = '  ' * depth\n",
        "                print(f\"{indent}{relation[2]} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "                recursive_print(relation[5], depth + 1)\n",
        "\n",
        "    recursive_print(class_name)\n",
        "\n",
        "output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 2\n",
        "iteration_count = 0\n",
        "\n",
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Class Name\", \"Relation Type\", \"Related Class\", \"From File\"])\n",
        "    for class_name in initial_class_names_to_check:\n",
        "        normalized_class_name = normalize_string(class_name)\n",
        "        print_hierarchy(normalized_class_name, all_relations, writer)\n",
        "\n",
        "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    # \"../Ontologies/materialsmine.ttl\", ### is not complete!\n",
        "    \"../Ontologies/materialsmine_converted.ttl\",\n",
        "    # \"../Ontologies/pmdco_core.ttl\",\n",
        "    # \"../Ontologies/nfdicore_2.ttl\",\n",
        "    # \"../Ontologies/bfo.owl\",\n",
        "    \"../Ontologies/emmo.ttl\",\n",
        "    # \"../Ontologies/owlapi.xrdf\",\n",
        "    # \"../Ontologies/schemaorg.owl\",\n",
        "    # \"../Ontologies/MaterialsMine.xrdf\",\n",
        "    # '../Ontologies/emmo.owl', ### has problem of reading file\n",
        "    # \"../Ontologies/Physical_Activity_Ontology_V2.owl\",\n",
        "    # \"../Ontologies/Physical_Activity_Ontology_V2.xrdf\",\n",
        "    # \"../Ontologies/oboe.owl\",\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "initial_class_names_to_check = [ 'Compression','AmperePerJoule','nfdi','stress']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Add \"ispartof\" to find the relations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, Literal\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Hardcode label change for the specific file and class\n",
        "    # if file_path == 'materials.ttl':\n",
        "    #     for cls in g.subjects(RDF.type, OWL.Class):\n",
        "    #         for label in g.objects(cls, RDFS.label):\n",
        "    #             if str(label) == 'amount':\n",
        "    #                 g.remove((cls, RDFS.label, label))\n",
        "    #                 g.add((cls, RDFS.label, Literal('quantity')))\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "    sio = Namespace(\"http://semanticscience.org/resource/\")\n",
        "    skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
        "    owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
        "    rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "    materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
        "    bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
        "    rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
        "    xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
        "    xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
        "    foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
        "    # isPartOf = Namespace(\"http://semanticscience.org/resource/isPartOf\")\n",
        "    dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
        "    isPartOf = dcterms.isPartOf\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, isPartOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels\n",
        "\n",
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "def print_hierarchy(class_name, relations, writer):\n",
        "    def recursive_print(class_name, depth=0):\n",
        "        for relation in relations:\n",
        "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "                writer.writerow([relation[2], relation[3], relation[5], relation[0]])\n",
        "                indent = '  ' * depth\n",
        "                print(f\"{indent}{relation[2]} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "                recursive_print(relation[5], depth + 1)\n",
        "\n",
        "    recursive_print(class_name)\n",
        "\n",
        "output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 2\n",
        "iteration_count = 0\n",
        "\n",
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Class Name\", \"Relation Type\", \"Related Class\", \"From File\"])\n",
        "    for class_name in initial_class_names_to_check:\n",
        "        normalized_class_name = normalize_string(class_name)\n",
        "        print_hierarchy(normalized_class_name, all_relations, writer)\n",
        "\n",
        "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered class data has been saved to ontology_classes.csv\n",
            "Filtered class relations have been saved to ontology_relations.csv\n",
            "\n",
            "Initial class names found in the output:\n",
            "Class 'Compression' found in:\n",
            "- ../Ontologies/materialsmine_converted.ttl\n",
            "Class 'AmperePerJoule' found in:\n",
            "- ../Ontologies/emmo.ttl\n",
            "Class 'nfdi' not found in the output.\n",
            "Class 'stress' found in:\n",
            "- ../Ontologies/materialsmine_converted.ttl\n",
            "- ../Ontologies/emmo.ttl\n",
            "\n",
            "Saving class hierarchy to CSV file:\n",
            "Compression is subClassOf Viscoelastic Property (from ../Ontologies/materialsmine_converted.ttl)\n",
            "  Viscoelastic Property is subClassOf Amount (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    Amount is subClassOf measurement value (from ../Ontologies/materialsmine_converted.ttl)\n",
            "      measurement value is subClassOf number (from ../Ontologies/materialsmine_converted.ttl)\n",
            "        number is subClassOf scalar (from ../Ontologies/materialsmine_converted.ttl)\n",
            "          scalar is subClassOf tensor (from ../Ontologies/materialsmine_converted.ttl)\n",
            "            tensor is subClassOf mathematical entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              mathematical entity is subClassOf information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                information content entity is subClassOf object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  object is subClassOf entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  object is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  Object is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "                information content entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "        Number is subClassOf Numerical (from ../Ontologies/emmo.ttl)\n",
            "          Numerical is subClassOf Mathematical (from ../Ontologies/emmo.ttl)\n",
            "            Mathematical is subClassOf Language (from ../Ontologies/emmo.ttl)\n",
            "              Language is subClassOf language entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                language entity is subClassOf information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  information content entity is subClassOf object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    object is subClassOf entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                      entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    object is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    Object is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "                  information content entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                language entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              language is subClassOf language entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                language entity is subClassOf information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  information content entity is subClassOf object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    object is subClassOf entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                      entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    object is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    Object is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "                  information content entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                language entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              Language is subClassOf Symbolic (from ../Ontologies/emmo.ttl)\n",
            "                Symbolic is subClassOf DiscreteData (from ../Ontologies/emmo.ttl)\n",
            "                Symbolic is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "        Number is subClassOf AlphabeticEntity (from ../Ontologies/emmo.ttl)\n",
            "          AlphabeticEntity is subClassOf Symbolic (from ../Ontologies/emmo.ttl)\n",
            "            Symbolic is subClassOf DiscreteData (from ../Ontologies/emmo.ttl)\n",
            "            Symbolic is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "      measurement value is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    Amount is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    Amount is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    Amount is equivalentClass None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "Ampere per Joule is subClassOf None (from ../Ontologies/emmo.ttl)\n",
            "Ampere per Joule is subClassOf None (from ../Ontologies/emmo.ttl)\n",
            "Ampere per Joule is subClassOf SICoherentDerivedUnit (from ../Ontologies/emmo.ttl)\n",
            "  SICoherentDerivedUnit is subClassOf DerivedUnit (from ../Ontologies/emmo.ttl)\n",
            "Ampere per Joule is subClassOf ElectricCurrentPerUnitEnergyUnit (from ../Ontologies/emmo.ttl)\n",
            "  ElectricCurrentPerUnitEnergyUnit is subClassOf SIDimensionalUnit (from ../Ontologies/emmo.ttl)\n",
            "    SIDimensionalUnit is subClassOf DimensionalUnit (from ../Ontologies/emmo.ttl)\n",
            "  ElectricCurrentPerUnitEnergyUnit is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "AmperePerJoule is subClassOf None (from ../Ontologies/emmo.ttl)\n",
            "AmperePerJoule is subClassOf None (from ../Ontologies/emmo.ttl)\n",
            "AmperePerJoule is subClassOf SICoherentDerivedUnit (from ../Ontologies/emmo.ttl)\n",
            "  SICoherentDerivedUnit is subClassOf DerivedUnit (from ../Ontologies/emmo.ttl)\n",
            "AmperePerJoule is subClassOf ElectricCurrentPerUnitEnergyUnit (from ../Ontologies/emmo.ttl)\n",
            "  ElectricCurrentPerUnitEnergyUnit is subClassOf SIDimensionalUnit (from ../Ontologies/emmo.ttl)\n",
            "    SIDimensionalUnit is subClassOf DimensionalUnit (from ../Ontologies/emmo.ttl)\n",
            "  ElectricCurrentPerUnitEnergyUnit is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "Stress is subClassOf Mechanical Property (from ../Ontologies/materialsmine_converted.ttl)\n",
            "  Mechanical Property is subClassOf Amount (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    Amount is subClassOf measurement value (from ../Ontologies/materialsmine_converted.ttl)\n",
            "      measurement value is subClassOf number (from ../Ontologies/materialsmine_converted.ttl)\n",
            "        number is subClassOf scalar (from ../Ontologies/materialsmine_converted.ttl)\n",
            "          scalar is subClassOf tensor (from ../Ontologies/materialsmine_converted.ttl)\n",
            "            tensor is subClassOf mathematical entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              mathematical entity is subClassOf information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                information content entity is subClassOf object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  object is subClassOf entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  object is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  Object is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "                information content entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "        Number is subClassOf Numerical (from ../Ontologies/emmo.ttl)\n",
            "          Numerical is subClassOf Mathematical (from ../Ontologies/emmo.ttl)\n",
            "            Mathematical is subClassOf Language (from ../Ontologies/emmo.ttl)\n",
            "              Language is subClassOf language entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                language entity is subClassOf information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  information content entity is subClassOf object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    object is subClassOf entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                      entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    object is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    Object is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "                  information content entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                language entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              language is subClassOf language entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                language entity is subClassOf information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  information content entity is subClassOf object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    object is subClassOf entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                      entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    object is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    Object is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "                  information content entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                language entity is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              Language is subClassOf Symbolic (from ../Ontologies/emmo.ttl)\n",
            "                Symbolic is subClassOf DiscreteData (from ../Ontologies/emmo.ttl)\n",
            "                Symbolic is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "        Number is subClassOf AlphabeticEntity (from ../Ontologies/emmo.ttl)\n",
            "          AlphabeticEntity is subClassOf Symbolic (from ../Ontologies/emmo.ttl)\n",
            "            Symbolic is subClassOf DiscreteData (from ../Ontologies/emmo.ttl)\n",
            "            Symbolic is equivalentClass None (from ../Ontologies/emmo.ttl)\n",
            "      measurement value is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    Amount is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    Amount is subClassOf None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    Amount is equivalentClass None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "Stress is subClassOf None (from ../Ontologies/emmo.ttl)\n",
            "Stress is subClassOf ISQDerivedQuantity (from ../Ontologies/emmo.ttl)\n",
            "  ISQDerivedQuantity is subClassOf DerivedQuantity (from ../Ontologies/emmo.ttl)\n",
            "  ISQDerivedQuantity is subClassOf InternationalSystemOfQuantity (from ../Ontologies/emmo.ttl)\n",
            "    InternationalSystemOfQuantity is subClassOf StandardizedPhysicalQuantity (from ../Ontologies/emmo.ttl)\n",
            "      StandardizedPhysicalQuantity is subClassOf PhysicalQuantity (from ../Ontologies/emmo.ttl)\n",
            "        PhysicalQuantity is subClassOf None (from ../Ontologies/emmo.ttl)\n",
            "Class hierarchy has been saved to class_hierarchy.csv\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Hardcode label change for the specific file and class\n",
        "    # if file_path == 'materials.ttl':\n",
        "    #     for cls in g.subjects(RDF.type, OWL.Class):\n",
        "    #         for label in g.objects(cls, RDFS.label):\n",
        "    #             if str(label) == 'amount':\n",
        "    #                 g.remove((cls, RDFS.label, label))\n",
        "    #                 g.add((cls, RDFS.label, Literal('quantity')))\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    # dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
        "    # isPartOf = dcterms.isPartOf\n",
        "\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "    sio = Namespace(\"http://semanticscience.org/resource/\")\n",
        "    skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
        "    owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
        "    rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "    materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
        "    bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
        "    rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
        "    xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
        "    xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
        "    foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
        "    # isPartOf = Namespace(\"http://semanticscience.org/resource/isPartOf\")\n",
        "    dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
        "    isPartOf = dcterms.isPartOf\n",
        "\n",
        "    # print(isPartOf)\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, isPartOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels\n",
        "\n",
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "def print_hierarchy(class_name, relations, writer):\n",
        "    def recursive_print(class_name, depth=0):\n",
        "        for relation in relations:\n",
        "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "                writer.writerow([relation[2], relation[3], relation[5], relation[0]])\n",
        "                indent = '  ' * depth\n",
        "                print(f\"{indent}{relation[2]} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "                recursive_print(relation[5], depth + 1)\n",
        "\n",
        "    recursive_print(class_name)\n",
        "\n",
        "output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 2\n",
        "iteration_count = 0\n",
        "\n",
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Class Name\", \"Relation Type\", \"Related Class\", \"From File\"])\n",
        "    for class_name in initial_class_names_to_check:\n",
        "        normalized_class_name = normalize_string(class_name)\n",
        "        print_hierarchy(normalized_class_name, all_relations, writer)\n",
        "\n",
        "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### save the URIs in csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered class data has been saved to ontology_classes.csv\n",
            "Filtered class relations have been saved to ontology_relations.csv\n",
            "\n",
            "Initial class names found in the output:\n",
            "Class 'Compression' found in:\n",
            "- ../Ontologies/materialsmine_converted.ttl\n",
            "Class 'AmperePerJoule' found in:\n",
            "- ../Ontologies/emmo.ttl\n",
            "Class 'nfdi' not found in the output.\n",
            "Class 'stress' found in:\n",
            "- ../Ontologies/materialsmine_converted.ttl\n",
            "- ../Ontologies/emmo.ttl\n",
            "\n",
            "Saving class hierarchy to CSV file:\n",
            "http://materialsmine.org/ns/Compression Compression is subClassOf http://materialsmine.org/ns/ViscoelasticProperty Viscoelastic Property (from ../Ontologies/materialsmine_converted.ttl)\n",
            "  http://materialsmine.org/ns/ViscoelasticProperty Viscoelastic Property is subClassOf http://semanticscience.org/resource/Quantity Amount (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    http://semanticscience.org/resource/Quantity Amount is subClassOf http://semanticscience.org/resource/MeasurementValue measurement value (from ../Ontologies/materialsmine_converted.ttl)\n",
            "      http://semanticscience.org/resource/MeasurementValue measurement value is subClassOf http://semanticscience.org/resource/Number number (from ../Ontologies/materialsmine_converted.ttl)\n",
            "        http://semanticscience.org/resource/Number number is subClassOf http://semanticscience.org/resource/Scalar scalar (from ../Ontologies/materialsmine_converted.ttl)\n",
            "          http://semanticscience.org/resource/Scalar scalar is subClassOf http://semanticscience.org/resource/Tensor tensor (from ../Ontologies/materialsmine_converted.ttl)\n",
            "            http://semanticscience.org/resource/Tensor tensor is subClassOf http://semanticscience.org/resource/MathematicalEntity mathematical entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              http://semanticscience.org/resource/MathematicalEntity mathematical entity is subClassOf http://semanticscience.org/resource/InformationContentEntity information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf http://semanticscience.org/resource/Object object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  http://semanticscience.org/resource/Object object is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  http://semanticscience.org/resource/Object object is subClassOf nb4afb35c038947bbaab98e485a623610b764 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  https://w3id.org/emmo#EMMO_6f5af708_f825_4feb_a0d1_a8d813d3022b Object is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b2342 None (from ../Ontologies/emmo.ttl)\n",
            "                http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf nb4afb35c038947bbaab98e485a623610b607 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "        https://w3id.org/emmo#EMMO_21f56795_ee72_4858_b571_11cfaa59c1a8 Number is subClassOf https://w3id.org/emmo#EMMO_4ce76d7f_03f8_45b6_9003_90052a79bfaa Numerical (from ../Ontologies/emmo.ttl)\n",
            "          https://w3id.org/emmo#EMMO_4ce76d7f_03f8_45b6_9003_90052a79bfaa Numerical is subClassOf https://w3id.org/emmo#EMMO_54ee6b5e_5261_44a8_86eb_5717e7fdb9d0 Mathematical (from ../Ontologies/emmo.ttl)\n",
            "            https://w3id.org/emmo#EMMO_54ee6b5e_5261_44a8_86eb_5717e7fdb9d0 Mathematical is subClassOf https://w3id.org/emmo#EMMO_d8d2144e_5c8d_455d_a643_5caf4d8d9df8 Language (from ../Ontologies/emmo.ttl)\n",
            "              http://semanticscience.org/resource/Language Language is subClassOf http://semanticscience.org/resource/LanguageEntity language entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                http://semanticscience.org/resource/LanguageEntity language entity is subClassOf http://semanticscience.org/resource/InformationContentEntity information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf http://semanticscience.org/resource/Object object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    http://semanticscience.org/resource/Object object is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                      http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    http://semanticscience.org/resource/Object object is subClassOf nb4afb35c038947bbaab98e485a623610b764 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    https://w3id.org/emmo#EMMO_6f5af708_f825_4feb_a0d1_a8d813d3022b Object is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b2342 None (from ../Ontologies/emmo.ttl)\n",
            "                  http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf nb4afb35c038947bbaab98e485a623610b607 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                http://semanticscience.org/resource/LanguageEntity language entity is subClassOf nb4afb35c038947bbaab98e485a623610b647 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              http://semanticscience.org/resource/Language language is subClassOf http://semanticscience.org/resource/LanguageEntity language entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                http://semanticscience.org/resource/LanguageEntity language entity is subClassOf http://semanticscience.org/resource/InformationContentEntity information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf http://semanticscience.org/resource/Object object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    http://semanticscience.org/resource/Object object is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                      http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    http://semanticscience.org/resource/Object object is subClassOf nb4afb35c038947bbaab98e485a623610b764 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    https://w3id.org/emmo#EMMO_6f5af708_f825_4feb_a0d1_a8d813d3022b Object is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b2342 None (from ../Ontologies/emmo.ttl)\n",
            "                  http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf nb4afb35c038947bbaab98e485a623610b607 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                http://semanticscience.org/resource/LanguageEntity language entity is subClassOf nb4afb35c038947bbaab98e485a623610b647 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              https://w3id.org/emmo#EMMO_d8d2144e_5c8d_455d_a643_5caf4d8d9df8 Language is subClassOf https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic (from ../Ontologies/emmo.ttl)\n",
            "                https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic is subClassOf https://w3id.org/emmo#EMMO_be8592a7_68d1_4a06_ad23_82f2b56ef926 DiscreteData (from ../Ontologies/emmo.ttl)\n",
            "                https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b2364 None (from ../Ontologies/emmo.ttl)\n",
            "        https://w3id.org/emmo#EMMO_21f56795_ee72_4858_b571_11cfaa59c1a8 Number is subClassOf https://w3id.org/emmo#EMMO_a1083d0a_c1fb_471f_8e20_a98f881ad527 AlphabeticEntity (from ../Ontologies/emmo.ttl)\n",
            "          https://w3id.org/emmo#EMMO_a1083d0a_c1fb_471f_8e20_a98f881ad527 AlphabeticEntity is subClassOf https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic (from ../Ontologies/emmo.ttl)\n",
            "            https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic is subClassOf https://w3id.org/emmo#EMMO_be8592a7_68d1_4a06_ad23_82f2b56ef926 DiscreteData (from ../Ontologies/emmo.ttl)\n",
            "            https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b2364 None (from ../Ontologies/emmo.ttl)\n",
            "      http://semanticscience.org/resource/MeasurementValue measurement value is subClassOf nb4afb35c038947bbaab98e485a623610b693 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    http://semanticscience.org/resource/Quantity Amount is subClassOf nb4afb35c038947bbaab98e485a623610b928 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    http://semanticscience.org/resource/Quantity Amount is subClassOf nb4afb35c038947bbaab98e485a623610b929 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    http://semanticscience.org/resource/Quantity Amount is equivalentClass nb4afb35c038947bbaab98e485a623610b925 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "https://w3id.org/emmo#AmperePerJoule Ampere per Joule is subClassOf n8456f385279a41e9b7627c6ec9a4ea6eb17 None (from ../Ontologies/emmo.ttl)\n",
            "https://w3id.org/emmo#AmperePerJoule Ampere per Joule is subClassOf n8456f385279a41e9b7627c6ec9a4ea6eb18 None (from ../Ontologies/emmo.ttl)\n",
            "https://w3id.org/emmo#AmperePerJoule Ampere per Joule is subClassOf https://w3id.org/emmo#EMMO_1273eb34_de48_43a9_925f_104110469dd2 SICoherentDerivedUnit (from ../Ontologies/emmo.ttl)\n",
            "  https://w3id.org/emmo#EMMO_1273eb34_de48_43a9_925f_104110469dd2 SICoherentDerivedUnit is subClassOf https://w3id.org/emmo#EMMO_08b308d4_31cd_4779_a784_aa92fc730f39 DerivedUnit (from ../Ontologies/emmo.ttl)\n",
            "https://w3id.org/emmo#AmperePerJoule Ampere per Joule is subClassOf https://w3id.org/emmo#EMMO_1aaaceb6_c5eb_4cf3_a494_f82d43fda10a ElectricCurrentPerUnitEnergyUnit (from ../Ontologies/emmo.ttl)\n",
            "  https://w3id.org/emmo#EMMO_1aaaceb6_c5eb_4cf3_a494_f82d43fda10a ElectricCurrentPerUnitEnergyUnit is subClassOf https://w3id.org/emmo#EMMO_9895a1b4_f0a5_4167_ac5e_97db40b8bfcc SIDimensionalUnit (from ../Ontologies/emmo.ttl)\n",
            "    https://w3id.org/emmo#EMMO_9895a1b4_f0a5_4167_ac5e_97db40b8bfcc SIDimensionalUnit is subClassOf https://w3id.org/emmo#EMMO_cbdea88b_fef1_4c7c_b69f_ae1f0f241c4a DimensionalUnit (from ../Ontologies/emmo.ttl)\n",
            "  https://w3id.org/emmo#EMMO_1aaaceb6_c5eb_4cf3_a494_f82d43fda10a ElectricCurrentPerUnitEnergyUnit is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b1511 None (from ../Ontologies/emmo.ttl)\n",
            "https://w3id.org/emmo#AmperePerJoule AmperePerJoule is subClassOf n8456f385279a41e9b7627c6ec9a4ea6eb17 None (from ../Ontologies/emmo.ttl)\n",
            "https://w3id.org/emmo#AmperePerJoule AmperePerJoule is subClassOf n8456f385279a41e9b7627c6ec9a4ea6eb18 None (from ../Ontologies/emmo.ttl)\n",
            "https://w3id.org/emmo#AmperePerJoule AmperePerJoule is subClassOf https://w3id.org/emmo#EMMO_1273eb34_de48_43a9_925f_104110469dd2 SICoherentDerivedUnit (from ../Ontologies/emmo.ttl)\n",
            "  https://w3id.org/emmo#EMMO_1273eb34_de48_43a9_925f_104110469dd2 SICoherentDerivedUnit is subClassOf https://w3id.org/emmo#EMMO_08b308d4_31cd_4779_a784_aa92fc730f39 DerivedUnit (from ../Ontologies/emmo.ttl)\n",
            "https://w3id.org/emmo#AmperePerJoule AmperePerJoule is subClassOf https://w3id.org/emmo#EMMO_1aaaceb6_c5eb_4cf3_a494_f82d43fda10a ElectricCurrentPerUnitEnergyUnit (from ../Ontologies/emmo.ttl)\n",
            "  https://w3id.org/emmo#EMMO_1aaaceb6_c5eb_4cf3_a494_f82d43fda10a ElectricCurrentPerUnitEnergyUnit is subClassOf https://w3id.org/emmo#EMMO_9895a1b4_f0a5_4167_ac5e_97db40b8bfcc SIDimensionalUnit (from ../Ontologies/emmo.ttl)\n",
            "    https://w3id.org/emmo#EMMO_9895a1b4_f0a5_4167_ac5e_97db40b8bfcc SIDimensionalUnit is subClassOf https://w3id.org/emmo#EMMO_cbdea88b_fef1_4c7c_b69f_ae1f0f241c4a DimensionalUnit (from ../Ontologies/emmo.ttl)\n",
            "  https://w3id.org/emmo#EMMO_1aaaceb6_c5eb_4cf3_a494_f82d43fda10a ElectricCurrentPerUnitEnergyUnit is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b1511 None (from ../Ontologies/emmo.ttl)\n",
            "http://materialsmine.org/ns/Stress Stress is subClassOf http://materialsmine.org/ns/MechanicalProperty Mechanical Property (from ../Ontologies/materialsmine_converted.ttl)\n",
            "  http://materialsmine.org/ns/MechanicalProperty Mechanical Property is subClassOf http://semanticscience.org/resource/Quantity Amount (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    http://semanticscience.org/resource/Quantity Amount is subClassOf http://semanticscience.org/resource/MeasurementValue measurement value (from ../Ontologies/materialsmine_converted.ttl)\n",
            "      http://semanticscience.org/resource/MeasurementValue measurement value is subClassOf http://semanticscience.org/resource/Number number (from ../Ontologies/materialsmine_converted.ttl)\n",
            "        http://semanticscience.org/resource/Number number is subClassOf http://semanticscience.org/resource/Scalar scalar (from ../Ontologies/materialsmine_converted.ttl)\n",
            "          http://semanticscience.org/resource/Scalar scalar is subClassOf http://semanticscience.org/resource/Tensor tensor (from ../Ontologies/materialsmine_converted.ttl)\n",
            "            http://semanticscience.org/resource/Tensor tensor is subClassOf http://semanticscience.org/resource/MathematicalEntity mathematical entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              http://semanticscience.org/resource/MathematicalEntity mathematical entity is subClassOf http://semanticscience.org/resource/InformationContentEntity information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf http://semanticscience.org/resource/Object object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  http://semanticscience.org/resource/Object object is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  http://semanticscience.org/resource/Object object is subClassOf nb4afb35c038947bbaab98e485a623610b764 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  https://w3id.org/emmo#EMMO_6f5af708_f825_4feb_a0d1_a8d813d3022b Object is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b2342 None (from ../Ontologies/emmo.ttl)\n",
            "                http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf nb4afb35c038947bbaab98e485a623610b607 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "        https://w3id.org/emmo#EMMO_21f56795_ee72_4858_b571_11cfaa59c1a8 Number is subClassOf https://w3id.org/emmo#EMMO_4ce76d7f_03f8_45b6_9003_90052a79bfaa Numerical (from ../Ontologies/emmo.ttl)\n",
            "          https://w3id.org/emmo#EMMO_4ce76d7f_03f8_45b6_9003_90052a79bfaa Numerical is subClassOf https://w3id.org/emmo#EMMO_54ee6b5e_5261_44a8_86eb_5717e7fdb9d0 Mathematical (from ../Ontologies/emmo.ttl)\n",
            "            https://w3id.org/emmo#EMMO_54ee6b5e_5261_44a8_86eb_5717e7fdb9d0 Mathematical is subClassOf https://w3id.org/emmo#EMMO_d8d2144e_5c8d_455d_a643_5caf4d8d9df8 Language (from ../Ontologies/emmo.ttl)\n",
            "              http://semanticscience.org/resource/Language Language is subClassOf http://semanticscience.org/resource/LanguageEntity language entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                http://semanticscience.org/resource/LanguageEntity language entity is subClassOf http://semanticscience.org/resource/InformationContentEntity information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf http://semanticscience.org/resource/Object object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    http://semanticscience.org/resource/Object object is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                      http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    http://semanticscience.org/resource/Object object is subClassOf nb4afb35c038947bbaab98e485a623610b764 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    https://w3id.org/emmo#EMMO_6f5af708_f825_4feb_a0d1_a8d813d3022b Object is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b2342 None (from ../Ontologies/emmo.ttl)\n",
            "                  http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf nb4afb35c038947bbaab98e485a623610b607 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                http://semanticscience.org/resource/LanguageEntity language entity is subClassOf nb4afb35c038947bbaab98e485a623610b647 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              http://semanticscience.org/resource/Language language is subClassOf http://semanticscience.org/resource/LanguageEntity language entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                http://semanticscience.org/resource/LanguageEntity language entity is subClassOf http://semanticscience.org/resource/InformationContentEntity information content entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                  http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf http://semanticscience.org/resource/Object object (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    http://semanticscience.org/resource/Object object is subClassOf http://semanticscience.org/resource/Entity entity (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                      http://semanticscience.org/resource/Entity entity is subClassOf http://www.w3.org/2002/07/owl#Thing None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    http://semanticscience.org/resource/Object object is subClassOf nb4afb35c038947bbaab98e485a623610b764 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                    https://w3id.org/emmo#EMMO_6f5af708_f825_4feb_a0d1_a8d813d3022b Object is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b2342 None (from ../Ontologies/emmo.ttl)\n",
            "                  http://semanticscience.org/resource/InformationContentEntity information content entity is subClassOf nb4afb35c038947bbaab98e485a623610b607 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "                http://semanticscience.org/resource/LanguageEntity language entity is subClassOf nb4afb35c038947bbaab98e485a623610b647 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "              https://w3id.org/emmo#EMMO_d8d2144e_5c8d_455d_a643_5caf4d8d9df8 Language is subClassOf https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic (from ../Ontologies/emmo.ttl)\n",
            "                https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic is subClassOf https://w3id.org/emmo#EMMO_be8592a7_68d1_4a06_ad23_82f2b56ef926 DiscreteData (from ../Ontologies/emmo.ttl)\n",
            "                https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b2364 None (from ../Ontologies/emmo.ttl)\n",
            "        https://w3id.org/emmo#EMMO_21f56795_ee72_4858_b571_11cfaa59c1a8 Number is subClassOf https://w3id.org/emmo#EMMO_a1083d0a_c1fb_471f_8e20_a98f881ad527 AlphabeticEntity (from ../Ontologies/emmo.ttl)\n",
            "          https://w3id.org/emmo#EMMO_a1083d0a_c1fb_471f_8e20_a98f881ad527 AlphabeticEntity is subClassOf https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic (from ../Ontologies/emmo.ttl)\n",
            "            https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic is subClassOf https://w3id.org/emmo#EMMO_be8592a7_68d1_4a06_ad23_82f2b56ef926 DiscreteData (from ../Ontologies/emmo.ttl)\n",
            "            https://w3id.org/emmo#EMMO_057e7d57_aff0_49de_911a_8861d85cef40 Symbolic is equivalentClass n76428c8aaeaa43b388d6f33d2687e5d6b2364 None (from ../Ontologies/emmo.ttl)\n",
            "      http://semanticscience.org/resource/MeasurementValue measurement value is subClassOf nb4afb35c038947bbaab98e485a623610b693 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    http://semanticscience.org/resource/Quantity Amount is subClassOf nb4afb35c038947bbaab98e485a623610b928 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    http://semanticscience.org/resource/Quantity Amount is subClassOf nb4afb35c038947bbaab98e485a623610b929 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "    http://semanticscience.org/resource/Quantity Amount is equivalentClass nb4afb35c038947bbaab98e485a623610b925 None (from ../Ontologies/materialsmine_converted.ttl)\n",
            "https://w3id.org/emmo#EMMO_d1917609_db5e_4b8a_9b76_ef1d6f860a81 Stress is subClassOf n8456f385279a41e9b7627c6ec9a4ea6eb714 None (from ../Ontologies/emmo.ttl)\n",
            "https://w3id.org/emmo#EMMO_d1917609_db5e_4b8a_9b76_ef1d6f860a81 Stress is subClassOf https://w3id.org/emmo#EMMO_2946d40b_24a1_47fa_8176_e3f79bb45064 ISQDerivedQuantity (from ../Ontologies/emmo.ttl)\n",
            "  https://w3id.org/emmo#EMMO_2946d40b_24a1_47fa_8176_e3f79bb45064 ISQDerivedQuantity is subClassOf https://w3id.org/emmo#EMMO_71f6ab56_342c_484b_bbe0_de86b7367cb3 DerivedQuantity (from ../Ontologies/emmo.ttl)\n",
            "  https://w3id.org/emmo#EMMO_2946d40b_24a1_47fa_8176_e3f79bb45064 ISQDerivedQuantity is subClassOf https://w3id.org/emmo#EMMO_f35cff4d_dc09_44cf_a729_22fb79e3bfb2 InternationalSystemOfQuantity (from ../Ontologies/emmo.ttl)\n",
            "    https://w3id.org/emmo#EMMO_f35cff4d_dc09_44cf_a729_22fb79e3bfb2 InternationalSystemOfQuantity is subClassOf https://w3id.org/emmo#EMMO_9c407ac0_fd4c_4178_8763_95fad9fe29ec StandardizedPhysicalQuantity (from ../Ontologies/emmo.ttl)\n",
            "      https://w3id.org/emmo#EMMO_9c407ac0_fd4c_4178_8763_95fad9fe29ec StandardizedPhysicalQuantity is subClassOf https://w3id.org/emmo#EMMO_02c0621e_a527_4790_8a0f_2bb51973c819 PhysicalQuantity (from ../Ontologies/emmo.ttl)\n",
            "        https://w3id.org/emmo#EMMO_02c0621e_a527_4790_8a0f_2bb51973c819 PhysicalQuantity is subClassOf n76428c8aaeaa43b388d6f33d2687e5d6b2361 None (from ../Ontologies/emmo.ttl)\n",
            "Class hierarchy has been saved to class_hierarchy.csv\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "    s = s.replace('...', '')\n",
        "    return s\n",
        "\n",
        "def get_class_label(g, cls):\n",
        "    labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "    return labels[0] if labels else None\n",
        "\n",
        "def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "    if file_path.endswith('.ttl'):\n",
        "        file_format = 'ttl'\n",
        "    elif file_path.endswith('.owl'):\n",
        "        file_format = 'xml'\n",
        "    elif file_path.endswith('.xrdf'):\n",
        "        file_format = 'xml'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
        "    \n",
        "    g = Graph()\n",
        "    g.parse(file_path, format=file_format)\n",
        "\n",
        "    # Hardcode label change for the specific file and class\n",
        "    # if file_path == 'materials.ttl':\n",
        "    #     for cls in g.subjects(RDF.type, OWL.Class):\n",
        "    #         for label in g.objects(cls, RDFS.label):\n",
        "    #             if str(label) == 'amount':\n",
        "    #                 g.remove((cls, RDFS.label, label))\n",
        "    #                 g.add((cls, RDFS.label, Literal('quantity')))\n",
        "\n",
        "    normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "    # dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
        "    # isPartOf = dcterms.isPartOf\n",
        "\n",
        "\n",
        "    ex = Namespace(\"http://example.org/ontology/\")\n",
        "    sio = Namespace(\"http://semanticscience.org/resource/\")\n",
        "    skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
        "    owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
        "    rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "    materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
        "    bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
        "    rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
        "    xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
        "    xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
        "    foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
        "    # isPartOf = Namespace(\"http://semanticscience.org/resource/isPartOf\")\n",
        "    dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
        "    isPartOf = dcterms.isPartOf\n",
        "\n",
        "    # print(isPartOf)\n",
        "\n",
        "    classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "    data = []\n",
        "    relations = []\n",
        "    found_class_labels = set()\n",
        "    for cls in classes:\n",
        "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "        for label in labels:\n",
        "            normalized_label = normalize_string(label)\n",
        "            found_class_labels.add(normalized_label)\n",
        "            if normalized_label in normalized_class_names_to_check:\n",
        "                data.append([file_path, str(cls), str(label)])\n",
        "                for obj in g.objects(cls, RDFS.subClassOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, OWL.equivalentClass):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "                for obj in g.objects(cls, isPartOf):\n",
        "                    obj_label = get_class_label(g, obj)\n",
        "                    relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label)])\n",
        "\n",
        "    return data, relations, found_class_labels\n",
        "\n",
        "def filter_relations(all_relations, initial_class_names_to_check):\n",
        "    normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "    return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "def print_hierarchy(class_name, relations, writer):\n",
        "    def recursive_print(class_name, depth=0):\n",
        "        for relation in relations:\n",
        "            if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "                writer.writerow([relation[1], relation[2], relation[3], relation[4], relation[5], relation[0]])\n",
        "                indent = '  ' * depth\n",
        "                print(f\"{indent}{relation[1]} {relation[2]} is {relation[3]} {relation[4]} {relation[5]} (from {relation[0]})\")\n",
        "                recursive_print(relation[5], depth + 1)\n",
        "\n",
        "    recursive_print(class_name)\n",
        "\n",
        "\n",
        "output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "class_output_file = \"ontology_classes.csv\"\n",
        "relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "all_data = []\n",
        "all_relations = []\n",
        "all_found_class_labels = set()\n",
        "\n",
        "class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "max_iterations = 2\n",
        "iteration_count = 0\n",
        "\n",
        "while class_names_to_check and iteration_count < max_iterations:\n",
        "    iteration_count += 1\n",
        "    new_data = []\n",
        "    new_relations = []\n",
        "    new_found_class_labels = set()\n",
        "\n",
        "    for ontology_file in ontology_files:\n",
        "        file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "        new_data.extend(file_data)\n",
        "        new_relations.extend(file_relations)\n",
        "        new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "    all_data.extend(new_data)\n",
        "    all_relations.extend(new_relations)\n",
        "    all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "    class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "    class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "    writer.writerows(filtered_data)\n",
        "\n",
        "filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "    writer.writerows(filtered_relations)\n",
        "\n",
        "print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "print(\"\\nInitial class names found in the output:\")\n",
        "for class_name in initial_class_names_to_check:\n",
        "    normalized_class_name = normalize_string(class_name)\n",
        "    found = False\n",
        "    for label in all_found_class_labels:\n",
        "        if normalized_class_name in label:\n",
        "            found = True\n",
        "            print(f\"Class '{class_name}' found in:\")\n",
        "            for ontology_file in ontology_files:\n",
        "                file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "                normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "                if normalized_class_name in normalized_labels:\n",
        "                    print(f\"- {ontology_file}\")\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([ \"Class URI\", \"Class Name\", \"Relation Type\", \"Related Class URI\", \"Related Class Name\" , \"File\"])\n",
        "    for class_name in initial_class_names_to_check:\n",
        "        normalized_class_name = normalize_string(class_name)\n",
        "        print_hierarchy(normalized_class_name, all_relations, writer)\n",
        "\n",
        "print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of ontology files to process\n",
        "ontology_files = [\n",
        "    # \"../Ontologies/materialsmine.ttl\", ### is not complete!\n",
        "    \"../Ontologies/materialsmine_converted.ttl\",\n",
        "    # \"../Ontologies/pmdco_core.ttl\",\n",
        "    # \"../Ontologies/nfdicore_2.ttl\",\n",
        "    # \"../Ontologies/bfo.owl\",\n",
        "    \"../Ontologies/emmo.ttl\",\n",
        "    # \"../Ontologies/owlapi.xrdf\",\n",
        "    # \"../Ontologies/schemaorg.owl\",\n",
        "    # \"../Ontologies/MaterialsMine.xrdf\",\n",
        "    # '../Ontologies/emmo.owl', ### has problem of reading file\n",
        "    # \"../Ontologies/Physical_Activity_Ontology_V2.owl\",\n",
        "    # \"../Ontologies/Physical_Activity_Ontology_V2.xrdf\",\n",
        "    # \"../Ontologies/oboe.owl\",\n",
        "    # Add more file paths as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import csv\n",
        "# import re\n",
        "# from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
        "\n",
        "# def normalize_string(s):\n",
        "#     s = s.lower()\n",
        "#     s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "#     s = s.replace('...', '')\n",
        "#     return s\n",
        "\n",
        "# def get_class_label(g, cls):\n",
        "#     labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "#     return labels[0] if labels else None\n",
        "\n",
        "# def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "#     if file_path.endswith('.ttl'):\n",
        "#         file_format = 'ttl'\n",
        "#     elif file_path.endswith('.owl'):\n",
        "#         file_format = 'xml'\n",
        "#     elif file_path.endswith('.xrdf'):\n",
        "#         file_format = 'xml'\n",
        "#     else:\n",
        "#         raise ValueError(\"Unsupported file format. Only .ttl, .owl, and xrdf files are supported.\")\n",
        "    \n",
        "#     g = Graph()\n",
        "#     g.parse(file_path, format=file_format)\n",
        "\n",
        "#     # Hardcode label change for the specific file and class\n",
        "#     # if file_path == 'materials.ttl':\n",
        "#     #     for cls in g.subjects(RDF.type, OWL.Class):\n",
        "#     #         for label in g.objects(cls, RDFS.label):\n",
        "#     #             if str(label) == 'amount':\n",
        "#     #                 g.remove((cls, RDFS.label, label))\n",
        "#     #                 g.add((cls, RDFS.label, Literal('quantity')))\n",
        "\n",
        "#     normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "#     ex = Namespace(\"http://example.org/ontology/\")\n",
        "#     sio = Namespace(\"http://semanticscience.org/resource/\")\n",
        "#     skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
        "#     owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
        "#     rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "#     materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
        "#     bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
        "#     rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
        "#     xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
        "#     xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
        "#     foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
        "#     # isPartOf = Namespace(\"http://semanticscience.org/resource/isPartOf\")\n",
        "#     dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
        "#     isPartOf = dcterms.isPartOf\n",
        "\n",
        "#     # print(isPartOf)\n",
        "\n",
        "#     classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "#     data = []\n",
        "#     relations = []\n",
        "#     found_class_labels = set()\n",
        "#     for cls in classes:\n",
        "#         labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "#         for label in labels:\n",
        "#             normalized_label = normalize_string(label)\n",
        "#             found_class_labels.add(normalized_label)\n",
        "#             if normalized_label in normalized_class_names_to_check:\n",
        "#                 data.append([file_path, str(cls), str(label)])\n",
        "#                 for obj in g.objects(cls, RDFS.subClassOf):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, OWL.equivalentClass):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, isPartOf):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, OWL.onProperty):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'onProperty', str(obj), str(obj_label)])\n",
        "\n",
        "#     return data, relations, found_class_labels\n",
        "\n",
        "# def filter_relations(all_relations, initial_class_names_to_check):\n",
        "#     normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "#     return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "# def print_hierarchy(class_name, relations, writer):\n",
        "#     def recursive_print(class_name, depth=0):\n",
        "#         for relation in relations:\n",
        "#             if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "#                 writer.writerow([relation[2], relation[3], relation[5], relation[0]])\n",
        "#                 indent = '  ' * depth\n",
        "#                 print(f\"{indent}{relation[2]} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "#                 recursive_print(relation[5], depth + 1)\n",
        "\n",
        "#     recursive_print(class_name)\n",
        "\n",
        "# output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "# class_output_file = \"ontology_classes.csv\"\n",
        "# relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "# all_data = []\n",
        "# all_relations = []\n",
        "# all_found_class_labels = set()\n",
        "\n",
        "# class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "# max_iterations = 2\n",
        "# iteration_count = 0\n",
        "\n",
        "# while class_names_to_check and iteration_count < max_iterations:\n",
        "#     iteration_count += 1\n",
        "#     new_data = []\n",
        "#     new_relations = []\n",
        "#     new_found_class_labels = set()\n",
        "\n",
        "#     for ontology_file in ontology_files:\n",
        "#         file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "#         new_data.extend(file_data)\n",
        "#         new_relations.extend(file_relations)\n",
        "#         new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "#     all_data.extend(new_data)\n",
        "#     all_relations.extend(new_relations)\n",
        "#     all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "#     class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "#     class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "# filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "# with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "#     writer.writerows(filtered_data)\n",
        "\n",
        "# filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "# with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "#     writer.writerows(filtered_relations)\n",
        "\n",
        "# print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "# print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "# print(\"\\nInitial class names found in the output:\")\n",
        "# for class_name in initial_class_names_to_check:\n",
        "#     normalized_class_name = normalize_string(class_name)\n",
        "#     found = False\n",
        "#     for label in all_found_class_labels:\n",
        "#         if normalized_class_name in label:\n",
        "#             found = True\n",
        "#             print(f\"Class '{class_name}' found in:\")\n",
        "#             for ontology_file in ontology_files:\n",
        "#                 file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "#                 normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "#                 if normalized_class_name in normalized_labels:\n",
        "#                     print(f\"- {ontology_file}\")\n",
        "#             break\n",
        "#     if not found:\n",
        "#         print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "# print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "# with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"Class Name\", \"Relation Type\", \"Related Class\", \"From File\"])\n",
        "#     for class_name in initial_class_names_to_check:\n",
        "#         normalized_class_name = normalize_string(class_name)\n",
        "#         print_hierarchy(normalized_class_name, all_relations, writer)\n",
        "\n",
        "# print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of class names to check\n",
        "initial_class_names_to_check = [ 'Compression','AmperePerJoule','nfdi','stress']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import csv\n",
        "# import re\n",
        "# from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal\n",
        "\n",
        "# def normalize_string(s):\n",
        "#     s = s.lower()\n",
        "#     s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "#     s = s.replace('...', '')\n",
        "#     return s\n",
        "\n",
        "# def get_class_label(g, cls):\n",
        "#     labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "#     return labels[0] if labels else None\n",
        "\n",
        "# def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "#     if file_path.endswith('.ttl'):\n",
        "#         file_format = 'ttl'\n",
        "#     elif file_path.endswith('.owl'):\n",
        "#         file_format = 'xml'\n",
        "#     elif file_path.endswith('.xrdf'):\n",
        "#         file_format = 'xml'\n",
        "#     else:\n",
        "#         raise ValueError(\"Unsupported file format. Only .ttl, .owl, and .xrdf files are supported.\")\n",
        "    \n",
        "#     g = Graph()\n",
        "#     g.parse(file_path, format=file_format)\n",
        "\n",
        "#     normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "#     ex = Namespace(\"http://example.org/ontology/\")\n",
        "#     sio = Namespace(\"http://semanticscience.org/resource/\")\n",
        "#     skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
        "#     owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
        "#     rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "#     materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
        "#     bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
        "#     rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
        "#     xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
        "#     xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
        "#     foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
        "#     dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
        "#     emmo = Namespace(\"https://w3id.org/emmo/ontology/\")\n",
        "\n",
        "#     # Define namespaces for isPartOf and onProperty correctly\n",
        "#     isPartOf = dcterms.isPartOf\n",
        "#     onProperty = OWL.onProperty\n",
        "\n",
        "#     classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "#     data = []\n",
        "#     relations = []\n",
        "#     found_class_labels = set()\n",
        "#     for cls in classes:\n",
        "#         labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "#         for label in labels:\n",
        "#             normalized_label = normalize_string(label)\n",
        "#             found_class_labels.add(normalized_label)\n",
        "#             if normalized_label in normalized_class_names_to_check:\n",
        "#                 data.append([file_path, str(cls), str(label)])\n",
        "#                 for obj in g.objects(cls, RDFS.subClassOf):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, OWL.equivalentClass):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, isPartOf):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, onProperty):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'onProperty', str(obj), str(obj_label)])\n",
        "\n",
        "#     return data, relations, found_class_labels\n",
        "\n",
        "# def filter_relations(all_relations, initial_class_names_to_check):\n",
        "#     normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "#     return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "# def print_hierarchy(class_name, relations, writer):\n",
        "#     def recursive_print(class_name, depth=0):\n",
        "#         for relation in relations:\n",
        "#             if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "#                 writer.writerow([relation[2], relation[3], relation[5], relation[0]])\n",
        "#                 indent = '  ' * depth\n",
        "#                 print(f\"{indent}{relation[2]} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "#                 recursive_print(relation[5], depth + 1)\n",
        "\n",
        "#     recursive_print(class_name)\n",
        "\n",
        "# output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "# class_output_file = \"ontology_classes.csv\"\n",
        "# relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "# all_data = []\n",
        "# all_relations = []\n",
        "# all_found_class_labels = set()\n",
        "\n",
        "# class_names_to_check = initial_class_names_to_check\n",
        "\n",
        "# max_iterations = 2\n",
        "# iteration_count = 0\n",
        "\n",
        "# while class_names_to_check and iteration_count < max_iterations:\n",
        "#     iteration_count += 1\n",
        "#     new_data = []\n",
        "#     new_relations = []\n",
        "#     new_found_class_labels = set()\n",
        "\n",
        "#     for ontology_file in ontology_files:\n",
        "#         file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check)\n",
        "#         new_data.extend(file_data)\n",
        "#         new_relations.extend(file_relations)\n",
        "#         new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "#     all_data.extend(new_data)\n",
        "#     all_relations.extend(new_relations)\n",
        "#     all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "#     class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "\n",
        "#     class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
        "\n",
        "# filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "# with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "#     writer.writerows(filtered_data)\n",
        "\n",
        "# filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "# with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "#     writer.writerows(filtered_relations)\n",
        "\n",
        "# print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "# print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "# print(\"\\nInitial class names found in the output:\")\n",
        "# for class_name in initial_class_names_to_check:\n",
        "#     normalized_class_name = normalize_string(class_name)\n",
        "#     found = False\n",
        "#     for label in all_found_class_labels:\n",
        "#         if normalized_class_name in label:\n",
        "#             found = True\n",
        "#             print(f\"Class '{class_name}' found in:\")\n",
        "#             for ontology_file in ontology_files:\n",
        "#                 file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "#                 normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "#                 if normalized_class_name in normalized_labels:\n",
        "#                     print(f\"- {ontology_file}\")\n",
        "#             break\n",
        "#     if not found:\n",
        "#         print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "# print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "# with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"Class Name\", \"Relation Type\", \"Related Class\", \"From File\"])\n",
        "#     for class_name in initial_class_names_to_check:\n",
        "#         normalized_class_name = normalize_string(class_name)\n",
        "#         print_hierarchy(normalized_class_name, all_relations, writer)\n",
        "\n",
        "# print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import csv\n",
        "# import re\n",
        "# from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, Literal\n",
        "\n",
        "# def normalize_string(s):\n",
        "#     s = s.lower()\n",
        "#     s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "#     s = s.replace('...', '')\n",
        "#     return s\n",
        "\n",
        "# def get_class_label(g, cls):\n",
        "#     labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "#     return labels[0] if labels else None\n",
        "\n",
        "# def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "#     if file_path.endswith('.ttl'):\n",
        "#         file_format = 'ttl'\n",
        "#     elif file_path.endswith('.owl'):\n",
        "#         file_format = 'xml'\n",
        "#     elif file_path.endswith('.xrdf'):\n",
        "#         file_format = 'xml'\n",
        "#     else:\n",
        "#         raise ValueError(\"Unsupported file format. Only .ttl, .owl, and .xrdf files are supported.\")\n",
        "    \n",
        "#     g = Graph()\n",
        "#     g.parse(file_path, format=file_format)\n",
        "\n",
        "#     normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "#     classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "#     data = []\n",
        "#     relations = []\n",
        "#     found_class_labels = set()\n",
        "    \n",
        "#     for cls in classes:\n",
        "#         labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "#         for label in labels:\n",
        "#             normalized_label = normalize_string(label)\n",
        "#             found_class_labels.add(normalized_label)\n",
        "#             if normalized_label in normalized_class_names_to_check:\n",
        "#                 data.append([file_path, str(cls), str(label)])\n",
        "#                 for obj in g.objects(cls, RDFS.subClassOf):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, OWL.equivalentClass):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, RDFS.isDefinedBy):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'isDefinedBy', str(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, OWL.disjointWith):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'disjointWith', str(obj), str(obj_label)])\n",
        "#                 for prop in g.objects(cls, RDFS.domain):\n",
        "#                     prop_label = get_class_label(g, prop)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'domain', str(prop), str(prop_label)])\n",
        "#                 for prop in g.objects(cls, RDFS.range):\n",
        "#                     prop_label = get_class_label(g, prop)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'range', str(prop), str(prop_label)])\n",
        "#                 for prop in g.objects(cls, OWL.inverseOf):\n",
        "#                     prop_label = get_class_label(g, prop)\n",
        "#                     relations.append([file_path, str(cls), str(label), 'inverseOf', str(prop), str(prop_label)])\n",
        "#                 for restriction in g.objects(cls, OWL.Restriction):\n",
        "#                     on_property = next(g.objects(restriction, OWL.onProperty), None)\n",
        "#                     if on_property:\n",
        "#                         on_property_label = get_class_label(g, on_property)\n",
        "#                         relations.append([file_path, str(cls), str(label), 'onProperty', str(on_property), str(on_property_label)])\n",
        "#                     all_values_from = next(g.objects(restriction, OWL.allValuesFrom), None)\n",
        "#                     if all_values_from:\n",
        "#                         all_values_from_label = get_class_label(g, all_values_from)\n",
        "#                         relations.append([file_path, str(cls), str(label), 'allValuesFrom', str(all_values_from), str(all_values_from_label)])\n",
        "#                     some_values_from = next(g.objects(restriction, OWL.someValuesFrom), None)\n",
        "#                     if some_values_from:\n",
        "#                         some_values_from_label = get_class_label(g, some_values_from)\n",
        "#                         relations.append([file_path, str(cls), str(label), 'someValuesFrom', str(some_values_from), str(some_values_from_label)])\n",
        "#                     has_value = next(g.objects(restriction, OWL.hasValue), None)\n",
        "#                     if has_value:\n",
        "#                         relations.append([file_path, str(cls), str(label), 'hasValue', None, str(has_value)])\n",
        "\n",
        "#     return data, relations, found_class_labels\n",
        "\n",
        "# def filter_relations(all_relations, initial_class_names_to_check):\n",
        "#     normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "#     return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "# def print_hierarchy(class_name, relations, writer):\n",
        "#     def recursive_print(class_name, depth=0):\n",
        "#         for relation in relations:\n",
        "#             if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "#                 writer.writerow([relation[2], relation[3], relation[5], relation[0]])\n",
        "#                 indent = '  ' * depth\n",
        "#                 print(f\"{indent}{relation[2]} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "#                 recursive_print(relation[5], depth + 1)\n",
        "\n",
        "#     recursive_print(class_name)\n",
        "\n",
        "# output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "# class_output_file = \"ontology_classes.csv\"\n",
        "# relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "\n",
        "# all_data = []\n",
        "# all_relations = []\n",
        "# all_found_class_labels = set()\n",
        "\n",
        "# max_iterations = 2\n",
        "# iteration_count = 0\n",
        "\n",
        "# while initial_class_names_to_check and iteration_count < max_iterations:\n",
        "#     iteration_count += 1\n",
        "#     new_data = []\n",
        "#     new_relations = []\n",
        "#     new_found_class_labels = set()\n",
        "\n",
        "#     for ontology_file in ontology_files:\n",
        "#         file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "#         new_data.extend(file_data)\n",
        "#         new_relations.extend(file_relations)\n",
        "#         new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "#     all_data.extend(new_data)\n",
        "#     all_relations.extend(new_relations)\n",
        "#     all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "#     initial_class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "#     initial_class_names_to_check = [normalize_string(name) for name in initial_class_names_to_check]\n",
        "\n",
        "# filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "# with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "#     writer.writerows(filtered_data)\n",
        "\n",
        "# filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "# with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "#     writer.writerows(filtered_relations)\n",
        "\n",
        "# print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "# print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "# print(\"\\nInitial class names found in the output:\")\n",
        "# for class_name in initial_class_names_to_check:\n",
        "#     normalized_class_name = normalize_string(class_name)\n",
        "#     found = False\n",
        "#     for label in all_found_class_labels:\n",
        "#         if normalized_class_name in label:\n",
        "#             found = True\n",
        "#             print(f\"Class '{class_name}' found in:\")\n",
        "#             for ontology_file in ontology_files:\n",
        "#                 file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "#                 normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "#                 if normalized_class_name in normalized_labels:\n",
        "#                     print(f\"- {ontology_file}\")\n",
        "#             break\n",
        "#     if not found:\n",
        "#         print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "# print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "# with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"Class Name\", \"Relation Type\", \"Related Class\", \"From File\"])\n",
        "#     for class_name in initial_class_names_to_check:\n",
        "#         normalized_class_name = normalize_string(class_name)\n",
        "#         print_hierarchy(normalized_class_name, all_relations, writer)\n",
        "\n",
        "# print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import csv\n",
        "# import re\n",
        "# from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, Literal\n",
        "\n",
        "# def normalize_string(s):\n",
        "#     s = s.lower()\n",
        "#     s = re.sub(r'[_\\-+\\s]+', '', s)\n",
        "#     s = s.replace('...', '')\n",
        "#     return s\n",
        "\n",
        "# def get_class_label(g, cls):\n",
        "#     labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "#     return labels[0] if labels else None\n",
        "\n",
        "# def get_uri_string(uri):\n",
        "#     if isinstance(uri, Literal):\n",
        "#         return str(uri)\n",
        "#     else:\n",
        "#         return f\"<{uri}>\"\n",
        "\n",
        "# def load_and_collect_classes_and_relations(file_path, class_names_to_check):\n",
        "#     if file_path.endswith('.ttl'):\n",
        "#         file_format = 'ttl'\n",
        "#     elif file_path.endswith('.owl'):\n",
        "#         file_format = 'xml'\n",
        "#     elif file_path.endswith('.xrdf'):\n",
        "#         file_format = 'xml'\n",
        "#     else:\n",
        "#         raise ValueError(\"Unsupported file format. Only .ttl, .owl, and .xrdf files are supported.\")\n",
        "\n",
        "\n",
        "#     ex = Namespace(\"http://example.org/ontology/\")\n",
        "#     sio = Namespace(\"http://semanticscience.org/resource/\")\n",
        "#     skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
        "#     owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
        "#     rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "#     materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
        "#     bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
        "#     rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
        "#     xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
        "#     xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
        "#     foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
        "#     dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
        "#     emmo = Namespace(\"https://w3id.org/emmo/ontology/\")\n",
        "\n",
        "#     # Define namespaces for isPartOf and onProperty correctly\n",
        "#     isPartOf = dcterms.isPartOf\n",
        "#     onProperty = OWL.onProperty\n",
        "    \n",
        "    \n",
        "#     g = Graph()\n",
        "#     g.parse(file_path, format=file_format)\n",
        "\n",
        "#     normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
        "\n",
        "#     classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
        "\n",
        "#     data = []\n",
        "#     relations = []\n",
        "#     found_class_labels = set()\n",
        "    \n",
        "#     for cls in classes:\n",
        "#         labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
        "#         for label in labels:\n",
        "#             normalized_label = normalize_string(label)\n",
        "#             found_class_labels.add(normalized_label)\n",
        "#             if normalized_label in normalized_class_names_to_check:\n",
        "#                 data.append([file_path, get_uri_string(cls), str(label)])\n",
        "#                 for obj in g.objects(cls, RDFS.subClassOf):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, get_uri_string(cls), str(label), 'subClassOf', get_uri_string(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, OWL.equivalentClass):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, get_uri_string(cls), str(label), 'equivalentClass', get_uri_string(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, RDFS.isDefinedBy):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, get_uri_string(cls), str(label), 'isDefinedBy', get_uri_string(obj), str(obj_label)])\n",
        "#                 for obj in g.objects(cls, OWL.disjointWith):\n",
        "#                     obj_label = get_class_label(g, obj)\n",
        "#                     relations.append([file_path, get_uri_string(cls), str(label), 'disjointWith', get_uri_string(obj), str(obj_label)])\n",
        "#                 for prop in g.objects(cls, RDFS.domain):\n",
        "#                     prop_label = get_class_label(g, prop)\n",
        "#                     relations.append([file_path, get_uri_string(cls), str(label), 'domain', get_uri_string(prop), str(prop_label)])\n",
        "#                 for prop in g.objects(cls, RDFS.range):\n",
        "#                     prop_label = get_class_label(g, prop)\n",
        "#                     relations.append([file_path, get_uri_string(cls), str(label), 'range', get_uri_string(prop), str(prop_label)])\n",
        "#                 for prop in g.objects(cls, OWL.inverseOf):\n",
        "#                     prop_label = get_class_label(g, prop)\n",
        "#                     relations.append([file_path, get_uri_string(cls), str(label), 'inverseOf', get_uri_string(prop), str(prop_label)])\n",
        "#                 for restriction in g.objects(cls, OWL.Restriction):\n",
        "#                     on_property = next(g.objects(restriction, OWL.onProperty), None)\n",
        "#                     if on_property:\n",
        "#                         on_property_label = get_class_label(g, on_property)\n",
        "#                         relations.append([file_path, get_uri_string(cls), str(label), 'onProperty', get_uri_string(on_property), str(on_property_label)])\n",
        "#                     all_values_from = next(g.objects(restriction, OWL.allValuesFrom), None)\n",
        "#                     if all_values_from:\n",
        "#                         all_values_from_label = get_class_label(g, all_values_from)\n",
        "#                         relations.append([file_path, get_uri_string(cls), str(label), 'allValuesFrom', get_uri_string(all_values_from), str(all_values_from_label)])\n",
        "#                     some_values_from = next(g.objects(restriction, OWL.someValuesFrom), None)\n",
        "#                     if some_values_from:\n",
        "#                         some_values_from_label = get_class_label(g, some_values_from)\n",
        "#                         relations.append([file_path, get_uri_string(cls), str(label), 'someValuesFrom', get_uri_string(some_values_from), str(some_values_from_label)])\n",
        "#                     has_value = next(g.objects(restriction, OWL.hasValue), None)\n",
        "#                     if has_value:\n",
        "#                         relations.append([file_path, get_uri_string(cls), str(label), 'hasValue', None, str(has_value)])\n",
        "\n",
        "#     return data, relations, found_class_labels\n",
        "\n",
        "# def filter_relations(all_relations, initial_class_names_to_check):\n",
        "#     normalized_initial_class_names = {normalize_string(name) for name in initial_class_names_to_check}\n",
        "#     return [relation for relation in all_relations if normalize_string(relation[2]) in normalized_initial_class_names or normalize_string(relation[5]) in normalized_initial_class_names]\n",
        "\n",
        "# def print_hierarchy(class_name, relations, writer):\n",
        "#     def recursive_print(class_name, depth=0):\n",
        "#         for relation in relations:\n",
        "#             if normalize_string(relation[2]) == normalize_string(class_name):\n",
        "#                 writer.writerow([relation[2], relation[3], relation[5], relation[0]])\n",
        "#                 indent = '  ' * depth\n",
        "#                 print(f\"{indent}{relation[2]} is {relation[3]} {relation[5]} (from {relation[0]})\")\n",
        "#                 recursive_print(relation[5], depth + 1)\n",
        "\n",
        "#     recursive_print(class_name)\n",
        "\n",
        "# output_hierarchy_file = \"class_hierarchy.csv\"\n",
        "# class_output_file = \"ontology_classes.csv\"\n",
        "# relations_output_file = \"ontology_relations.csv\"\n",
        "\n",
        "# all_data = []\n",
        "# all_relations = []\n",
        "# all_found_class_labels = set()\n",
        "\n",
        "# max_iterations = 2\n",
        "# iteration_count = 0\n",
        "\n",
        "# while initial_class_names_to_check and iteration_count < max_iterations:\n",
        "#     iteration_count += 1\n",
        "#     new_data = []\n",
        "#     new_relations = []\n",
        "#     new_found_class_labels = set()\n",
        "\n",
        "#     for ontology_file in ontology_files:\n",
        "#         file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "#         new_data.extend(file_data)\n",
        "#         new_relations.extend(file_relations)\n",
        "#         new_found_class_labels.update(found_class_labels)\n",
        "\n",
        "#     all_data.extend(new_data)\n",
        "#     all_relations.extend(new_relations)\n",
        "#     all_found_class_labels.update(new_found_class_labels)\n",
        "\n",
        "#     initial_class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
        "#     initial_class_names_to_check = [normalize_string(name) for name in initial_class_names_to_check]\n",
        "\n",
        "# filtered_data = [row for row in all_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
        "# with open(class_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"File\", \"Class URI\", \"Label\"])\n",
        "#     writer.writerows(filtered_data)\n",
        "\n",
        "# filtered_relations = filter_relations(all_relations, initial_class_names_to_check)\n",
        "# with open(relations_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"File\", \"Subject Class URI\", \"Subject Label\", \"Relation\", \"Object Class URI\", \"Object Label\"])\n",
        "#     writer.writerows(filtered_relations)\n",
        "\n",
        "# print(f\"Filtered class data has been saved to {class_output_file}\")\n",
        "# print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
        "\n",
        "# print(\"\\nInitial class names found in the output:\")\n",
        "# for class_name in initial_class_names_to_check:\n",
        "#     normalized_class_name = normalize_string(class_name)\n",
        "#     found = False\n",
        "#     for label in all_found_class_labels:\n",
        "#         if normalized_class_name in label:\n",
        "#             found = True\n",
        "#             print(f\"Class '{class_name}' found in:\")\n",
        "#             for ontology_file in ontology_files:\n",
        "#                 file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check)\n",
        "#                 normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
        "#                 if normalized_class_name in normalized_labels:\n",
        "#                     print(f\"- {ontology_file}\")\n",
        "#             break\n",
        "#     if not found:\n",
        "#         print(f\"Class '{class_name}' not found in the output.\")\n",
        "\n",
        "# print(\"\\nSaving class hierarchy to CSV file:\")\n",
        "# with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow([\"Class Name\", \"Relation Type\", \"Related Class\", \"From File\"])\n",
        "#     for class_name in initial_class_names_to_check:\n",
        "#         normalized_class_name = normalize_string(class_name)\n",
        "#         print_hierarchy(normalized_class_name, all_relations, writer)\n",
        "\n",
        "# print(f\"Class hierarchy has been saved to {output_hierarchy_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "01 - DataCollection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 64-bit ('3.8.13')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
