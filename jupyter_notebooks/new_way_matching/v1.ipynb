{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: http://www.w3.org/2002/07/owl#Class\n",
      "SubClass: http://semanticscience.org/resource/ChemicalSubstance\n",
      "SubClass is a blank node with the following properties:\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#type, Object: http://www.w3.org/2002/07/owl#Restriction\n",
      "Blank Node Predicate: http://www.w3.org/2002/07/owl#onProperty, Object: http://semanticscience.org/resource/hasCapability\n",
      "Blank Node Predicate: http://www.w3.org/2002/07/owl#someValuesFrom, Object is another Blank Node\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#type, Object: http://www.w3.org/2002/07/owl#Class\n",
      "Blank Node Predicate: http://www.w3.org/2002/07/owl#intersectionOf, Object is another Blank Node\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#first, Object: http://semanticscience.org/resource/ToRegulate\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#rest, Object is another Blank Node\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#first, Object is another Blank Node\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#type, Object: http://www.w3.org/2002/07/owl#Restriction\n",
      "Blank Node Predicate: http://www.w3.org/2002/07/owl#onProperty, Object: http://semanticscience.org/resource/inRelationTo\n",
      "Blank Node Predicate: http://www.w3.org/2002/07/owl#someValuesFrom, Object: http://semanticscience.org/resource/BiologicalEntity\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#rest, Object: http://www.w3.org/1999/02/22-rdf-syntax-ns#nil\n",
      "Predicate: http://data.bioontology.org/metadata/prefixIRI, Object: sio:Drug\n",
      "Predicate: http://purl.org/dc/terms/description, Object: A drug is a chemical substance that contains one or more active ingredients that regulate one or more biological processes.\n",
      "Predicate: http://www.w3.org/2000/01/rdf-schema#isDefinedBy, Object: http://semanticscience.org/ontology/sio/v1.53/sio-subset-labels.owl\n",
      "Predicate: http://www.w3.org/2000/01/rdf-schema#label, Object: drug\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, OWL, Namespace, Literal\n",
    "\n",
    "# Define the Turtle data with necessary prefixes\n",
    "turtle_data = \"\"\"\n",
    "@prefix sio: <http://semanticscience.org/resource/> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix bio: <http://data.bioontology.org/metadata/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "\n",
    "<http://semanticscience.org/resource/Drug> a owl:Class ;\n",
    "    rdfs:subClassOf <http://semanticscience.org/resource/ChemicalSubstance>, [\n",
    "        rdf:type owl:Restriction ;\n",
    "        owl:onProperty <http://semanticscience.org/resource/hasCapability> ;\n",
    "        owl:someValuesFrom [\n",
    "            rdf:type owl:Class ;\n",
    "            owl:intersectionOf (\n",
    "                <http://semanticscience.org/resource/ToRegulate>\n",
    "                [\n",
    "                    rdf:type owl:Restriction ;\n",
    "                    owl:onProperty <http://semanticscience.org/resource/inRelationTo> ;\n",
    "                    owl:someValuesFrom <http://semanticscience.org/resource/BiologicalEntity> ;\n",
    "                ]\n",
    "            ) ;\n",
    "        ] ;\n",
    "    ] ;\n",
    "    bio:prefixIRI \"sio:Drug\" ;\n",
    "    dcterms:description \"A drug is a chemical substance that contains one or more active ingredients that regulate one or more biological processes.\"@en ;\n",
    "    rdfs:isDefinedBy <http://semanticscience.org/ontology/sio/v1.53/sio-subset-labels.owl> ;\n",
    "    rdfs:label \"drug\"@en .\n",
    "\"\"\"\n",
    "\n",
    "# Create a Graph and parse the data\n",
    "g = Graph()\n",
    "g.parse(data=turtle_data, format=\"turtle\")\n",
    "\n",
    "# Define the necessary namespaces\n",
    "SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "\n",
    "# Bind prefixes to the graph (optional but useful for serialization and querying)\n",
    "g.bind(\"sio\", SIO)\n",
    "g.bind(\"dcterms\", DC)\n",
    "g.bind(\"bio\", BIO)\n",
    "g.bind(\"owl\", OWL)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"rdf\", RDF)\n",
    "\n",
    "# Define the resource URI\n",
    "drug_uri = URIRef(\"http://semanticscience.org/resource/Drug\")\n",
    "\n",
    "# Function to recursively print blank nodes\n",
    "def print_blank_node(bnode):\n",
    "    for p, o in g.predicate_objects(bnode):\n",
    "        if isinstance(o, BNode):\n",
    "            print(f\"Blank Node Predicate: {p}, Object is another Blank Node\")\n",
    "            print_blank_node(o)\n",
    "        else:\n",
    "            print(f\"Blank Node Predicate: {p}, Object: {o}\")\n",
    "\n",
    "# Extract Class\n",
    "for _, _, o in g.triples((drug_uri, RDF.type, None)):\n",
    "    print(f\"Class: {o}\")\n",
    "\n",
    "# Extract SubClass\n",
    "for _, _, o in g.triples((drug_uri, RDFS.subClassOf, None)):\n",
    "    if isinstance(o, BNode):\n",
    "        print(\"SubClass is a blank node with the following properties:\")\n",
    "        print_blank_node(o)\n",
    "    else:\n",
    "        print(f\"SubClass: {o}\")\n",
    "\n",
    "# Extract other information\n",
    "for _, p, o in g.triples((drug_uri, None, None)):\n",
    "    if p not in {RDF.type, RDFS.subClassOf}:\n",
    "        print(f\"Predicate: {p}, Object: {o}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to drug_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, OWL, Namespace\n",
    "\n",
    "# Define the Turtle data with necessary prefixes\n",
    "turtle_data = \"\"\"\n",
    "@prefix sio: <http://semanticscience.org/resource/> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix bio: <http://data.bioontology.org/metadata/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "\n",
    "<http://semanticscience.org/resource/Drug> a owl:Class ;\n",
    "    rdfs:subClassOf <http://semanticscience.org/resource/ChemicalSubstance>, [\n",
    "        rdf:type owl:Restriction ;\n",
    "        owl:onProperty <http://semanticscience.org/resource/hasCapability> ;\n",
    "        owl:someValuesFrom [\n",
    "            rdf:type owl:Class ;\n",
    "            owl:intersectionOf (\n",
    "                <http://semanticscience.org/resource/ToRegulate>\n",
    "                [\n",
    "                    rdf:type owl:Restriction ;\n",
    "                    owl:onProperty <http://semanticscience.org/resource/inRelationTo> ;\n",
    "                    owl:someValuesFrom <http://semanticscience.org/resource/BiologicalEntity> ;\n",
    "                ]\n",
    "            ) ;\n",
    "        ] ;\n",
    "    ] ;\n",
    "    bio:prefixIRI \"sio:Drug\" ;\n",
    "    dcterms:description \"A drug is a chemical substance that contains one or more active ingredients that regulate one or more biological processes.\"@en ;\n",
    "    rdfs:isDefinedBy <http://semanticscience.org/ontology/sio/v1.53/sio-subset-labels.owl> ;\n",
    "    rdfs:label \"drug\"@en .\n",
    "\"\"\"\n",
    "\n",
    "# Create a Graph and parse the data\n",
    "g = Graph()\n",
    "g.parse(data=turtle_data, format=\"turtle\")\n",
    "\n",
    "# Define the necessary namespaces\n",
    "SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "\n",
    "# Bind prefixes to the graph (optional but useful for serialization and querying)\n",
    "g.bind(\"sio\", SIO)\n",
    "g.bind(\"dcterms\", DC)\n",
    "g.bind(\"bio\", BIO)\n",
    "g.bind(\"owl\", OWL)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"rdf\", RDF)\n",
    "\n",
    "# Define the resource URI\n",
    "drug_uri = URIRef(\"http://semanticscience.org/resource/Drug\")\n",
    "\n",
    "# Function to recursively extract blank node information\n",
    "def extract_blank_node(bnode):\n",
    "    blank_node_data = []\n",
    "    for p, o in g.predicate_objects(bnode):\n",
    "        if isinstance(o, BNode):\n",
    "            blank_node_data.append((p, \"Blank Node\"))\n",
    "            blank_node_data.extend(extract_blank_node(o))\n",
    "        else:\n",
    "            blank_node_data.append((p, o))\n",
    "    return blank_node_data\n",
    "\n",
    "# Prepare data for CSV\n",
    "csv_data = [[\"Subject\", \"Predicate\", \"Object\"]]\n",
    "\n",
    "# Extract Class\n",
    "for _, _, o in g.triples((drug_uri, RDF.type, None)):\n",
    "    csv_data.append([drug_uri, RDF.type, o])\n",
    "\n",
    "# Extract SubClass\n",
    "for _, _, o in g.triples((drug_uri, RDFS.subClassOf, None)):\n",
    "    if isinstance(o, BNode):\n",
    "        csv_data.append([drug_uri, RDFS.subClassOf, \"Blank Node\"])\n",
    "        csv_data.extend(extract_blank_node(o))\n",
    "    else:\n",
    "        csv_data.append([drug_uri, RDFS.subClassOf, o])\n",
    "\n",
    "# Extract other information\n",
    "for _, p, o in g.triples((drug_uri, None, None)):\n",
    "    if p not in {RDF.type, RDFS.subClassOf}:\n",
    "        csv_data.append([drug_uri, p, o])\n",
    "\n",
    "# Write data to CSV\n",
    "with open(\"drug_info.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(\"Data saved to drug_info.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to drug_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, Namespace\n",
    "\n",
    "# Define the Turtle data with necessary prefixes\n",
    "turtle_data = \"\"\"\n",
    "@prefix sio: <http://semanticscience.org/resource/> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix bio: <http://data.bioontology.org/metadata/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "\n",
    "<http://semanticscience.org/resource/Drug> a owl:Class ;\n",
    "    rdfs:subClassOf <http://semanticscience.org/resource/ChemicalSubstance>, [\n",
    "        rdf:type owl:Restriction ;\n",
    "        owl:onProperty <http://semanticscience.org/resource/hasCapability> ;\n",
    "        owl:someValuesFrom [\n",
    "            rdf:type owl:Class ;\n",
    "            owl:intersectionOf (\n",
    "                <http://semanticscience.org/resource/ToRegulate>\n",
    "                [\n",
    "                    rdf:type owl:Restriction ;\n",
    "                    owl:onProperty <http://semanticscience.org/resource/inRelationTo> ;\n",
    "                    owl:someValuesFrom <http://semanticscience.org/resource/BiologicalEntity> ;\n",
    "                ]\n",
    "            ) ;\n",
    "        ] ;\n",
    "    ] ;\n",
    "    bio:prefixIRI \"sio:Drug\" ;\n",
    "    dcterms:description \"A drug is a chemical substance that contains one or more active ingredients that regulate one or more biological processes.\"@en ;\n",
    "    rdfs:isDefinedBy <http://semanticscience.org/ontology/sio/v1.53/sio-subset-labels.owl> ;\n",
    "    rdfs:label \"drug\"@en .\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create a Graph and parse the data\n",
    "g = Graph()\n",
    "g.parse(data=turtle_data, format=\"turtle\")\n",
    "\n",
    "# Define the necessary namespaces\n",
    "SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "\n",
    "# Bind prefixes to the graph (optional but useful for serialization and querying)\n",
    "g.bind(\"sio\", SIO)\n",
    "g.bind(\"dcterms\", DC)\n",
    "g.bind(\"bio\", BIO)\n",
    "g.bind(\"owl\", OWL)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"rdf\", RDF)\n",
    "\n",
    "# Define the resource URI\n",
    "drug_uri = URIRef(\"http://semanticscience.org/resource/Drug\")\n",
    "\n",
    "# Function to get label or URI parts\n",
    "def get_label_and_uri(uri):\n",
    "    qres = g.query(\n",
    "        \"\"\"\n",
    "        SELECT ?label WHERE {\n",
    "            ?uri rdfs:label ?label .\n",
    "        }\n",
    "        \"\"\",\n",
    "        initBindings={'uri': uri}\n",
    "    )\n",
    "    for row in qres:\n",
    "        return str(row.label), str(uri)\n",
    "    return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "# Function to recursively extract blank node information\n",
    "def extract_blank_node(bnode):\n",
    "    blank_node_data = []\n",
    "    for p, o in g.predicate_objects(bnode):\n",
    "        if isinstance(o, BNode):\n",
    "            pred_label, pred_uri = get_label_and_uri(p)\n",
    "            blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "            blank_node_data.extend(extract_blank_node(o))\n",
    "        else:\n",
    "            pred_label, pred_uri = get_label_and_uri(p)\n",
    "            obj_label, obj_uri = get_label_and_uri(o)\n",
    "            blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "    return blank_node_data\n",
    "\n",
    "# Prepare data for CSV\n",
    "csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "# Extract Class\n",
    "for _, _, o in g.triples((drug_uri, RDF.type, None)):\n",
    "    subj_label, subj_uri = get_label_and_uri(drug_uri)\n",
    "    obj_label, obj_uri = get_label_and_uri(o)\n",
    "    csv_data.append([subj_label, subj_uri, \"type\", str(RDF.type), obj_label, obj_uri])\n",
    "\n",
    "# Extract SubClass\n",
    "for _, _, o in g.triples((drug_uri, RDFS.subClassOf, None)):\n",
    "    subj_label, subj_uri = get_label_and_uri(drug_uri)\n",
    "    pred_label, pred_uri = get_label_and_uri(RDFS.subClassOf)\n",
    "    if isinstance(o, BNode):\n",
    "        csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "        csv_data.extend(extract_blank_node(o))\n",
    "    else:\n",
    "        obj_label, obj_uri = get_label_and_uri(o)\n",
    "        csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "# Extract other information\n",
    "for _, p, o in g.triples((drug_uri, None, None)):\n",
    "    if p not in {RDF.type, RDFS.subClassOf}:\n",
    "        subj_label, subj_uri = get_label_and_uri(drug_uri)\n",
    "        pred_label, pred_uri = get_label_and_uri(p)\n",
    "        obj_label, obj_uri = get_label_and_uri(o)\n",
    "        csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "# Write data to CSV\n",
    "with open(\"drug_info.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(\"Data saved to drug_info.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to all_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, Namespace\n",
    "\n",
    "# Define the path to the Turtle file\n",
    "turtle_file_path = \"../Ontologies/materialsmine_converted.ttl\"\n",
    "\n",
    "# Create a Graph and parse the data from the file\n",
    "g = Graph()\n",
    "g.parse(turtle_file_path, format=\"turtle\")\n",
    "\n",
    "# Define the necessary namespaces\n",
    "SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "\n",
    "# Bind prefixes to the graph (optional but useful for serialization and querying)\n",
    "g.bind(\"sio\", SIO)\n",
    "g.bind(\"dcterms\", DC)\n",
    "g.bind(\"bio\", BIO)\n",
    "g.bind(\"owl\", OWL)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"rdf\", RDF)\n",
    "\n",
    "# Function to get label or URI parts\n",
    "def get_label_and_uri(uri):\n",
    "    qres = g.query(\n",
    "        \"\"\"\n",
    "        SELECT ?label WHERE {\n",
    "            ?uri rdfs:label ?label .\n",
    "        }\n",
    "        \"\"\",\n",
    "        initBindings={'uri': uri}\n",
    "    )\n",
    "    for row in qres:\n",
    "        return str(row.label), str(uri)\n",
    "    return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "# Function to recursively extract blank node information\n",
    "def extract_blank_node(bnode):\n",
    "    blank_node_data = []\n",
    "    for p, o in g.predicate_objects(bnode):\n",
    "        if isinstance(o, BNode):\n",
    "            pred_label, pred_uri = get_label_and_uri(p)\n",
    "            blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "            blank_node_data.extend(extract_blank_node(o))\n",
    "        else:\n",
    "            pred_label, pred_uri = get_label_and_uri(p)\n",
    "            obj_label, obj_uri = get_label_and_uri(o)\n",
    "            blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "    return blank_node_data\n",
    "\n",
    "# Prepare data for CSV\n",
    "csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "# Extract all triples\n",
    "for subj, pred, obj in g:\n",
    "    subj_label, subj_uri = get_label_and_uri(subj)\n",
    "    pred_label, pred_uri = get_label_and_uri(pred)\n",
    "    \n",
    "    if isinstance(obj, BNode):\n",
    "        csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "        csv_data.extend(extract_blank_node(obj))\n",
    "    else:\n",
    "        obj_label, obj_uri = get_label_and_uri(obj)\n",
    "        csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "# Write data to CSV\n",
    "with open(\"all_info.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(\"Data saved to all_info.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Reading from the file ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, Namespace\n",
    "\n",
    "\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "class RDFGraphHandler:\n",
    "    def __init__(self, turtle_file_path):\n",
    "        self.turtle_file_path = turtle_file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        self.graph.parse(self.turtle_file_path, format=\"turtle\")\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        qres = self.graph.query(\n",
    "            \"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initBindings={'uri': uri}\n",
    "        )\n",
    "        for row in qres:\n",
    "            return str(row.label), str(uri)\n",
    "        return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def save_to_csv(self, class_names, output_csv_path):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "            \n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    csv_data.extend(self.extract_blank_node(obj))\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(csv_data)\n",
    "\n",
    "        print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "# Usage example\n",
    "turtle_file_path = \"../Ontologies/materialsmine_converted.ttl\"\n",
    "class_names = [\"drug\" , \"stress\", 'AmperePerJoule', 'Tensiletest']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "rdf_handler = RDFGraphHandler(turtle_file_path)\n",
    "rdf_handler.save_to_csv(class_names, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Improving the speed of info capturing ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, Namespace\n",
    "\n",
    "\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "class RDFGraphHandler:\n",
    "    def __init__(self, turtle_file_path):\n",
    "        self.turtle_file_path = turtle_file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        self.graph.parse(self.turtle_file_path, format=\"turtle\")\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        qres = self.graph.query(\n",
    "            \"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initBindings={'uri': uri}\n",
    "        )\n",
    "        for row in qres:\n",
    "            return str(row.label), str(uri)\n",
    "        return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    \n",
    "    def save_to_csv(self, class_names, output_csv_path):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "        label_cache = {}\n",
    "\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = label_cache.get(subj, self.get_label_and_uri(subj))\n",
    "            pred_label, pred_uri = label_cache.get(pred, self.get_label_and_uri(pred))\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "            \n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    csv_data.extend(self.extract_blank_node(obj))\n",
    "                    # Set placeholders for obj_label and obj_uri when obj is a BNode\n",
    "                    obj_label, obj_uri = \"Blank Node\", \"Blank Node\"\n",
    "                else:\n",
    "                    obj_label, obj_uri = label_cache.get(obj, self.get_label_and_uri(obj))\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "            else:\n",
    "                # Handle the case where normalized_subj_label is not in normalized_class_names\n",
    "                continue  # Or handle as needed\n",
    "\n",
    "            # Ensure obj_label and obj_uri are always assigned outside of the conditional\n",
    "            if not isinstance(obj, BNode):\n",
    "                obj_label, obj_uri = label_cache.get(obj, self.get_label_and_uri(obj))\n",
    "\n",
    "\n",
    "        with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(csv_data)\n",
    "\n",
    "        print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "turtle_file_path = \"../Ontologies/materialsmine_converted.ttl\"\n",
    "class_names = [\"drug\" , \"stress\", 'AmperePerJoule', 'Tensiletest']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "rdf_handler = RDFGraphHandler(turtle_file_path)\n",
    "rdf_handler.save_to_csv(class_names, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Implementing multiple input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info_2.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, Namespace\n",
    "\n",
    "\n",
    "class RDFGraphHandler:\n",
    "    def __init__(self, turtle_file_path):\n",
    "        self.turtle_file_path = turtle_file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        self.graph.parse(self.turtle_file_path, format=\"turtle\")\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        qres = self.graph.query(\n",
    "            \"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initBindings={'uri': uri}\n",
    "        )\n",
    "        for row in qres:\n",
    "            return str(row.label), str(uri)\n",
    "        return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    \n",
    "    def save_to_csv(self, class_names, output_csv_path):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "        label_cache = {}\n",
    "\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = label_cache.get(subj, self.get_label_and_uri(subj))\n",
    "            pred_label, pred_uri = label_cache.get(pred, self.get_label_and_uri(pred))\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "            \n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    csv_data.extend(self.extract_blank_node(obj))\n",
    "                    # Set placeholders for obj_label and obj_uri when obj is a BNode\n",
    "                    obj_label, obj_uri = \"Blank Node\", \"Blank Node\"\n",
    "                else:\n",
    "                    obj_label, obj_uri = label_cache.get(obj, self.get_label_and_uri(obj))\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "            # Ensure obj_label and obj_uri are always assigned outside of the conditional\n",
    "            if not isinstance(obj, BNode):\n",
    "                obj_label, obj_uri = label_cache.get(obj, self.get_label_and_uri(obj))\n",
    "\n",
    "        with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(csv_data)\n",
    "\n",
    "        print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "input_turtle_files = [\"../Ontologies/materialsmine_converted.ttl\"]\n",
    "class_names = [\"drug\", \"stress\", 'AmperePerJoule', 'Tensiletest']\n",
    "output_csv_path = \"filtered_info_2.csv\"\n",
    "\n",
    "for turtle_file_path in input_turtle_files:\n",
    "    rdf_handler = RDFGraphHandler(turtle_file_path)\n",
    "    rdf_handler.save_to_csv(class_names, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using multiprocessing ti speed up the process ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info_5.csv\n",
      "Data saved to filtered_info_5.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    directory = '.'\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            os.remove(os.path.join(directory, filename))\n",
    "\n",
    "\n",
    "    def __init__(self, turtle_file_path):\n",
    "        self.turtle_file_path = turtle_file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        self.graph.parse(self.turtle_file_path, format=\"turtle\")\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes):\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            results = pool.map(self.extract_blank_node, bnodes)\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def save_to_csv(self, class_names, output_csv_path):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes)\n",
    "        csv_data.extend(blank_node_data)\n",
    "\n",
    "        with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(csv_data)\n",
    "\n",
    "        print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "# Usage example\n",
    "input_turtle_files = [\"../Ontologies/materialsmine_converted.ttl\", \"../Ontologies/emmo.ttl\"]\n",
    "class_names = [\"drug\", \"stress\", 'AmperePerJoule', 'Tensiletest']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "for turtle_file_path in input_turtle_files:\n",
    "    rdf_handler = RDFGraphHandler(turtle_file_path)\n",
    "    rdf_handler.save_to_csv(class_names, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The above code work much faster! however when two or more inputs are given then for each inputs the outputs in CSV are removed! ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here I try to solve the above issue ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    def __init__(self, turtle_file_path):\n",
    "        self.turtle_file_path = turtle_file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        self.graph.parse(self.turtle_file_path, format=\"turtle\")\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes):\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            results = pool.map(self.extract_blank_node, bnodes)\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def extract_data(self, class_names):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = []\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes)\n",
    "        csv_data.extend(blank_node_data)\n",
    "        \n",
    "        return csv_data\n",
    "\n",
    "# Usage example\n",
    "input_turtle_files = [\"../Ontologies/emmo.ttl\" , \"../Ontologies/materialsmine_converted.ttl\"]\n",
    "class_names = [\"drug\", \"stress\", 'AmperePerJoule', 'Tensiletest']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "# Remove existing CSV file if it exists\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "\n",
    "# Initialize CSV data with headers\n",
    "all_csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "for turtle_file_path in input_turtle_files:\n",
    "    rdf_handler = RDFGraphHandler(turtle_file_path)\n",
    "    file_csv_data = rdf_handler.extract_data(class_names)\n",
    "    all_csv_data.extend(file_csv_data)\n",
    "\n",
    "# Write accumulated CSV data to file\n",
    "with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(all_csv_data)\n",
    "\n",
    "print(f\"Data saved to {output_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Above code down not accept OWL file; Let's fix it ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        file_format = 'turtle' if self.file_path.endswith('.ttl') else 'xml'\n",
    "        self.graph.parse(self.file_path, format=file_format)\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes):\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            results = pool.map(self.extract_blank_node, bnodes)\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def extract_data(self, class_names):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = []\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes)\n",
    "        csv_data.extend(blank_node_data)\n",
    "        \n",
    "        return csv_data\n",
    "\n",
    "# Usage example\n",
    "input_files = [\n",
    "    \"../Ontologies/MatWerk.xrdf\", \n",
    "    \"../Ontologies/materialsmine_converted.ttl\" , \n",
    "    \"../Ontologies/emmo.ttl\",\n",
    "    # \"../Ontologies/schemaorg.owl\"\n",
    "    ]\n",
    "class_names = [\"drug\", \"stress\", 'AmperePerJoule', 'Tensiletest', 'Electronic lab Notebook']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "# Remove existing CSV file if it exists\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "# Initialize CSV data with headers\n",
    "all_csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "for file_path in input_files:\n",
    "    rdf_handler = RDFGraphHandler(file_path)\n",
    "    file_csv_data = rdf_handler.extract_data(class_names)\n",
    "    all_csv_data.extend(file_csv_data)\n",
    "\n",
    "# Write accumulated CSV data to file\n",
    "with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(all_csv_data)\n",
    "\n",
    "print(f\"Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Let's save the input name to csv file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        file_format = 'turtle' if self.file_path.endswith('.ttl') else 'xml'\n",
    "        self.graph.parse(self.file_path, format=file_format)\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def extract_blank_node(self, bnode, file_name):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o, file_name))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes, file_name):\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            results = pool.starmap(self.extract_blank_node, [(bnode, file_name) for bnode in bnodes])\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def extract_data(self, class_names):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = []\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes, os.path.basename(self.file_path))\n",
    "        csv_data.extend(blank_node_data)\n",
    "        \n",
    "        return csv_data\n",
    "\n",
    "# Usage example\n",
    "input_files = [\n",
    "    \"../Ontologies/MatWerk.xrdf\", \n",
    "    \"../Ontologies/materialsmine_converted.ttl\" , \n",
    "    \"../Ontologies/emmo.ttl\",\n",
    "    # \"../Ontologies/schemaorg.owl\"\n",
    "    ]\n",
    "class_names = [\"drug\", \"stress\", 'AmperePerJoule', 'Tensiletest', 'Electronic lab Notebook']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "# Remove existing CSV file if it exists\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "# Initialize CSV data with headers\n",
    "all_csv_data = [[\"File Name\", \"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "for file_path in input_files:\n",
    "    rdf_handler = RDFGraphHandler(file_path)\n",
    "    file_csv_data = rdf_handler.extract_data(class_names)\n",
    "    all_csv_data.extend(file_csv_data)\n",
    "\n",
    "# Write accumulated CSV data to file\n",
    "with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(all_csv_data)\n",
    "\n",
    "print(f\"Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using RDF lib parsing to improvment ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using multiprocessing ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import multiprocessing\n",
    "from multiprocessing import Manager, Pool\n",
    "from rdflib import Graph, BNode, Namespace\n",
    "\n",
    "class RDFGraphHandler:\n",
    "    def __init__(self, turtle_file_path):\n",
    "        self.turtle_file_path = turtle_file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        self.graph.parse(self.turtle_file_path, format=\"turtle\")\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        qres = self.graph.query(\n",
    "            \"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initBindings={'uri': uri}\n",
    "        )\n",
    "        for row in qres:\n",
    "            return str(row.label), str(uri)\n",
    "        return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "    def extract_blank_node(self, bnode, label_cache):\n",
    "        def process_node(bnode):\n",
    "            blank_node_data = []\n",
    "            for p, o in self.graph.predicate_objects(bnode):\n",
    "                if isinstance(o, BNode):\n",
    "                    pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                    blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                    blank_node_data.extend(self.extract_blank_node(o, label_cache))\n",
    "                else:\n",
    "                    pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                    blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "            return blank_node_data\n",
    "        \n",
    "        return process_node(bnode)\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def process_chunk(self, chunk, normalized_class_names, label_cache):\n",
    "        chunk_data = []\n",
    "        for subj, pred, obj in chunk:\n",
    "            subj_label, subj_uri = label_cache.get(subj, self.get_label_and_uri(subj))\n",
    "            pred_label, pred_uri = label_cache.get(pred, self.get_label_and_uri(pred))\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    chunk_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    chunk_data.extend(self.extract_blank_node(obj, label_cache))\n",
    "                else:\n",
    "                    obj_label, obj_uri = label_cache.get(obj, self.get_label_and_uri(obj))\n",
    "                    chunk_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        return chunk_data\n",
    "\n",
    "    def save_to_csv(self, class_names, output_csv_path):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "        manager = Manager()\n",
    "        label_cache = manager.dict()\n",
    "\n",
    "        # Determine chunk size (you can adjust this as needed)\n",
    "        chunk_size = 1000\n",
    "        chunks = [list(self.graph[i:i + chunk_size]) for i in range(0, len(self.graph), chunk_size)]\n",
    "\n",
    "        # Use multiprocessing Pool to parallelize processing of chunks\n",
    "        with Pool() as pool:\n",
    "            chunk_results = pool.starmap(self.process_chunk, [(chunk, normalized_class_names, label_cache) for chunk in chunks])\n",
    "\n",
    "        for result in chunk_results:\n",
    "            csv_data.extend(result)\n",
    "\n",
    "        # Write data to CSV file\n",
    "        try:\n",
    "            with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerows(csv_data)\n",
    "            print(f\"Data saved to {output_csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to {output_csv_path}: {e}\")\n",
    "\n",
    "# Usage example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info_opti.csv\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    turtle_file_path = \"../Ontologies/materialsmine_converted.ttl\"\n",
    "    class_names = [\"drug\", \"stress\", \"AmperePerJoule\", \"Tensiletest\"]\n",
    "    output_csv_path = \"filtered_info_opti.csv\"\n",
    "\n",
    "    rdf_handler = RDFGraphHandler(turtle_file_path)\n",
    "    rdf_handler.save_to_csv(class_names, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The above code does not print anyting in CSV file! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Try to combine the above way to previous one on v4 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "from rdflib import Graph, RDF, RDFS, OWL, SKOS, Namespace, URIRef, Literal, BNode\n",
    "\n",
    "# Define namespaces (assuming these are already defined in your script)...\n",
    "ex = Namespace(\"http://example.org/ontology/\")\n",
    "sio = Namespace(\"http://semanticscience.org/resource/\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "materialsmine = Namespace(\"http://materialsmine.org/ns/\")\n",
    "bibo = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "xsd = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "xml = Namespace(\"http://www.w3.org/XML/1998/namespace\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "isPartOf = dcterms.isPartOf\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[_\\-+\\s]+', '', s)\n",
    "    s = s.replace('...', '')\n",
    "    return s\n",
    "\n",
    "# def get_label_and_uri(g, uri):\n",
    "#     qres = g.query(\n",
    "#         \"\"\"\n",
    "#         SELECT ?label WHERE {\n",
    "#             ?uri rdfs:label ?label .\n",
    "#         }\n",
    "#         \"\"\",\n",
    "#         initBindings={'uri': uri}\n",
    "#     )\n",
    "#     for row in qres:\n",
    "#         return str(row.label), str(uri)\n",
    "#     return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "def extract_blank_node(g, bnode):\n",
    "    blank_node_data = []\n",
    "    for p, o in g.predicate_objects(bnode):\n",
    "        if isinstance(o, BNode):\n",
    "            pred_label, pred_uri = get_label_and_uri(g, p)\n",
    "            blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "            blank_node_data.extend(extract_blank_node(g, o))\n",
    "        else:\n",
    "            pred_label, pred_uri = get_label_and_uri(g, p)\n",
    "            obj_label, obj_uri = get_label_and_uri(g, o)\n",
    "            blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "    return blank_node_data\n",
    "\n",
    "def process_ontology(ontology_files, initial_class_names_to_check, output_hierarchy_file, class_output_file, relations_output_file):\n",
    "    directory = '.'\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            os.remove(os.path.join(directory, filename))\n",
    "\n",
    "    g = Graph()\n",
    "    processed_classes = set()\n",
    "    processed_relations = set()\n",
    "\n",
    "    def load_and_collect_classes_and_relations(file_path, class_names_to_check, g):\n",
    "        if file_path.endswith('.ttl'):\n",
    "            file_format = 'ttl'\n",
    "        elif file_path.endswith('.owl'):\n",
    "            file_format = 'xml'\n",
    "        elif file_path.endswith('.xrdf'):\n",
    "            file_format = 'xml'\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Only .ttl and .owl files are supported.\")\n",
    "        \n",
    "        g.parse(file_path, format=file_format)\n",
    "\n",
    "        normalized_class_names_to_check = {normalize_string(name) for name in class_names_to_check}\n",
    "\n",
    "        classes = set(g.subjects(RDF.type, OWL.Class)).union(g.subjects(RDF.type, RDFS.Class))\n",
    "\n",
    "        data = []\n",
    "        relations = []\n",
    "        found_class_labels = set()\n",
    "\n",
    "        for cls in classes:\n",
    "            if isinstance(cls, URIRef):  # Check if the subject is a URI\n",
    "                labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "                description = get_class_descriptions(g, cls)\n",
    "\n",
    "                for label in labels:\n",
    "                    if label is not None:\n",
    "                        normalized_label = normalize_string(label)\n",
    "                        found_class_labels.add(normalized_label)\n",
    "\n",
    "                        if normalized_label in normalized_class_names_to_check:\n",
    "                            # Check if already processed in this iteration\n",
    "                            if str(cls) not in processed_classes:\n",
    "                                processed_classes.add(str(cls))\n",
    "                                data.append([file_path, str(cls), str(label), str(description) if description is not None else \"\"])\n",
    "\n",
    "                            for obj in g.objects(cls, RDFS.subClassOf):\n",
    "                                if isinstance(obj, URIRef):  # Check if the object is a URI\n",
    "                                    obj_label = get_class_label(g, obj)\n",
    "                                    obj_description = get_class_descriptions(g, obj)\n",
    "\n",
    "                                    # Check if already processed in this iteration\n",
    "                                    if (str(cls), 'subClassOf', str(obj)) not in processed_relations:\n",
    "                                        processed_relations.add((str(cls), 'subClassOf', str(obj)))\n",
    "                                        relations.append([file_path, str(cls), str(label), 'subClassOf', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "\n",
    "                            for obj in g.objects(cls, OWL.equivalentClass):\n",
    "                                if isinstance(obj, URIRef):\n",
    "                                    obj_label = get_class_label(g, obj)\n",
    "                                    obj_description = get_class_descriptions(g, obj)\n",
    "\n",
    "                                    # Check if already processed in this iteration\n",
    "                                    if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                        processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                        relations.append([file_path, str(cls), str(label), 'equivalentClass', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "                                else:\n",
    "                                    # Handle blank nodes for equivalentClass\n",
    "                                    obj_label = get_complex_expression_label(g, obj)\n",
    "                                    obj_description = \"Complex class expression\"\n",
    "\n",
    "                                    if (str(cls), 'equivalentClass', str(obj)) not in processed_relations:\n",
    "                                        processed_relations.add((str(cls), 'equivalentClass', str(obj)))\n",
    "                                        relations.append([file_path, str(cls), str(label), 'equivalentClass', \"Complex class expression\", str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "\n",
    "                            for obj in g.objects(cls, DCTERMS.isPartOf):\n",
    "                                if isinstance(obj, URIRef):\n",
    "                                    obj_label = get_class_label(g, obj)\n",
    "                                    obj_description = get_class_descriptions(g, obj)\n",
    "\n",
    "                                    # Check if already processed in this iteration\n",
    "                                    if (str(cls), 'isPartOf', str(obj)) not in processed_relations:\n",
    "                                        processed_relations.add((str(cls), 'isPartOf', str(obj)))\n",
    "                                        relations.append([file_path, str(cls), str(label), 'isPartOf', str(obj), str(obj_label) if obj_label is not None else \"\", str(obj_description) if obj_description is not None else \"\"])\n",
    "\n",
    "        return data, relations, found_class_labels\n",
    "\n",
    "    def get_class_label(g, cls):\n",
    "        labels = list(g.objects(cls, SKOS.altLabel)) + list(g.objects(cls, SKOS.prefLabel)) + list(g.objects(cls, RDFS.label))\n",
    "        return labels[0] if labels else None\n",
    "\n",
    "    def get_class_descriptions(g, cls):\n",
    "        descriptions = list(g.objects(cls, DCTERMS.description)) + list(g.objects(cls, SKOS.definition)) + list(g.objects(cls, RDFS.comment))\n",
    "        return \" \".join([str(desc) for desc in descriptions if desc is not None]) if descriptions else None\n",
    "\n",
    "    def get_complex_expression_label(g, node):\n",
    "        if (node, RDF.type, OWL.Restriction) in g:\n",
    "            prop = list(g.objects(node, OWL.onProperty))\n",
    "            val = list(g.objects(node, OWL.someValuesFrom))\n",
    "            if prop and val:\n",
    "                prop_label = get_class_label(g, prop[0])\n",
    "                val_label = get_class_label(g, val[0])\n",
    "                return f\"Restriction on {prop_label} some {val_label}\"\n",
    "        elif (node, RDF.type, OWL.Class) in g:\n",
    "            intersection = list(g.objects(node, OWL.intersectionOf))\n",
    "            if intersection:\n",
    "                components = []\n",
    "                for item in g.items(intersection[0]):\n",
    "                    if isinstance(item, URIRef):\n",
    "                        component_label = get_class_label(g, item)\n",
    "                        if component_label:\n",
    "                            components.append(component_label)\n",
    "                    elif isinstance(item, BNode):\n",
    "                        restriction_label = get_complex_expression_label(g, item)\n",
    "                        if restriction_label:\n",
    "                            components.append(restriction_label)\n",
    "                if components:\n",
    "                    return f\"Intersection of {' and '.join(components)}\"\n",
    "        return None\n",
    "\n",
    "    g = Graph()\n",
    "    all_data = []\n",
    "    all_relations = []\n",
    "    all_found_class_labels = set()\n",
    "    max_iterations = 2\n",
    "    iteration_count = 0\n",
    "\n",
    "    class_names_to_check = initial_class_names_to_check\n",
    "\n",
    "    while class_names_to_check and iteration_count < max_iterations:\n",
    "        iteration_count += 1\n",
    "        new_data = []\n",
    "        new_relations = []\n",
    "        new_found_class_labels = set()\n",
    "\n",
    "        for ontology_file in ontology_files:\n",
    "            file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, class_names_to_check, g)\n",
    "            new_data.extend(file_data)\n",
    "            new_relations.extend(file_relations)\n",
    "            new_found_class_labels.update(found_class_labels)\n",
    "\n",
    "        all_data.extend(new_data)\n",
    "        all_relations.extend(new_relations)\n",
    "        all_found_class_labels.update(new_found_class_labels)\n",
    "\n",
    "        class_names_to_check = {str(label) for label in new_found_class_labels} - {normalize_string(name) for name in initial_class_names_to_check}\n",
    "        class_names_to_check = [normalize_string(name) for name in class_names_to_check]\n",
    "\n",
    "        # Filter and save class data\n",
    "        filtered_data = [row for row in new_data if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "        with open(class_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if filtered_data:\n",
    "                current_class_name = filtered_data[0][2]  # Get the class name from the first row\n",
    "                writer.writerows(filtered_data)\n",
    "\n",
    "        # Filter and save class relations\n",
    "        filtered_relations = [row for row in new_relations if normalize_string(row[2]) in {normalize_string(name) for name in initial_class_names_to_check}]\n",
    "        with open(relations_output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if filtered_relations:\n",
    "                current_class_name = filtered_relations[0][2]  # Get the class name from the first row\n",
    "                writer.writerows(filtered_relations)\n",
    "\n",
    "    # Save all found classes and relations\n",
    "    with open(output_hierarchy_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"File\", \"Subject Class URI\", \"Subject Class Name\", \"Predicate\", \"Object Class URI\", \"Object Class Name\"])\n",
    "        for relation in all_relations:\n",
    "            writer.writerow(relation)\n",
    "\n",
    "    print(f\"Filtered class data has been saved to {class_output_file}\")\n",
    "    print(f\"Filtered class relations have been saved to {relations_output_file}\")\n",
    "\n",
    "    print(\"\\nInitial class names found in the output:\")\n",
    "    for class_name in initial_class_names_to_check:\n",
    "        normalized_class_name = normalize_string(class_name)\n",
    "        found = False\n",
    "        for label in all_found_class_labels:\n",
    "            if normalized_class_name in label:\n",
    "                found = True\n",
    "                print(f\"Class '{class_name}' found in:\")\n",
    "                for ontology_file in ontology_files:\n",
    "                    file_data, file_relations, found_class_labels = load_and_collect_classes_and_relations(ontology_file, initial_class_names_to_check, g)\n",
    "                    normalized_labels = [normalize_string(l) for l in found_class_labels]\n",
    "                    if normalized_class_name in normalized_labels:\n",
    "                        print(f\"- {ontology_file}\")\n",
    "                break\n",
    "        if not found:\n",
    "            print(f\"Class '{class_name}' not found in the output.\")\n",
    "\n",
    "    print(f\"\\nClass hierarchy has been saved to {output_hierarchy_file}\")\n",
    "\n",
    "    def save_intersection_info_to_csv(data, output_file):\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"File\", \"Class URI\", \"Class Name\", \"Intersection Description\"])\n",
    "            writer.writerows(data)\n",
    "\n",
    "    # Inside your main loop where you process ontology files:\n",
    "    intersection_data = [row for row in all_data if row[3].startswith(\"Intersection of\")]\n",
    "    save_intersection_info_to_csv(intersection_data, \"intersection_info.csv\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "ontology_files = [\n",
    "    # \"../Ontologies/materialsmine.ttl\", ### is not complete!\n",
    "    \"../Ontologies/materialsmine_converted.ttl\",\n",
    "    \"../Ontologies/pmdco_core.ttl\",\n",
    "    # \"../Ontologies/nfdicore_2.ttl\",\n",
    "    # \"../Ontologies/bfo.owl\", #### using this ---->  long time to proccess!\n",
    "    \"../Ontologies/emmo.ttl\",\n",
    "    # \"../Ontologies/owlapi.xrdf\",\n",
    "    # \"../Ontologies/schemaorg.owl\",\n",
    "    # \"../Ontologies/MaterialsMine.xrdf\",\n",
    "    # '../Ontologies/emmo.owl', ### has problem of reading file\n",
    "    # \"../Ontologies/Physical_Activity_Ontology_V2.owl\",\n",
    "    # \"../Ontologies/Physical_Activity_Ontology_V2.xrdf\",\n",
    "    # \"../Ontologies/oboe.owl\",\n",
    "    # \"../Ontologies/fabio.ttl\",\n",
    "]\n",
    "\n",
    "\n",
    "initial_class_names_to_check = [\"drug\" , \"stress\", 'AmperePerJoule', 'Tensiletest']\n",
    "\n",
    "\n",
    "# ontology_files = [\"../Ontologies/materialsmine_converted.ttl\"]\n",
    "# initial_class_names_to_check = [\"Drug\"]\n",
    "\n",
    "\n",
    "\n",
    "output_hierarchy_file = \"class_hierarchy.csv\"\n",
    "class_output_file = \"ontology_classes.csv\"\n",
    "relations_output_file = \"ontology_relations.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered class data has been saved to ontology_classes.csv\n",
      "Filtered class relations have been saved to ontology_relations.csv\n",
      "\n",
      "Initial class names found in the output:\n",
      "Class 'drug' found in:\n",
      "- ../Ontologies/materialsmine_converted.ttl\n",
      "- ../Ontologies/pmdco_core.ttl\n",
      "- ../Ontologies/emmo.ttl\n",
      "Class 'stress' found in:\n",
      "- ../Ontologies/materialsmine_converted.ttl\n",
      "- ../Ontologies/pmdco_core.ttl\n",
      "- ../Ontologies/emmo.ttl\n",
      "Class 'AmperePerJoule' found in:\n",
      "- ../Ontologies/materialsmine_converted.ttl\n",
      "- ../Ontologies/pmdco_core.ttl\n",
      "- ../Ontologies/emmo.ttl\n",
      "Class 'Tensiletest' found in:\n",
      "- ../Ontologies/materialsmine_converted.ttl\n",
      "- ../Ontologies/pmdco_core.ttl\n",
      "- ../Ontologies/emmo.ttl\n",
      "\n",
      "Class hierarchy has been saved to class_hierarchy.csv\n"
     ]
    }
   ],
   "source": [
    "process_ontology(ontology_files, initial_class_names_to_check, output_hierarchy_file, class_output_file, relations_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Above code does not capture class hierarchy correctly #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
