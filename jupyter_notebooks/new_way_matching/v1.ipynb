{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: http://www.w3.org/2002/07/owl#Class\n",
      "SubClass: http://semanticscience.org/resource/ChemicalSubstance\n",
      "SubClass is a blank node with the following properties:\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#type, Object: http://www.w3.org/2002/07/owl#Restriction\n",
      "Blank Node Predicate: http://www.w3.org/2002/07/owl#onProperty, Object: http://semanticscience.org/resource/hasCapability\n",
      "Blank Node Predicate: http://www.w3.org/2002/07/owl#someValuesFrom, Object is another Blank Node\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#type, Object: http://www.w3.org/2002/07/owl#Class\n",
      "Blank Node Predicate: http://www.w3.org/2002/07/owl#intersectionOf, Object is another Blank Node\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#first, Object: http://semanticscience.org/resource/ToRegulate\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#rest, Object is another Blank Node\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#first, Object is another Blank Node\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#type, Object: http://www.w3.org/2002/07/owl#Restriction\n",
      "Blank Node Predicate: http://www.w3.org/2002/07/owl#onProperty, Object: http://semanticscience.org/resource/inRelationTo\n",
      "Blank Node Predicate: http://www.w3.org/2002/07/owl#someValuesFrom, Object: http://semanticscience.org/resource/BiologicalEntity\n",
      "Blank Node Predicate: http://www.w3.org/1999/02/22-rdf-syntax-ns#rest, Object: http://www.w3.org/1999/02/22-rdf-syntax-ns#nil\n",
      "Predicate: http://data.bioontology.org/metadata/prefixIRI, Object: sio:Drug\n",
      "Predicate: http://purl.org/dc/terms/description, Object: A drug is a chemical substance that contains one or more active ingredients that regulate one or more biological processes.\n",
      "Predicate: http://www.w3.org/2000/01/rdf-schema#isDefinedBy, Object: http://semanticscience.org/ontology/sio/v1.53/sio-subset-labels.owl\n",
      "Predicate: http://www.w3.org/2000/01/rdf-schema#label, Object: drug\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, OWL, Namespace, Literal\n",
    "\n",
    "# Define the Turtle data with necessary prefixes\n",
    "turtle_data = \"\"\"\n",
    "@prefix sio: <http://semanticscience.org/resource/> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix bio: <http://data.bioontology.org/metadata/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "\n",
    "<http://semanticscience.org/resource/Drug> a owl:Class ;\n",
    "    rdfs:subClassOf <http://semanticscience.org/resource/ChemicalSubstance>, [\n",
    "        rdf:type owl:Restriction ;\n",
    "        owl:onProperty <http://semanticscience.org/resource/hasCapability> ;\n",
    "        owl:someValuesFrom [\n",
    "            rdf:type owl:Class ;\n",
    "            owl:intersectionOf (\n",
    "                <http://semanticscience.org/resource/ToRegulate>\n",
    "                [\n",
    "                    rdf:type owl:Restriction ;\n",
    "                    owl:onProperty <http://semanticscience.org/resource/inRelationTo> ;\n",
    "                    owl:someValuesFrom <http://semanticscience.org/resource/BiologicalEntity> ;\n",
    "                ]\n",
    "            ) ;\n",
    "        ] ;\n",
    "    ] ;\n",
    "    bio:prefixIRI \"sio:Drug\" ;\n",
    "    dcterms:description \"A drug is a chemical substance that contains one or more active ingredients that regulate one or more biological processes.\"@en ;\n",
    "    rdfs:isDefinedBy <http://semanticscience.org/ontology/sio/v1.53/sio-subset-labels.owl> ;\n",
    "    rdfs:label \"drug\"@en .\n",
    "\"\"\"\n",
    "\n",
    "# Create a Graph and parse the data\n",
    "g = Graph()\n",
    "g.parse(data=turtle_data, format=\"turtle\")\n",
    "\n",
    "# Define the necessary namespaces\n",
    "SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "\n",
    "# Bind prefixes to the graph (optional but useful for serialization and querying)\n",
    "g.bind(\"sio\", SIO)\n",
    "g.bind(\"dcterms\", DC)\n",
    "g.bind(\"bio\", BIO)\n",
    "g.bind(\"owl\", OWL)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"rdf\", RDF)\n",
    "\n",
    "# Define the resource URI\n",
    "drug_uri = URIRef(\"http://semanticscience.org/resource/Drug\")\n",
    "\n",
    "# Function to recursively print blank nodes\n",
    "def print_blank_node(bnode):\n",
    "    for p, o in g.predicate_objects(bnode):\n",
    "        if isinstance(o, BNode):\n",
    "            print(f\"Blank Node Predicate: {p}, Object is another Blank Node\")\n",
    "            print_blank_node(o)\n",
    "        else:\n",
    "            print(f\"Blank Node Predicate: {p}, Object: {o}\")\n",
    "\n",
    "# Extract Class\n",
    "for _, _, o in g.triples((drug_uri, RDF.type, None)):\n",
    "    print(f\"Class: {o}\")\n",
    "\n",
    "# Extract SubClass\n",
    "for _, _, o in g.triples((drug_uri, RDFS.subClassOf, None)):\n",
    "    if isinstance(o, BNode):\n",
    "        print(\"SubClass is a blank node with the following properties:\")\n",
    "        print_blank_node(o)\n",
    "    else:\n",
    "        print(f\"SubClass: {o}\")\n",
    "\n",
    "# Extract other information\n",
    "for _, p, o in g.triples((drug_uri, None, None)):\n",
    "    if p not in {RDF.type, RDFS.subClassOf}:\n",
    "        print(f\"Predicate: {p}, Object: {o}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to drug_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, OWL, Namespace\n",
    "\n",
    "# Define the Turtle data with necessary prefixes\n",
    "turtle_data = \"\"\"\n",
    "@prefix sio: <http://semanticscience.org/resource/> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix bio: <http://data.bioontology.org/metadata/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "\n",
    "<http://semanticscience.org/resource/Drug> a owl:Class ;\n",
    "    rdfs:subClassOf <http://semanticscience.org/resource/ChemicalSubstance>, [\n",
    "        rdf:type owl:Restriction ;\n",
    "        owl:onProperty <http://semanticscience.org/resource/hasCapability> ;\n",
    "        owl:someValuesFrom [\n",
    "            rdf:type owl:Class ;\n",
    "            owl:intersectionOf (\n",
    "                <http://semanticscience.org/resource/ToRegulate>\n",
    "                [\n",
    "                    rdf:type owl:Restriction ;\n",
    "                    owl:onProperty <http://semanticscience.org/resource/inRelationTo> ;\n",
    "                    owl:someValuesFrom <http://semanticscience.org/resource/BiologicalEntity> ;\n",
    "                ]\n",
    "            ) ;\n",
    "        ] ;\n",
    "    ] ;\n",
    "    bio:prefixIRI \"sio:Drug\" ;\n",
    "    dcterms:description \"A drug is a chemical substance that contains one or more active ingredients that regulate one or more biological processes.\"@en ;\n",
    "    rdfs:isDefinedBy <http://semanticscience.org/ontology/sio/v1.53/sio-subset-labels.owl> ;\n",
    "    rdfs:label \"drug\"@en .\n",
    "\"\"\"\n",
    "\n",
    "# Create a Graph and parse the data\n",
    "g = Graph()\n",
    "g.parse(data=turtle_data, format=\"turtle\")\n",
    "\n",
    "# Define the necessary namespaces\n",
    "SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "\n",
    "# Bind prefixes to the graph (optional but useful for serialization and querying)\n",
    "g.bind(\"sio\", SIO)\n",
    "g.bind(\"dcterms\", DC)\n",
    "g.bind(\"bio\", BIO)\n",
    "g.bind(\"owl\", OWL)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"rdf\", RDF)\n",
    "\n",
    "# Define the resource URI\n",
    "drug_uri = URIRef(\"http://semanticscience.org/resource/Drug\")\n",
    "\n",
    "# Function to recursively extract blank node information\n",
    "def extract_blank_node(bnode):\n",
    "    blank_node_data = []\n",
    "    for p, o in g.predicate_objects(bnode):\n",
    "        if isinstance(o, BNode):\n",
    "            blank_node_data.append((p, \"Blank Node\"))\n",
    "            blank_node_data.extend(extract_blank_node(o))\n",
    "        else:\n",
    "            blank_node_data.append((p, o))\n",
    "    return blank_node_data\n",
    "\n",
    "# Prepare data for CSV\n",
    "csv_data = [[\"Subject\", \"Predicate\", \"Object\"]]\n",
    "\n",
    "# Extract Class\n",
    "for _, _, o in g.triples((drug_uri, RDF.type, None)):\n",
    "    csv_data.append([drug_uri, RDF.type, o])\n",
    "\n",
    "# Extract SubClass\n",
    "for _, _, o in g.triples((drug_uri, RDFS.subClassOf, None)):\n",
    "    if isinstance(o, BNode):\n",
    "        csv_data.append([drug_uri, RDFS.subClassOf, \"Blank Node\"])\n",
    "        csv_data.extend(extract_blank_node(o))\n",
    "    else:\n",
    "        csv_data.append([drug_uri, RDFS.subClassOf, o])\n",
    "\n",
    "# Extract other information\n",
    "for _, p, o in g.triples((drug_uri, None, None)):\n",
    "    if p not in {RDF.type, RDFS.subClassOf}:\n",
    "        csv_data.append([drug_uri, p, o])\n",
    "\n",
    "# Write data to CSV\n",
    "with open(\"drug_info.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(\"Data saved to drug_info.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to drug_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, Namespace\n",
    "\n",
    "# Define the Turtle data with necessary prefixes\n",
    "turtle_data = \"\"\"\n",
    "@prefix sio: <http://semanticscience.org/resource/> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix bio: <http://data.bioontology.org/metadata/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "\n",
    "<http://semanticscience.org/resource/Drug> a owl:Class ;\n",
    "    rdfs:subClassOf <http://semanticscience.org/resource/ChemicalSubstance>, [\n",
    "        rdf:type owl:Restriction ;\n",
    "        owl:onProperty <http://semanticscience.org/resource/hasCapability> ;\n",
    "        owl:someValuesFrom [\n",
    "            rdf:type owl:Class ;\n",
    "            owl:intersectionOf (\n",
    "                <http://semanticscience.org/resource/ToRegulate>\n",
    "                [\n",
    "                    rdf:type owl:Restriction ;\n",
    "                    owl:onProperty <http://semanticscience.org/resource/inRelationTo> ;\n",
    "                    owl:someValuesFrom <http://semanticscience.org/resource/BiologicalEntity> ;\n",
    "                ]\n",
    "            ) ;\n",
    "        ] ;\n",
    "    ] ;\n",
    "    bio:prefixIRI \"sio:Drug\" ;\n",
    "    dcterms:description \"A drug is a chemical substance that contains one or more active ingredients that regulate one or more biological processes.\"@en ;\n",
    "    rdfs:isDefinedBy <http://semanticscience.org/ontology/sio/v1.53/sio-subset-labels.owl> ;\n",
    "    rdfs:label \"drug\"@en .\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create a Graph and parse the data\n",
    "g = Graph()\n",
    "g.parse(data=turtle_data, format=\"turtle\")\n",
    "\n",
    "# Define the necessary namespaces\n",
    "SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "\n",
    "# Bind prefixes to the graph (optional but useful for serialization and querying)\n",
    "g.bind(\"sio\", SIO)\n",
    "g.bind(\"dcterms\", DC)\n",
    "g.bind(\"bio\", BIO)\n",
    "g.bind(\"owl\", OWL)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"rdf\", RDF)\n",
    "\n",
    "# Define the resource URI\n",
    "drug_uri = URIRef(\"http://semanticscience.org/resource/Drug\")\n",
    "\n",
    "# Function to get label or URI parts\n",
    "def get_label_and_uri(uri):\n",
    "    qres = g.query(\n",
    "        \"\"\"\n",
    "        SELECT ?label WHERE {\n",
    "            ?uri rdfs:label ?label .\n",
    "        }\n",
    "        \"\"\",\n",
    "        initBindings={'uri': uri}\n",
    "    )\n",
    "    for row in qres:\n",
    "        return str(row.label), str(uri)\n",
    "    return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "# Function to recursively extract blank node information\n",
    "def extract_blank_node(bnode):\n",
    "    blank_node_data = []\n",
    "    for p, o in g.predicate_objects(bnode):\n",
    "        if isinstance(o, BNode):\n",
    "            pred_label, pred_uri = get_label_and_uri(p)\n",
    "            blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "            blank_node_data.extend(extract_blank_node(o))\n",
    "        else:\n",
    "            pred_label, pred_uri = get_label_and_uri(p)\n",
    "            obj_label, obj_uri = get_label_and_uri(o)\n",
    "            blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "    return blank_node_data\n",
    "\n",
    "# Prepare data for CSV\n",
    "csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "# Extract Class\n",
    "for _, _, o in g.triples((drug_uri, RDF.type, None)):\n",
    "    subj_label, subj_uri = get_label_and_uri(drug_uri)\n",
    "    obj_label, obj_uri = get_label_and_uri(o)\n",
    "    csv_data.append([subj_label, subj_uri, \"type\", str(RDF.type), obj_label, obj_uri])\n",
    "\n",
    "# Extract SubClass\n",
    "for _, _, o in g.triples((drug_uri, RDFS.subClassOf, None)):\n",
    "    subj_label, subj_uri = get_label_and_uri(drug_uri)\n",
    "    pred_label, pred_uri = get_label_and_uri(RDFS.subClassOf)\n",
    "    if isinstance(o, BNode):\n",
    "        csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "        csv_data.extend(extract_blank_node(o))\n",
    "    else:\n",
    "        obj_label, obj_uri = get_label_and_uri(o)\n",
    "        csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "# Extract other information\n",
    "for _, p, o in g.triples((drug_uri, None, None)):\n",
    "    if p not in {RDF.type, RDFS.subClassOf}:\n",
    "        subj_label, subj_uri = get_label_and_uri(drug_uri)\n",
    "        pred_label, pred_uri = get_label_and_uri(p)\n",
    "        obj_label, obj_uri = get_label_and_uri(o)\n",
    "        csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "# Write data to CSV\n",
    "with open(\"drug_info.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(\"Data saved to drug_info.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, Namespace\n",
    "\n",
    "# Define the path to the Turtle file\n",
    "turtle_file_path = \"../Ontologies/materialsmine_converted.ttl\"\n",
    "\n",
    "# Create a Graph and parse the data from the file\n",
    "g = Graph()\n",
    "g.parse(turtle_file_path, format=\"turtle\")\n",
    "\n",
    "# Define the necessary namespaces\n",
    "SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "\n",
    "# Bind prefixes to the graph (optional but useful for serialization and querying)\n",
    "g.bind(\"sio\", SIO)\n",
    "g.bind(\"dcterms\", DC)\n",
    "g.bind(\"bio\", BIO)\n",
    "g.bind(\"owl\", OWL)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"rdf\", RDF)\n",
    "\n",
    "# Function to get label or URI parts\n",
    "def get_label_and_uri(uri):\n",
    "    qres = g.query(\n",
    "        \"\"\"\n",
    "        SELECT ?label WHERE {\n",
    "            ?uri rdfs:label ?label .\n",
    "        }\n",
    "        \"\"\",\n",
    "        initBindings={'uri': uri}\n",
    "    )\n",
    "    for row in qres:\n",
    "        return str(row.label), str(uri)\n",
    "    return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "# Function to recursively extract blank node information\n",
    "def extract_blank_node(bnode):\n",
    "    blank_node_data = []\n",
    "    for p, o in g.predicate_objects(bnode):\n",
    "        if isinstance(o, BNode):\n",
    "            pred_label, pred_uri = get_label_and_uri(p)\n",
    "            blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "            blank_node_data.extend(extract_blank_node(o))\n",
    "        else:\n",
    "            pred_label, pred_uri = get_label_and_uri(p)\n",
    "            obj_label, obj_uri = get_label_and_uri(o)\n",
    "            blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "    return blank_node_data\n",
    "\n",
    "# Prepare data for CSV\n",
    "csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "# Extract all triples\n",
    "for subj, pred, obj in g:\n",
    "    subj_label, subj_uri = get_label_and_uri(subj)\n",
    "    pred_label, pred_uri = get_label_and_uri(pred)\n",
    "    \n",
    "    if isinstance(obj, BNode):\n",
    "        csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "        csv_data.extend(extract_blank_node(obj))\n",
    "    else:\n",
    "        obj_label, obj_uri = get_label_and_uri(obj)\n",
    "        csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "# Write data to CSV\n",
    "with open(\"all_info.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(\"Data saved to all_info.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Reading from the file ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, Namespace\n",
    "\n",
    "\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "class RDFGraphHandler:\n",
    "    def __init__(self, turtle_file_path):\n",
    "        self.turtle_file_path = turtle_file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        self.graph.parse(self.turtle_file_path, format=\"turtle\")\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        qres = self.graph.query(\n",
    "            \"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initBindings={'uri': uri}\n",
    "        )\n",
    "        for row in qres:\n",
    "            return str(row.label), str(uri)\n",
    "        return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def save_to_csv(self, class_names, output_csv_path):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "            \n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    csv_data.extend(self.extract_blank_node(obj))\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(csv_data)\n",
    "\n",
    "        print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "# Usage example\n",
    "turtle_file_path = \"../Ontologies/materialsmine_converted.ttl\"\n",
    "class_names = [\"drug\" , \"stress\", 'AmperePerJoule', 'Tensiletest']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "rdf_handler = RDFGraphHandler(turtle_file_path)\n",
    "rdf_handler.save_to_csv(class_names, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Improving the speed of info capturing ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, Namespace\n",
    "\n",
    "\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "class RDFGraphHandler:\n",
    "    def __init__(self, turtle_file_path):\n",
    "        self.turtle_file_path = turtle_file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        self.graph.parse(self.turtle_file_path, format=\"turtle\")\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        qres = self.graph.query(\n",
    "            \"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initBindings={'uri': uri}\n",
    "        )\n",
    "        for row in qres:\n",
    "            return str(row.label), str(uri)\n",
    "        return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    \n",
    "    def save_to_csv(self, class_names, output_csv_path):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "        label_cache = {}\n",
    "\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = label_cache.get(subj, self.get_label_and_uri(subj))\n",
    "            pred_label, pred_uri = label_cache.get(pred, self.get_label_and_uri(pred))\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "            \n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    csv_data.extend(self.extract_blank_node(obj))\n",
    "                    # Set placeholders for obj_label and obj_uri when obj is a BNode\n",
    "                    obj_label, obj_uri = \"Blank Node\", \"Blank Node\"\n",
    "                else:\n",
    "                    obj_label, obj_uri = label_cache.get(obj, self.get_label_and_uri(obj))\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "            else:\n",
    "                # Handle the case where normalized_subj_label is not in normalized_class_names\n",
    "                continue  # Or handle as needed\n",
    "\n",
    "            # Ensure obj_label and obj_uri are always assigned outside of the conditional\n",
    "            if not isinstance(obj, BNode):\n",
    "                obj_label, obj_uri = label_cache.get(obj, self.get_label_and_uri(obj))\n",
    "\n",
    "\n",
    "        with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(csv_data)\n",
    "\n",
    "        print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "turtle_file_path = \"../Ontologies/materialsmine_converted.ttl\"\n",
    "class_names = [\"drug\" , \"stress\", 'AmperePerJoule', 'Tensiletest']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "rdf_handler = RDFGraphHandler(turtle_file_path)\n",
    "rdf_handler.save_to_csv(class_names, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Implementing multiple input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info_2.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, Namespace\n",
    "\n",
    "\n",
    "class RDFGraphHandler:\n",
    "    def __init__(self, turtle_file_path):\n",
    "        self.turtle_file_path = turtle_file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        self.graph.parse(self.turtle_file_path, format=\"turtle\")\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        qres = self.graph.query(\n",
    "            \"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initBindings={'uri': uri}\n",
    "        )\n",
    "        for row in qres:\n",
    "            return str(row.label), str(uri)\n",
    "        return uri.split('/')[-1].split('#')[-1], str(uri)\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    \n",
    "    def save_to_csv(self, class_names, output_csv_path):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "        label_cache = {}\n",
    "\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = label_cache.get(subj, self.get_label_and_uri(subj))\n",
    "            pred_label, pred_uri = label_cache.get(pred, self.get_label_and_uri(pred))\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "            \n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    csv_data.extend(self.extract_blank_node(obj))\n",
    "                    # Set placeholders for obj_label and obj_uri when obj is a BNode\n",
    "                    obj_label, obj_uri = \"Blank Node\", \"Blank Node\"\n",
    "                else:\n",
    "                    obj_label, obj_uri = label_cache.get(obj, self.get_label_and_uri(obj))\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "            # Ensure obj_label and obj_uri are always assigned outside of the conditional\n",
    "            if not isinstance(obj, BNode):\n",
    "                obj_label, obj_uri = label_cache.get(obj, self.get_label_and_uri(obj))\n",
    "\n",
    "        with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(csv_data)\n",
    "\n",
    "        print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "input_turtle_files = [\"../Ontologies/materialsmine_converted.ttl\"]\n",
    "class_names = [\"drug\", \"stress\", 'AmperePerJoule', 'Tensiletest']\n",
    "output_csv_path = \"filtered_info_2.csv\"\n",
    "\n",
    "for turtle_file_path in input_turtle_files:\n",
    "    rdf_handler = RDFGraphHandler(turtle_file_path)\n",
    "    rdf_handler.save_to_csv(class_names, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using multiprocessing ti speed up the process ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info_5.csv\n",
      "Data saved to filtered_info_5.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    directory = '.'\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            os.remove(os.path.join(directory, filename))\n",
    "\n",
    "\n",
    "    def __init__(self, turtle_file_path):\n",
    "        self.turtle_file_path = turtle_file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        self.graph.parse(self.turtle_file_path, format=\"turtle\")\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes):\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            results = pool.map(self.extract_blank_node, bnodes)\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def save_to_csv(self, class_names, output_csv_path):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes)\n",
    "        csv_data.extend(blank_node_data)\n",
    "\n",
    "        with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(csv_data)\n",
    "\n",
    "        print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "# Usage example\n",
    "input_turtle_files = [\"../Ontologies/materialsmine_converted.ttl\", \"../Ontologies/emmo.ttl\"]\n",
    "class_names = [\"drug\", \"stress\", 'AmperePerJoule', 'Tensiletest']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "for turtle_file_path in input_turtle_files:\n",
    "    rdf_handler = RDFGraphHandler(turtle_file_path)\n",
    "    rdf_handler.save_to_csv(class_names, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The above code work much faster! however when two or more inputs are given then for each inputs the outputs in CSV are removed! ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here I try to solve the above issue ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    def __init__(self, turtle_file_path):\n",
    "        self.turtle_file_path = turtle_file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        self.graph.parse(self.turtle_file_path, format=\"turtle\")\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes):\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            results = pool.map(self.extract_blank_node, bnodes)\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def extract_data(self, class_names):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = []\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes)\n",
    "        csv_data.extend(blank_node_data)\n",
    "        \n",
    "        return csv_data\n",
    "\n",
    "# Usage example\n",
    "input_turtle_files = [\"../Ontologies/emmo.ttl\" , \"../Ontologies/materialsmine_converted.ttl\"]\n",
    "class_names = [\"drug\", \"stress\", 'AmperePerJoule', 'Tensiletest']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "# Remove existing CSV file if it exists\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "\n",
    "# Initialize CSV data with headers\n",
    "all_csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "for turtle_file_path in input_turtle_files:\n",
    "    rdf_handler = RDFGraphHandler(turtle_file_path)\n",
    "    file_csv_data = rdf_handler.extract_data(class_names)\n",
    "    all_csv_data.extend(file_csv_data)\n",
    "\n",
    "# Write accumulated CSV data to file\n",
    "with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(all_csv_data)\n",
    "\n",
    "print(f\"Data saved to {output_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Above code down not accept OWL file; Let's fix it ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        file_format = 'turtle' if self.file_path.endswith('.ttl') else 'xml'\n",
    "        self.graph.parse(self.file_path, format=file_format)\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def extract_blank_node(self, bnode):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((\"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes):\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            results = pool.map(self.extract_blank_node, bnodes)\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def extract_data(self, class_names):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = []\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes)\n",
    "        csv_data.extend(blank_node_data)\n",
    "        \n",
    "        return csv_data\n",
    "\n",
    "# Usage example\n",
    "input_files = [\n",
    "    \"../Ontologies/MatWerk.xrdf\", \n",
    "    \"../Ontologies/materialsmine_converted.ttl\" , \n",
    "    \"../Ontologies/emmo.ttl\",\n",
    "    # \"../Ontologies/schemaorg.owl\"\n",
    "    ]\n",
    "class_names = [\"drug\", \"stress\", 'AmperePerJoule', 'Tensiletest', 'Electronic lab Notebook']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "# Remove existing CSV file if it exists\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "# Initialize CSV data with headers\n",
    "all_csv_data = [[\"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "for file_path in input_files:\n",
    "    rdf_handler = RDFGraphHandler(file_path)\n",
    "    file_csv_data = rdf_handler.extract_data(class_names)\n",
    "    all_csv_data.extend(file_csv_data)\n",
    "\n",
    "# Write accumulated CSV data to file\n",
    "with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(all_csv_data)\n",
    "\n",
    "print(f\"Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Let's save the input name to csv file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        file_format = 'turtle' if self.file_path.endswith('.ttl') else 'xml'\n",
    "        self.graph.parse(self.file_path, format=file_format)\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def extract_blank_node(self, bnode, file_name):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o, file_name))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes, file_name):\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            results = pool.starmap(self.extract_blank_node, [(bnode, file_name) for bnode in bnodes])\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def extract_data(self, class_names):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = []\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes, os.path.basename(self.file_path))\n",
    "        csv_data.extend(blank_node_data)\n",
    "        \n",
    "        return csv_data\n",
    "\n",
    "# Usage example\n",
    "input_files = [\n",
    "    \"../Ontologies/MatWerk.xrdf\", \n",
    "    \"../Ontologies/materialsmine_converted.ttl\" , \n",
    "    \"../Ontologies/emmo.ttl\",\n",
    "    # \"../Ontologies/schemaorg.owl\"\n",
    "    ]\n",
    "class_names = [\"drug\", \"stress\", 'AmperePerJoule', 'Tensiletest', 'Electronic lab Notebook']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "\n",
    "# Remove existing CSV file if it exists\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "# Initialize CSV data with headers\n",
    "all_csv_data = [[\"File Name\", \"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "for file_path in input_files:\n",
    "    rdf_handler = RDFGraphHandler(file_path)\n",
    "    file_csv_data = rdf_handler.extract_data(class_names)\n",
    "    all_csv_data.extend(file_csv_data)\n",
    "\n",
    "# Write accumulated CSV data to file\n",
    "with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(all_csv_data)\n",
    "\n",
    "print(f\"Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Use 8 cores to process ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        file_format = 'turtle' if self.file_path.endswith('.ttl') else 'xml'\n",
    "        self.graph.parse(self.file_path, format=file_format)\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def extract_blank_node(self, bnode, file_name):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o, file_name))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes, file_name, num_cores):\n",
    "        with multiprocessing.Pool(processes=num_cores) as pool:\n",
    "            results = pool.starmap(self.extract_blank_node, [(bnode, file_name) for bnode in bnodes])\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def extract_data(self, class_names, num_cores=8):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = []\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes, os.path.basename(self.file_path), num_cores)\n",
    "        csv_data.extend(blank_node_data)\n",
    "        \n",
    "        return csv_data\n",
    "\n",
    "# Usage example\n",
    "input_files = [\n",
    "    \"../Ontologies/MatWerk.xrdf\", \n",
    "    \"../Ontologies/materialsmine_converted.ttl\" , \n",
    "    \"../Ontologies/emmo.ttl\",\n",
    "    # \"../Ontologies/schemaorg.owl\"\n",
    "    ]\n",
    "class_names = [\"drug\", \"stress\", 'AmperePerJoule', 'Tensiletest', 'Electronic lab Notebook']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "num_cores = 8\n",
    "\n",
    "# Remove existing CSV file if it exists\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "# Initialize CSV data with headers\n",
    "all_csv_data = [[\"File Name\", \"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "for file_path in input_files:\n",
    "    rdf_handler = RDFGraphHandler(file_path)\n",
    "    file_csv_data = rdf_handler.extract_data(class_names, num_cores)\n",
    "    all_csv_data.extend(file_csv_data)\n",
    "\n",
    "# Write accumulated CSV data to file\n",
    "with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(all_csv_data)\n",
    "\n",
    "print(f\"Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using Threads ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        file_format = 'turtle' if self.file_path.endswith('.ttl') else 'xml'\n",
    "        self.graph.parse(self.file_path, format=file_format)\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def extract_blank_node(self, bnode, file_name):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o, file_name))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes, file_name, num_cores, num_threads):\n",
    "        if num_threads > 1:\n",
    "            with ThreadPool(num_threads) as pool:\n",
    "                results = pool.starmap(self.extract_blank_node, [(bnode, file_name) for bnode in bnodes])\n",
    "        else:\n",
    "            with multiprocessing.Pool(processes=num_cores) as pool:\n",
    "                results = pool.starmap(self.extract_blank_node, [(bnode, file_name) for bnode in bnodes])\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def extract_data(self, class_names, num_cores=8, num_threads=1):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = []\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes, os.path.basename(self.file_path), num_cores, num_threads)\n",
    "        csv_data.extend(blank_node_data)\n",
    "        \n",
    "        return csv_data\n",
    "\n",
    "# Usage example\n",
    "input_files = [\n",
    "    # \"../Ontologies/MatWerk.xrdf\", \n",
    "    \"../Ontologies/materialsmine_converted.ttl\" , \n",
    "    # \"../Ontologies/emmo.ttl\",\n",
    "    # \"../Ontologies/schemaorg.owl\"\n",
    "    ]\n",
    "# class_names = [\"drug\", \"stress\", 'Ampere_Per+Joule', 'Tensiletest', 'Electronic lab Notebook']\n",
    "class_names = ['stress']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "num_cores = 4\n",
    "num_threads = 2\n",
    "\n",
    "# Remove existing CSV file if it exists\n",
    "directory = '.'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "\n",
    "# Initialize CSV data with headers\n",
    "all_csv_data = [[\"File Name\", \"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "for file_path in input_files:\n",
    "    rdf_handler = RDFGraphHandler(file_path)\n",
    "    file_csv_data = rdf_handler.extract_data(class_names, num_cores, num_threads)\n",
    "    all_csv_data.extend(file_csv_data)\n",
    "\n",
    "# Write accumulated CSV data to file\n",
    "with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(all_csv_data)\n",
    "\n",
    "print(f\"Data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Capturing the hierarchy ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n",
      "Class hierarchy data saved to class_hierarchy_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        file_format = 'turtle' if self.file_path.endswith('.ttl') else 'xml'\n",
    "        self.graph.parse(self.file_path, format=file_format)\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def extract_blank_node(self, bnode, file_name):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o, file_name))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes, file_name, num_cores, num_threads):\n",
    "        if num_threads > 1:\n",
    "            with ThreadPool(num_threads) as pool:\n",
    "                results = pool.starmap(self.extract_blank_node, [(bnode, file_name) for bnode in bnodes])\n",
    "        else:\n",
    "            with multiprocessing.Pool(processes=num_cores) as pool:\n",
    "                results = pool.starmap(self.extract_blank_node, [(bnode, file_name) for bnode in bnodes])\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def traverse_class_hierarchy(self, class_uri):\n",
    "        class_info = []\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?subclass ?subclass_label WHERE {\n",
    "                ?class_uri rdfs:subClassOf+ ?subclass .\n",
    "                ?subclass rdfs:label ?subclass_label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'class_uri': class_uri})\n",
    "        for row in qres:\n",
    "            class_info.append((str(class_uri), str(row.subclass_label), str(row.subclass)))\n",
    "        return class_info\n",
    "\n",
    "    def extract_data_with_hierarchy(self, class_names, num_cores=8, num_threads=1):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = []\n",
    "        class_hierarchy_data = []\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "                # Extract class hierarchy information\n",
    "                class_hierarchy_data.extend(self.traverse_class_hierarchy(subj))\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes, os.path.basename(self.file_path), num_cores, num_threads)\n",
    "        csv_data.extend(blank_node_data)\n",
    "        \n",
    "        return csv_data, class_hierarchy_data\n",
    "\n",
    "# Main script to run the RDF processing and CSV writing\n",
    "if __name__ == \"__main__\":\n",
    "    # Usage example\n",
    "    input_files = [\n",
    "        # \"../Ontologies/MatWerk.xrdf\", \n",
    "        \"../Ontologies/materialsmine_converted.ttl\" , \n",
    "        # \"../Ontologies/emmo.ttl\",\n",
    "        # \"../Ontologies/schemaorg.owl\"\n",
    "        ]\n",
    "    # class_names = [\"drug\", \"stress\", 'Ampere_Per+Joule', 'Tensiletest', 'Electronic lab Notebook']\n",
    "    class_names = ['stress']\n",
    "    output_csv_path = \"filtered_info.csv\"\n",
    "    hierarchy_csv_path = \"class_hierarchy_info.csv\"\n",
    "    num_cores = 4\n",
    "    num_threads = 2\n",
    "\n",
    "    # Remove existing CSV files if they exist\n",
    "    for filename in [output_csv_path, hierarchy_csv_path]:\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "\n",
    "    # Initialize CSV data with headers\n",
    "    all_csv_data = [[\"File Name\", \"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "    # Initialize hierarchy data with headers\n",
    "    all_hierarchy_data = [[\"Class URI\", \"Superclass Label\", \"Superclass URI\"]]\n",
    "\n",
    "    for file_path in input_files:\n",
    "        rdf_handler = RDFGraphHandler(file_path)\n",
    "        file_csv_data, file_hierarchy_data = rdf_handler.extract_data_with_hierarchy(class_names, num_cores, num_threads)\n",
    "        all_csv_data.extend(file_csv_data)\n",
    "        all_hierarchy_data.extend(file_hierarchy_data)\n",
    "\n",
    "    # Write accumulated CSV data to files\n",
    "    with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(all_csv_data)\n",
    "\n",
    "    print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "    # Write hierarchy data to a separate CSV file\n",
    "    with open(hierarchy_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(all_hierarchy_data)\n",
    "\n",
    "    print(f\"Class hierarchy data saved to {hierarchy_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Above code can capture the class hierarchy but in output it is not correctly shown! ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "input_files = [\n",
    "    # \"../Ontologies/MatWerk.xrdf\", \n",
    "    \"../Ontologies/materialsmine_converted.ttl\" , \n",
    "    # \"../Ontologies/emmo.ttl\",\n",
    "    # \"../Ontologies/schemaorg.owl\"\n",
    "    ]\n",
    "# class_names = [\"drug\", \"stress\", 'Ampere_Per+Joule', 'Tensiletest', 'Electronic lab Notebook']\n",
    "class_names = ['stress']\n",
    "output_csv_path = \"filtered_info.csv\"\n",
    "hierarchy_csv_path = \"class_hierarchy_info.csv\"\n",
    "num_cores = 4\n",
    "num_threads = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try to solve the above issue ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to filtered_info.csv\n",
      "Class hierarchy data saved to class_hierarchy_info.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rdflib import Graph, BNode, URIRef, RDF, RDFS\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Namespace\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "class RDFGraphHandler:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.graph = Graph()\n",
    "        self.SIO = Namespace(\"http://semanticscience.org/resource/\")\n",
    "        self.DC = Namespace(\"http://purl.org/dc/terms/\")\n",
    "        self.BIO = Namespace(\"http://data.bioontology.org/metadata/\")\n",
    "        self.OWL = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "        self.RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "        self.RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "        self.label_cache = {}\n",
    "        self.bind_namespaces()\n",
    "        self.parse_graph()\n",
    "\n",
    "    def bind_namespaces(self):\n",
    "        self.graph.bind(\"sio\", self.SIO)\n",
    "        self.graph.bind(\"dcterms\", self.DC)\n",
    "        self.graph.bind(\"bio\", self.BIO)\n",
    "        self.graph.bind(\"owl\", self.OWL)\n",
    "        self.graph.bind(\"rdfs\", self.RDFS)\n",
    "        self.graph.bind(\"rdf\", self.RDF)\n",
    "\n",
    "    def parse_graph(self):\n",
    "        file_format = 'turtle' if self.file_path.endswith('.ttl') else 'xml'\n",
    "        self.graph.parse(self.file_path, format=file_format)\n",
    "\n",
    "    def get_label_and_uri(self, uri):\n",
    "        if uri in self.label_cache:\n",
    "            return self.label_cache[uri]\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?label WHERE {\n",
    "                ?uri rdfs:label ?label .\n",
    "            }\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'uri': uri})\n",
    "        for row in qres:\n",
    "            result = (str(row.label), str(uri))\n",
    "            self.label_cache[uri] = result\n",
    "            return result\n",
    "        \n",
    "        result = (uri.split('/')[-1].split('#')[-1], str(uri))\n",
    "        self.label_cache[uri] = result\n",
    "        return result\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        return re.sub(r'[^a-z0-9]', '', s.lower())\n",
    "\n",
    "    def traverse_class_hierarchy_recursive(self, class_uri, class_label, visited):\n",
    "        class_hierarchy_data = []\n",
    "\n",
    "        query = prepareQuery(\"\"\"\n",
    "            SELECT ?subclass ?subclass_label WHERE {\n",
    "                ?class_uri rdfs:subClassOf ?subclass .\n",
    "                ?subclass rdfs:label ?subclass_label .\n",
    "            }\n",
    "            ORDER BY DESC(?subclass)\n",
    "            \"\"\",\n",
    "            initNs={\"rdfs\": self.RDFS}\n",
    "        )\n",
    "        qres = self.graph.query(query, initBindings={'class_uri': class_uri})\n",
    "        for row in qres:\n",
    "            subclass_uri = row.subclass\n",
    "            subclass_label = row.subclass_label\n",
    "\n",
    "            if subclass_uri not in visited:\n",
    "                visited.add(subclass_uri)\n",
    "                # class_hierarchy_data.append((subclass_uri, subclass_label, class_uri, class_label))\n",
    "                class_hierarchy_data.append((class_label, class_uri, subclass_label, subclass_uri))\n",
    "                class_hierarchy_data.extend(self.traverse_class_hierarchy_recursive(subclass_uri, subclass_label, visited))\n",
    "\n",
    "        return class_hierarchy_data\n",
    "\n",
    "    def extract_class_hierarchy(self, class_names):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        hierarchy_data = []\n",
    "\n",
    "        for class_uri in self.graph.subjects(RDFS.subClassOf):\n",
    "            class_label, _ = self.get_label_and_uri(class_uri)\n",
    "            if self.normalize_string(class_label) in normalized_class_names:\n",
    "                visited = set()\n",
    "                hierarchy_data.extend(self.traverse_class_hierarchy_recursive(class_uri, class_label, visited))\n",
    "\n",
    "        return hierarchy_data\n",
    "\n",
    "    def extract_data_with_hierarchy(self, class_names, num_cores=1, num_threads=0):\n",
    "        normalized_class_names = {self.normalize_string(name) for name in class_names}\n",
    "        csv_data = []\n",
    "        hierarchy_data = self.extract_class_hierarchy(class_names)\n",
    "\n",
    "        bnodes = []\n",
    "        for subj, pred, obj in self.graph:\n",
    "            subj_label, subj_uri = self.get_label_and_uri(subj)\n",
    "            pred_label, pred_uri = self.get_label_and_uri(pred)\n",
    "            normalized_subj_label = self.normalize_string(subj_label)\n",
    "\n",
    "            if normalized_subj_label in normalized_class_names:\n",
    "                if isinstance(obj, BNode):\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, \"Blank Node\", \"Blank Node\"])\n",
    "                    bnodes.append(obj)\n",
    "                else:\n",
    "                    obj_label, obj_uri = self.get_label_and_uri(obj)\n",
    "                    csv_data.append([os.path.basename(self.file_path), subj_label, subj_uri, pred_label, pred_uri, obj_label, obj_uri])\n",
    "\n",
    "        blank_node_data = self.extract_blank_nodes_parallel(bnodes, os.path.basename(self.file_path), num_cores, num_threads)\n",
    "        csv_data.extend(blank_node_data)\n",
    "        \n",
    "        return csv_data, hierarchy_data\n",
    "\n",
    "    def extract_blank_node(self, bnode, file_name):\n",
    "        blank_node_data = []\n",
    "        for p, o in self.graph.predicate_objects(bnode):\n",
    "            if isinstance(o, BNode):\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, \"Blank Node\", pred_uri))\n",
    "                blank_node_data.extend(self.extract_blank_node(o, file_name))\n",
    "            else:\n",
    "                pred_label, pred_uri = self.get_label_and_uri(p)\n",
    "                obj_label, obj_uri = self.get_label_and_uri(o)\n",
    "                blank_node_data.append((file_name, \"Blank Node\", \"Blank Node\", pred_label, obj_label, pred_uri, obj_uri))\n",
    "        return blank_node_data\n",
    "\n",
    "    def extract_blank_nodes_parallel(self, bnodes, file_name, num_cores, num_threads):\n",
    "        if num_threads > 1:\n",
    "            with ThreadPool(num_threads) as pool:\n",
    "                results = pool.starmap(self.extract_blank_node, [(bnode, file_name) for bnode in bnodes])\n",
    "        else:\n",
    "            with multiprocessing.Pool(processes=num_cores) as pool:\n",
    "                results = pool.starmap(self.extract_blank_node, [(bnode, file_name) for bnode in bnodes])\n",
    "        blank_node_data = [item for sublist in results for item in sublist]\n",
    "        return blank_node_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_files = [\n",
    "    # \"../Ontologies/MatWerk.xrdf\", \n",
    "    \"../Ontologies/nfdicore_2.ttl\", \n",
    "    # \"../Ontologies/fabio.ttl\", \n",
    "    \"../Ontologies/pmdco_core.ttl\", \n",
    "    \"../Ontologies/materialsmine_converted.ttl\" , \n",
    "    # \"../Ontologies/emmo.ttl\",\n",
    "    # \"../Ontologies/schemaorg.owl\"\n",
    "    ]\n",
    "    class_names = ['stress','drug','AmpereHour+Per_Litre','Electroniclab_Notebook', 'ClampingPressure']  # Adjust class names as needed\n",
    "    output_csv_path = \"filtered_info.csv\"\n",
    "    hierarchy_csv_path = \"class_hierarchy_info.csv\"\n",
    "    num_cores = 1\n",
    "    num_threads = 1\n",
    "\n",
    "    # Remove existing CSV files if they exist\n",
    "    for filename in [output_csv_path, hierarchy_csv_path]:\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "\n",
    "    # Initialize CSV data with headers\n",
    "    all_csv_data = [[\"File Name\", \"Subject\", \"Subject URI\", \"Predicate\", \"Predicate URI\", \"Object\", \"Object URI\"]]\n",
    "\n",
    "    # Initialize hierarchy data with headers\n",
    "    all_hierarchy_data = [[\"Class Label\", \"Class URI\", \"is SubClass of\", \"Superclass URI\"]]\n",
    "\n",
    "    for file_path in input_files:\n",
    "        rdf_handler = RDFGraphHandler(file_path)\n",
    "        file_csv_data, file_hierarchy_data = rdf_handler.extract_data_with_hierarchy(class_names, num_cores, num_threads)\n",
    "        all_csv_data.extend(file_csv_data)\n",
    "\n",
    "        # Append hierarchy data to all_hierarchy_data\n",
    "        all_hierarchy_data.extend(file_hierarchy_data)\n",
    "\n",
    "    # Write accumulated CSV data to files\n",
    "    with open(output_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(all_csv_data)\n",
    "\n",
    "    print(f\"Data saved to {output_csv_path}\")\n",
    "\n",
    "    # Write hierarchy data to a separate CSV file\n",
    "    with open(hierarchy_csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(all_hierarchy_data)\n",
    "\n",
    "    print(f\"Class hierarchy data saved to {hierarchy_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
